[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal inference notes",
    "section": "",
    "text": "In this space I want to collect my notes on causal inference.\nIf you find the notes helpful, find any errors, or have any suggestions, please get in touch by writing to fa.gunzinger@gmail.com.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>index.html</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#sampling",
    "href": "chapters/stats_foundations.html#sampling",
    "title": "2  Statistics foundation",
    "section": "",
    "text": "We rely on a sample to learn about a larger population.\nWe thus need to make sure that the sampling procedure is free of bias, so that units in the sample are representative of those in the population.\nWhile representativeness cannot be achieved perfectly, it’s important to ensure that non-representativeness is due to random error and not due to systematic bias.\nRandom errors produce deviations that vary over repeated samples, while systematic bias persists. Such selection bias can lead to misleading and ephemeral conclusions.\nTwo basic sampling procedures are simple random sampling (randomly select \\(n\\) units from a population of \\(N\\)) and stratified random sampling (randomly select \\(n_s\\) from each stratum \\(S\\) of a population of \\(N\\)).\nThe mean outcome of the sample is denoted \\(\\bar{x}\\); that of the population, \\(\\mu\\).\n(On stratification: why does it reduce variance? Imagine an extreme case, where the number of strata were equal to the number of different units in the sample. In this case, the variance would be zero. Number of diff units here needns be individuals, but groups of units that share all relevant characteristics)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#sampling-distributions",
    "href": "chapters/stats_foundations.html#sampling-distributions",
    "title": "2  Statistics foundation",
    "section": "2.2 Sampling distributions",
    "text": "2.2 Sampling distributions\n\nA sampling distribution is the distribution of a statistic (e.g. the mean) over many repeated samples. Classical statistics is much concerned with making inferences from samples about the population based on such statistics.\nWhen we measure an attribute of the population based on a sample using a statistic, the result will vary over repeated samples. To capture by how much it varies, we are concerned with the sampling variability.\nKey distinctions:\n\nThe data distribution is the distribution of the data in the sample, and its spread is measured by the standard deviation.\nThe sampling distribution is the distribution of the sample statistic, and its spread is measured by the standard error.\n\n\n\nFigure show that:\n\nData distribution has larger spread than sampling distributions (each data point is a special case of a sample with n = 1)\nThe spread of sampling distributions decreases with increasing sample size",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#law-of-large-numbers-and-central-limit-theorem",
    "href": "chapters/stats_foundations.html#law-of-large-numbers-and-central-limit-theorem",
    "title": "2  Statistics foundation",
    "section": "2.3 Law of large numbers and central limit theorem",
    "text": "2.3 Law of large numbers and central limit theorem\n\nSuppose that we have a sequence of independent and identically distributed (iid) random variables \\(\\{x_1, ..., x_n\\}\\) drawn from a distribution with expected value \\(\\mu\\) and finite variance \\(\\sigma^2\\), and we are interested in the mean value \\(\\bar{x} = \\frac{x_1 + ... + x_n}{n}\\).\nThe law or large numbers states that \\(\\bar{x}\\) converges to \\(\\mu\\) as we increase the sample size. Formally:\n\n\\[\n\\bar{x} \\rightarrow \\mu \\text{ as } n \\rightarrow \\infty.\n\\]\n\nThe (classical, Lindeberg-Lévy) central limit theorem describes the spread of the sampling distribution of \\(\\bar{x}\\) around \\(\\mu\\) during this convergence. In particular, it implies that for large enough \\(n\\), the distribution of \\(\\bar{x}\\) will be close to a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). The above figures are a visual representation of this. Formally:\n\n\\[\n\\lim _{n\\to\\infty} \\sqrt{n}(\\bar{x} - \\mu) \\rightarrow \\mathcal{N}\\left(0,\\sigma ^{2}\\right).\n\\]\n\nThis is useful because it means that irrespective of the underlying distribution (i.e. the distribution of the values in our sequence above), we can use the normal distribution and approximations to it (such as the t-distribution) to calculate sampling distributions when we do inference. Because of this, the CLT is at the heart of the theory of hypothesis testing and confidence intervals, and thus of much of classical statistics.\nFor experiments, this means that our estiamted treatment effect is normally distributed, which is what allows us to draw inferences from our experimental setting ot the population as a whole. The CLT is thus at the heart of the experimental approach.\nThe CLT also explains the prevalence of the normal distribution in the natural world. Many characteristics of living things we observe and measure are the sum of the additive effects of many genetic and environmental factors, so their distribution tends to be normal. –&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#standard-error",
    "href": "chapters/stats_foundations.html#standard-error",
    "title": "2  Statistics foundation",
    "section": "2.4 Standard error",
    "text": "2.4 Standard error\n\nThe standard error is a measure for the variability of the sampling distribution.\nIt is related to the standard deviation of the observations, \\(\\sigma\\) and the sample size \\(n\\) in the following way:\n\n\\[\nse = \\frac{\\sigma}{\\sqrt{n}}\n\\]\n\nThe relationship between sample size and se is sometimes called the “Square-root of n rule”, since reducing the \\(se\\) by a factor of 2 requires an increase in the sample size by a factor of 4.\n\nDerivation:\nThe sum of a sequence of independent random variables is:\n\\[\nT = (x_1 + x_2 + ... + x_n)\n\\]\nWhich has variance\n\\[\nVar(T) = Var(x_1) + Var(x_2) + ... + Var(x_n) = n\\sigma^2\\]\nand mean\n\\[\n\\bar{x} = T/n.\n\\]\nThe variance of \\(\\bar{x}\\) is then given by:\n\\[\nVar(\\bar{x}) = Var\\left(\\frac{T}{n}\\right) = \\frac{1}{n^2}Var(T) = \\frac{1}{n^2}n\\sigma^2 = \\frac{\\sigma^2}{n}.\n\\]\nThe standard error is defined as the standard deviation of \\(\\bar{x}\\), and is thus\n\\[\nse(\\bar{x}) = \\sqrt{Var(\\bar{x})} = \\frac{\\sigma}{\\sqrt{n}}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#bootstrap",
    "href": "chapters/stats_foundations.html#bootstrap",
    "title": "2  Statistics foundation",
    "section": "2.5 Bootstrap",
    "text": "2.5 Bootstrap\n\nIn practice, we often use the bootstrap to calculate standard errors of model parameters or statistics.\nConceptually, the bootstrap works as follows:\n\nwe draw an original sample and calculate our statistic\nwe then create a blown-up version of that sample by duplicating it many times\nwe then draw repeated samples from the large sample, recalculate our statistic, and calculate the standard deviation of these statistics to get the standard error.\n\nTo achieve this easily, we can skip step 2) by simply sampling with replacement from the original distribution in step 3).\nThe full procedure makes clear what the bootstrap results tell us, however: they tell us how lots of additional samples would behave if they were drawn from a population like our original sample.\nHence, if the original sample is not representative of the population of interest, then bootstrap results are not informative about that population either.\nThe bootstrap can also be used to improve the performance of classification or regression trees by fitting multiple trees on bootstrapped sample and then averaging their predictions. This is called “bagging”, short for “bootstrap aggregating”.\nWe can use to boostrap also to calculate CIs following this algorithm:\n\nDraw a large number of bootstrap samples and calculate the statistic of interest\nTrim [(100-x)/2] percent of the bootstrap results on either end of the distribution\nThe trim points are the end point of the CI.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#selection-bias",
    "href": "chapters/stats_foundations.html#selection-bias",
    "title": "2  Statistics foundation",
    "section": "2.6 Selection bias",
    "text": "2.6 Selection bias\nCommon types of selection bias in data science: - The vast search effect (using the data to answer many questions will eventually reveal something interesting by mere chance – if 20,000 people flip a coin 10 times, some will have 10 straight heads) - Nonrandom sampling - Cherry-picking data - Selecting specific time-intervals - Stopping experiments prematurely - Regression to the mean (occurs in settings where we measure outcomes repeatedly over time and where luck and skill combine to determine outcomes, since winners of one period will be less lucky next period and perform closer to the mean performer)\nWays to guard against selection bias: - have one or many holdout datasets to confirm your results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#standard-deviation-vs-standard-error",
    "href": "chapters/stats_foundations.html#standard-deviation-vs-standard-error",
    "title": "2  Statistics foundation",
    "section": "2.7 Standard deviation vs standard error",
    "text": "2.7 Standard deviation vs standard error\n\nStandard deviation is the spread of the distribution of the values in the population of interest\nStandard error is the spread of the distribution of a sample statistic (such as the mean) based on a random sample of population values.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#degrees-of-freedom",
    "href": "chapters/stats_foundations.html#degrees-of-freedom",
    "title": "2  Statistics foundation",
    "section": "2.8 Degrees of freedom",
    "text": "2.8 Degrees of freedom\nIn statistics, degrees of freedom generally refers to the number of values in a calculation that can vary freely.\nExamples:\n\nVariance calculation: given that we have a mean, once we know all but one value, we also know final value, since sum of mean deviations has to be zero.\nCovariance calculation: given the two means, once we know the values for all but one x and y pair, we also know the values of the final pair. Hence, we loose one df (not clear to me why not two, given that both x and y are determined – because we treat their product as a single value? but that seems arbitrary)\nAlso, why no correction when we have popultion means? See wikipedia article on variance for section on bias correction\nThere is lots of confusion out there when it comes to df. For instance, you sometimes hear people say that df is the number of parameters you had to calculate on route. But this is wrong. It happens to come to the same when calculating variance, but not if you calcualte covariance (where you calculate two means beforehand, but only loose one df).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#commonly-used-distributions",
    "href": "chapters/stats_foundations.html#commonly-used-distributions",
    "title": "2  Statistics foundation",
    "section": "2.9 Commonly used distributions",
    "text": "2.9 Commonly used distributions\nfrom here\n\n2.9.1 Commonly used probability distributions\nThe following table lists the variance for some commonly used probability distributions.\n\n\n\n\n\n\n\n\n\nName of the probability distribution\nProbability distribution function\nMean\nVariance\n\n\n\n\nBinomial distribution\n\\(\\Pr\\,(X=k) = \\binom{n}{k}p^k(1 - p)^{n-k}\\)\n\\(np\\)\n\\(np(1 - p)\\)\n\n\nGeometric distribution\n\\(\\Pr\\,(X=k) = (1 - p)^{k-1}p\\)\n\\(\\frac{1}{p}\\)\n\\(\\frac{1 - p}{p^2}\\)\n\n\nNormal distribution\n\\(f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\n\nUniform distribution (continuous)\n\\(f(x \\mid a, b) = \\begin{cases} \\frac{1}{b - a} & \\text{for } a \\le x \\le b, \\\\[3pt] 0 & \\text{for } x &lt; a \\text{ or } x &gt; b \\end{cases}\\)\n\\(\\frac{a + b}{2}\\)\n\\(\\frac{(b - a)^2}{12}\\)\n\n\nExponential distribution\n\\(f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}\\)\n\\(\\frac{1}{\\lambda}\\)\n\\(\\frac{1}{\\lambda^2}\\)\n\n\nPoisson distribution\n\\(f(k \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^{k}}{k!}\\)\n\\(\\lambda\\)\n\\(\\lambda\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#p-values-how-to-draw-statistical-conclusions",
    "href": "chapters/stats_foundations.html#p-values-how-to-draw-statistical-conclusions",
    "title": "2  Statistics foundation",
    "section": "2.10 p-values – how to draw statistical conclusions ?",
    "text": "2.10 p-values – how to draw statistical conclusions ?\nLimitations of reliying on pvap\n\nArbitrary cutoff\nNo appreciation for variation of coefficient – focus on ci instead (see Romer (2020), Imbens (2021))\nMultiple hypothesis testing (actual) – report and apply MHT-correction\nMultiple hypothesis testing (potential Gelman post)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#false-positive-rate-vs-false-discovery-rate",
    "href": "chapters/stats_foundations.html#false-positive-rate-vs-false-discovery-rate",
    "title": "2  Statistics foundation",
    "section": "2.11 False positive rate vs false discovery rate",
    "text": "2.11 False positive rate vs false discovery rate\n\nThe false positive rate is \\(P(significant result|no true effect)\\)\nThe false discovery rate is \\(P(no true effect|significant result)\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#confidence-interval-interpretations",
    "href": "chapters/stats_foundations.html#confidence-interval-interpretations",
    "title": "2  Statistics foundation",
    "section": "2.12 Confidence interval interpretations",
    "text": "2.12 Confidence interval interpretations\n\n95% CI for control and treatment overlap. Does this imply treatment is not significant?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html#sources",
    "href": "chapters/stats_foundations.html#sources",
    "title": "2  Statistics foundation",
    "section": "2.13 Sources",
    "text": "2.13 Sources\n\nPractical statistics for data science –&gt;\n\n\n\n\n\nImbens, Guido W. 2021. “Statistical Significance, p-Values, and the Reporting of Uncertainty.” Journal of Economic Perspectives 35 (3): 157–74.\n\n\nRomer, David. 2020. “In Praise of Confidence Intervals.” In AEA Papers and Proceedings, 110:55–60.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/regression.html#glossary",
    "href": "chapters/regression.html#glossary",
    "title": "3  Regression",
    "section": "3.1 Glossary",
    "text": "3.1 Glossary\nTODO: - Linear regression vs OLS"
  },
  {
    "objectID": "chapters/regression.html#the-setup",
    "href": "chapters/regression.html#the-setup",
    "title": "3  Regression",
    "section": "3.2 The setup",
    "text": "3.2 The setup\nWe usually start with data of the form \\(\\{y_i, x_{i1}, \\cdots, x_{ik}\\}_{i=1}^N\\), where we observe an outcome variable \\(y_i\\) and a set of \\(k\\) explanatory variables \\(x_i = (x_{i1}, \\cdots, x_{ik})\\) for each unit \\(i\\) in the dataset. We think that it might be reasonable to think of the outcome being linearly related to the regressors, so that, for each unit in our dataset, we can write the following linear equation:\n\\[\ny_{i} = \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + ... + \\beta_{k}x_{ik} + \\epsilon_{i}\n\\]\nThis says that the outcome \\(y\\) can be thought of as a linear combination of all explanatory variables plus some error term.\nTODO: What makes this a “linear” equation: - The highest power to which any regressor is raised is 1 - The coefficients are constants, not variables - Regressors are related to one another using addition and subtraction only - The resulting line (in 2-D space), plane (in 3-D space) and hyperplane (in N-D space) are linear (the term linear equation originates from the simple case where there are two regressors, one of which is a constant, in which case we get a straight line in a Cartesian plane.)\nTODO: discuss all the assumptions we’re making here.\nTODO: Discuss the Angist & Pischke view of linear regression being good approximation even if relationship is not linear.\nWe thus have a system of linear equations of the form\n\\[\n\\begin{aligned}\ny_1 = \\beta_0 + \\beta_1 x_{1_1} + \\beta_2 x_{2_1} + \\ldots + \\beta_k x_{k_1} + \\epsilon_1 \\\\\ny_2 = \\beta_0 + \\beta_1 x_{1_2} + \\beta_2 x_{2_2} + \\ldots + \\beta_k x_{k_2} + \\epsilon_2 \\\\\n\\vdots\\\\\ny_n = \\beta_0 + \\beta_1 x_{1_n} + \\beta_2 x_{2_n} + \\ldots + \\beta_k x_{k_n} + \\epsilon_n \\\\\n\\end{aligned}\n\\]\nwhich we can rewrite in vector notation as\n\\[\n\\begin{aligned}\ny_1 = x_1'\\beta + \\epsilon_1 \\\\\ny_2 = x_2'\\beta + \\epsilon_2 \\\\\n\\vdots\\\\\ny_n = x_n'\\beta + \\epsilon_n,\n\\end{aligned}\n\\]\nwhere \\[\nx_i' = (x_{i1}, x_{i2}, \\ldots, x_{ik})\n\\]\nis a \\(1 \\times k\\) row vector that contains all \\(k\\) explanatory variables for each unit \\(i\\) and \\[ \\beta =\n\\begin{pmatrix}\n  \\beta_{1}\\\\\n  \\beta_{2}\\\\\n  \\vdots \\\\\n  \\beta_{k}\n\\end{pmatrix}\n\\]\nis a \\(k \\times 1\\) column vector that contains all \\(k\\) regression coefficients.\nTo be even more succinct, we can stack all n equations to get the matrix notation: \\[\ny = X\\beta + \\epsilon,\n\\]\nwhere \\(\\beta\\) is defined as above,\n\\[ y =\n\\begin{pmatrix}\n  y_{1}\\\\\n  y_{2}\\\\\n  \\vdots \\\\\n  y_{k}\n\\end{pmatrix}\n\\]\nis a \\(n \\times 1\\) vector containing the \\(n\\) outcome variables, one for each unit in the data,\n\\[ X =\n\\begin{pmatrix}\n  x_{1}' \\\\\n  x_{2}' \\\\\n  \\vdots \\\\\n  x_{n}''\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  x_{1,1} & x_{1,2} & \\cdots & x_{1,k} \\\\\n  x_{2,1} & x_{2,2} & \\cdots & x_{2,k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  x_{n,1} & x_{n,2} & \\cdots & x_{n,k}\n\\end{pmatrix}\n\\]\nis an \\(n \\times k\\) matrix that contains all \\(n\\) row vectors \\(x_i'\\) stacked on top of each other, and\n\\[ \\epsilon =\n\\begin{pmatrix}\n  \\epsilon_{1}\\\\\n  \\epsilon_{2}\\\\\n  \\vdots \\\\\n  \\epsilon_{k}\n\\end{pmatrix}\n\\]\na column vector containing the \\(n\\) error terms.\n## The problem\nSo, what do we want to do here? We have data on an outcome variable \\(y\\) and explanatory variables \\(x\\) for each unit \\(i\\), and we think that it is reasonable to think that this data is generated by a process whereby \\(y\\) is the result of a linear combination of the \\(x\\)s plus some noise, which we capture in the error term. The challenge is to find the right linear combination."
  },
  {
    "objectID": "chapters/regression.html#classic-motivation-of-the-solution",
    "href": "chapters/regression.html#classic-motivation-of-the-solution",
    "title": "3  Regression",
    "section": "3.3 Classic motivation of the solution",
    "text": "3.3 Classic motivation of the solution\nTODO"
  },
  {
    "objectID": "chapters/regression.html#linear-algebra-motivation-of-the-solution",
    "href": "chapters/regression.html#linear-algebra-motivation-of-the-solution",
    "title": "3  Regression",
    "section": "3.4 Linear algebra motivation of the solution",
    "text": "3.4 Linear algebra motivation of the solution\nNotice how our problem here is exactly akin to the motivation for orthogonal projection discussed in Chapter 20. There we had a system of linear equations of the form\n\\[\nAx = b\n\\]\nwhich was overdetermined because the number of equations exceeded the number of unknowns. Our setup is equivalent. We have \\(n\\) equations and \\(k\\) unknowns (the \\(\\beta\\)s), so that – in practice – there will be no solution to the system:\n\\[\nX\\beta = y.\n\\]\nIn other words, there is no choice of \\(\\beta\\) that would linearly combine all the explanatory variables in each equation such that the result would be exactly \\(y\\). We account for this by adding the error term \\(\\epsilon\\), so that we have\n\\[\nX\\beta + \\epsilon = y.\n\\]\nWhat we do, now, is to say that we want to find that linear combination of the explanatory variables that is closest to \\(y\\), so that \\(\\epsilon\\) is as small as possible, which is the same as finding the orthogonal projection of \\(y\\) onto \\(X\\), which, traditionally, we call \\(\\hat{y}\\).\nThe solution is then the same as in Chapter 20:1\n\\[\n\\begin{aligned}\nX'\\epsilon &= 0 \\\\\nX'(y - \\hat{y}) &= 0 \\\\\nX'(y - X\\beta) &= 0 \\\\\nX'y - X'X\\beta &= 0 \\\\\nX'X\\beta &= X'y \\\\\n\\beta &= (X'X)^{-1}X'y\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/regression.html#resources",
    "href": "chapters/regression.html#resources",
    "title": "3  Regression",
    "section": "3.5 Resources",
    "text": "3.5 Resources\n\nHayashi, Wooldridge, Verbeek, online resources"
  },
  {
    "objectID": "chapters/regression.html#assumptions",
    "href": "chapters/regression.html#assumptions",
    "title": "3  Regression",
    "section": "3.6 Assumptions",
    "text": "3.6 Assumptions\n\nDirectly copied from or heavily based on https://people.duke.edu/~rnau/testing.htm. Edit and expand over time.\nIn short, the assumptions are that the model is linear and additive, and that errors are iid. The latter assumption is often stated as three separate assumptions, as shown below.\nNote: The dependent and independent variables in a regression model do not need to be normally distributed by themselves–only the prediction errors need to be normally distributed. (In fact, independent variables do not even need to be random, as in the case of trend or dummy or treatment or pricing variables.) But if the distributions of some of the variables that are random are extremely asymmetric or long-tailed, it may be hard to fit them into a linear model whose errors will be normally distributed, and explaining the shape of their distributions may be an interesting topic all by itself. Keep in mind that the normal error assumption is usually justified by appeal to the central limit theorem, which holds in the case where many random variations are added together. If the underlying sources of randomness are not interacting additively, this argument fails to hold.\n\nThere are four principal assumptions which justify the use of linear regression models for purposes of inference or prediction:\n\nlinearity and additivity of the relationship between dependent and independent variables:\n\nThe expected value of dependent variable is a straight-line function of each independent variable, holding the others fixed.\nThe slope of that line does not depend on the values of the other variables.\nThe effects of different independent variables on the expected value of the dependent variable are additive.\n\nstatistical independence of the errors (in particular, no correlation between consecutive errors in the case of time series data)\nhomoscedasticity (constant variance) of the errors\n\n\nversus time (in the case of time series data)\nversus the predictions\nversus any independent variable\n\n\nnormality of the error distribution.\n\nIf any of these assumptions is violated, then the forecasts, confidence intervals, and scientific insights yielded by a regression model may be (at best) inefficient or (at worst) seriously biased or misleading.\n\n3.6.1 Linearity and additivity\nWhy assume linearity?\n\nIt’s the simplest non-trivial relationship and the easiest to work with\nTrue relationships are often at least approximately linear for ranges we care about\nEven if above not true, we can often transform variables so that relationshiop is linear\n\nHow to test linearity\n\nLook at scatterplot before running regression\nLook at predicted values vs residual plot after running regression (linear model leads to evenly distributed dots across a horizontal line) – can look at predicted vs observed value plot, too, in which case you expect points to fall on the 45 degree line, but this diagonal introduces additional visual noise.\n\nWhat if linearity is violated?\n\nAdd non-linear transformation like log (if data is strictly positive)\nAdd regressor that is a non-liner function of a current regressor (e.g. x^2)\nAdd new regressor that explains non-linear nature of current relationshiop\n\n\n\n3.6.2 iid errors\nWhy assume iid errors?\n\nIn many cases, it’s justified by the CLT. As long as we calculate averages (sums or random variables, really), which are independent and identically distributed, the distribution will be approximately normal.\nIt’s convenient because it implies that the optimal coefficient estimates for linear models minimise the MSE, and because it justified using tests based on the normal family (t, F, Chi-square distribution).\nEven if the “true” error process is not normal in terms of the original units of the data, it may be possible to transform the data so that your model’s prediction errors are approximately normal.\nNote that normality is required for inferenence, not for parameter estimation or prediction.\n\nPotential issues\n\nHeteroskedasticity (variance larger depending on come conditions – certain covariate values)\nInterdependence (e.g. in time-series models, in the context of network effects)\n\nHow to check assumption\n\nIndependence\n\nIn time-series: look at residual autocorrelation\nIn non-time series: plot independent variables vs residuals – independence suggests symmetric distribution around zero and no dependence in subsequent residuals under any ordering that is not based on the independent variables (i.e. no correlation given covariates)\n\nHomoskedasticity\n\nTime series: plot time vs residuals\nNon-time series: plot predicted values vs residuals and independent variables vs residuals\n\nNormality\n\nNormal probability plot and normal quantile plot\nCould also use formal tests (Kolmogorov-Smirnov test, Shapiro-Wilk test, Jarque-Bera test, and the Anderson-Darling test), but they are often too restrictive in practice.\n\n\nHow to fix violations\n\nIndependence\n\nTime-series: for mild violations (Durbin-Watson between 1.2 and 1) adding lags, for serious violations (DW &gt; 2.6) use difference model\nNon-time series: either due to non-linearity of model or due to omitted variable.\n\nHomoskedasticity\n\nLog transformation\nModel seasonality with dummies (do linearly or multiplicatively (take log of dependent variable))\nSeasonally adjust data before fitting model\n\nNormality\n\nViolations of normality often arise either because (a) the distributions of the dependent and/or independent variables are themselves significantly non-normal, and/or (b) the linearity assumption is violated. In such cases, a nonlinear transformation of variables might cure both problems.\nAnother possibility is that there are two or more subsets of the data having different statistical properties, in which case separate models should be built, or else some data should merely be excluded, provided that there is some a priori criterion that can be applied to make this determination.\nIn some cases, the problem with the error distribution is mainly due to one or two very large errors. Such values should be scrutinized closely: are they genuine (i.e., not the result of data entry errors), are they explainable, are similar events likely to occur again in the future, and how influential are they in your model-fitting results? If they are merely errors or if they can be explained as unique events not likely to be repeated, then you may have cause to remove them. In some cases, however, it may be that the extreme values in the data provide the most useful information about values of some of the coefficients and/or provide the most realistic guide to the magnitudes of forecast errors."
  },
  {
    "objectID": "chapters/regression.html#footnotes",
    "href": "chapters/regression.html#footnotes",
    "title": "3  Regression",
    "section": "",
    "text": "One thing I would always wonder about in textbook is how I knew that the condition was \\(X'\\epsilon\\) instead of \\(\\epsilon'X\\). The answer is that in cases where order doesn’t matter, texts tend to choose what is more convenient for the math. We could solve \\(\\epsilon'X\\): \\[\n\\begin{aligned}\n\\epsilon'X &= 0 \\\\\n(y - \\hat{y})'X &= 0 \\\\\n(y - X\\beta)'X &= 0 \\\\\n(y' - \\beta'X')X &= 0 \\\\\ny'X - \\beta'X'X &= 0 \\\\\n\\beta'X'X &= y'X \\\\\n(\\beta'X'X)' &= (y'X)' \\\\\nX'X\\beta &= X'y \\\\\n\\beta &= (X'X)^{-1}X'y\n\\end{aligned}\n\\] which gets us to the same result but in more steps.↩︎"
  },
  {
    "objectID": "chapters/neyman_rubin_causal_model.html#potential-outcomes",
    "href": "chapters/neyman_rubin_causal_model.html#potential-outcomes",
    "title": "4  Neyman-Rubin causal model",
    "section": "4.1 Potential outcomes",
    "text": "4.1 Potential outcomes"
  },
  {
    "objectID": "chapters/neyman_rubin_causal_model.html#causal-estimands",
    "href": "chapters/neyman_rubin_causal_model.html#causal-estimands",
    "title": "2  Neyman-Rubin causal model",
    "section": "2.2 Causal estimands",
    "text": "2.2 Causal estimands\n\nWe have a population of units \\(i = 1, \\dots, N\\).\nEach unit in the population can be exposed to one of two treatments, which are identical across units, so that \\(\\mathbb{T}_i = \\mathbb{T} = \\{0, 1\\}\\).\nEach unit \\(i\\) has potential outcomes \\(Y_i(0)\\) and \\(Y_i(1)\\) corresponding to each of the two possible treatments.\nUnit-level causal effects are given by comparisons of \\(Y_i(0)\\) and \\(Y_i(1)\\), often expressed as a simple difference:\n\n\\[\nY_i(1) - Y_i(0).\n\\]\n\nWe often want to summarise unit-level treatment effects, to which effect we can calculate many different causal estimands.\nThe average treatment effect over the entire population (the finite sample) is defined as:\n\n\\[\n\\tau_{fs} = \\frac{1}{N}\\sum_{i=1}^N \\left(Y_i(1) - Y_i(0)\\right).\n\\]\n\nHence: the causal effect is the average difference in individual potential outcomes.\nWe can generalise this in a number of ways.\nWe can focus only on a subset of the population, which can also happen in different ways.\nWe can condition on covariates, such as when we focus only on women: \\[\n\\tau_{fs, f} = \\frac{1}{N_f}\\sum_{i: X_i = f} \\left(Y_i(1) - Y_i(0)\\right), \\quad \\text{where $X_i = \\{m , f\\}$ and $N_f = \\sum_{i=1}^N \\mathbb{1}_{X_i = f}$}\n\\]\nWe can condition on treatment status, such as when we focus only on units that were exposed to the treatment:\n\n\\[\n\\tau_{fs, t} = \\frac{1}{N_t}\\sum_{i: W_i = 1} \\left(Y_i(1) - Y_i(0)\\right), \\quad \\text{where $W_i = \\{0 , 1\\}$ and $N_t = \\sum_{i=1}^N \\mathbb{1}_{W_i = 1}$}\n\\]\n\nWe can condition on potential outcomes, such as when we focus only on units with positive potential outcomes (e.g. positive earnings) regardless of treatment status:\n\n\\[\n\\tau_{fs, pos} = \\frac{1}{N_{pos}}\\sum_{i: Y_i(0)&gt;0, Y_i(1)&gt;0} \\left(Y_i(1) - Y_i(0)\\right), \\quad \\text{where $N_{post} = \\sum_{i=1}^N \\mathbb{1}_{Y_i(0)&gt;0, Y_i(1)&gt;0}$}\n\\]\n\nWe can also generalise the estimand by focusing on more general functions of the potential outcomes (e.g. we may focus on the median outcome of the entire population or a subpopulation).\nIn all these cases, we can write the causal estimand as a row-exchangeable function (a function that takes vectors or matrices as arguments and the result of which does not change if the rows in its input are permuted):\n\n\\[\n\\tau = \\tau(Y(0), Y(1), X, W)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Neyman-Rubin causal model</span>"
    ]
  },
  {
    "objectID": "chapters/neyman_rubin_causal_model.html#stable-unit-treatment-value-assumption-sutva",
    "href": "chapters/neyman_rubin_causal_model.html#stable-unit-treatment-value-assumption-sutva",
    "title": "2  Neyman-Rubin causal model",
    "section": "2.3 Stable unit treatment value assumption (SUTVA)",
    "text": "2.3 Stable unit treatment value assumption (SUTVA)\n\nSUTVA has two components: no interference, and no hidden variations of treatments.\n\nNo interference\n\nThe no interference assumption states that a unit’s potential outcumes are independent of the treatment assignment of all other units.\nThis will be violated if there are network effects: if the behaviour of units is mutually dependent. For instance: if my wife and I both use an online photo-sharing service and my wife sees a new feature that we both like while I’m in the control group, we might stil share the same number of family photos but start sharing them all on her account instead of mine. This creates an artificial treatment effect because if I had also had access to the new feature, we might not have changed our behaviour at all, while, during the experiment, her sharing volume went up while mine went down, suggesting the existent of a positive effect.\nAnother case where the no interference assumption can be violated is in the form of general equilibrium effects. A classic example is the effect of further education: the effect of my doing a PhD in statistics on my earnings while nobody else changes their behaviour (the partial-equilibrium effect) is surely different from the outcome of my earnings if suddenly everyone decided to do a PhD in statistics (the general-equilibrium effect).\nThe two violations capture the two different ways interference can lead to incorrect results: interference can happen and bias our results either during the experiment or once the feature is fully rolled out. In either case, the treatment of some unit has an externality on other units.\n\nNo hidden treatment variations\n\nThe second component, no hidden treatment variation, states that a unit receiving a specific treatment level cannot receive different forms of that treatment level. This does not mean that the form of the treatment level has to be the same for each unit, but only that a given treatment level is well specified for a given unit. To use Imbens and Rubin’s aspirin example: suppose we test the effect of aspirin on reducing headaches but have old and new aspirins which vary in strength, so that we effectively have three possible treatment statuses: no aspirin (control), weak aspirin, and strong aspirin. SUTVA does not require that all treatment units either get the weak or the strong aspirin, but requires that each unit can only receive one or the other in case they are treated, so that there is no ambiguity what form of the treatment a given unit will receive in case it is treated. (It would be permissible to have the treatment be randomly weak or strong, but this is not relevant in my world.)\nEssentially, both parts of SUTVA ensure the same thing: that \\(Y_i(w)\\) is well defined: that it does not depend on the treatment status of other units, and that, for each possible treatment level, \\(w\\), the precise form of that treatment level is well specified.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Neyman-Rubin causal model</span>"
    ]
  },
  {
    "objectID": "chapters/neyman_rubin_causal_model.html#the-assignment-mechanism",
    "href": "chapters/neyman_rubin_causal_model.html#the-assignment-mechanism",
    "title": "4  Neyman-Rubin causal model",
    "section": "4.4 The assignment mechanism",
    "text": "4.4 The assignment mechanism"
  },
  {
    "objectID": "chapters/neyman_rubin_causal_model.html#finite-sample-vs-super-population-perspective",
    "href": "chapters/neyman_rubin_causal_model.html#finite-sample-vs-super-population-perspective",
    "title": "2  Neyman-Rubin causal model",
    "section": "2.4 Finite sample vs super-population perspective",
    "text": "2.4 Finite sample vs super-population perspective\nThere are two ways to perform inference:\n\nWhen using the finite sample perspective, the \\(N\\) units in the experiment sample are treated as the entire population of interest. Hence, there is no notion of randomisation due to sampling from a larger population, and all the randomness in the outcomes is due to randomness generates by the assignment mechanism.\nWhen treating the \\(N\\) units in the sample as a random sample from a larger super-population, then in addition to randomness from the randomisation, we also have randomness from the sampling.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Neyman-Rubin causal model</span>"
    ]
  },
  {
    "objectID": "chapters/neyman_rubin_causal_model.html#useful-resources",
    "href": "chapters/neyman_rubin_causal_model.html#useful-resources",
    "title": "2  Neyman-Rubin causal model",
    "section": "2.6 Useful resources",
    "text": "2.6 Useful resources",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Neyman-Rubin causal model</span>"
    ]
  },
  {
    "objectID": "chapters/fisher.html#a-simple-example",
    "href": "chapters/fisher.html#a-simple-example",
    "title": "3  Fisher’s exact P-value approach",
    "section": "",
    "text": "\\(Y_i(0)\\)\n\\(Y_i(1)\\)\n\\(W_i\\)\n\\(Y_i^{obs}\\)\n\n\n\n\n1\n?\n3\n1\n3\n\n\n2\n?\n5\n1\n5\n\n\n3\n?\n0\n1\n0\n\n\n4\n4\n?\n0\n4\n\n\n5\n0\n?\n0\n0\n\n\n6\n1\n?\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n\\(Y_i(0)\\)\n\\(Y_i(1)\\)\n\\(W_i\\)\n\\(Y_i^{obs}\\)\n\n\n\n\n1\n(3)\n3\n1\n3\n\n\n2\n(5)\n5\n1\n5\n\n\n3\n(0)\n0\n1\n0\n\n\n4\n4\n(4)\n0\n4\n\n\n5\n0\n(0)\n0\n0\n\n\n6\n1\n(1)\n0\n1",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fisher's exact P-value approach</span>"
    ]
  },
  {
    "objectID": "chapters/fisher.html#useful-resources",
    "href": "chapters/fisher.html#useful-resources",
    "title": "3  Fisher’s exact P-value approach",
    "section": "3.2 Useful resources",
    "text": "3.2 Useful resources\nImbens and Rubin (2015)\n\n\n\n\nImbens, Guido W, and Donald B Rubin. 2015. Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fisher's exact P-value approach</span>"
    ]
  },
  {
    "objectID": "chapters/neyman.html#estimator-for-the-average-treatment-effect",
    "href": "chapters/neyman.html#estimator-for-the-average-treatment-effect",
    "title": "6  Neyman’s repeated sampling approach",
    "section": "6.1 Estimator for the average treatment effect",
    "text": "6.1 Estimator for the average treatment effect\nIn a setting with \\(i = 1, \\dots, N\\) units with fixed potential outcomes \\(Y_i(0)\\) and \\(Y_i(1)\\), where the only random component is the random assignment, captured by the assignment vector \\(W\\)1, Neyman was interested in the population average treatment effect:\n\\[\n\\tau_{fs} = \\frac{1}{N}\\sum_{i=1}^N \\left(Y_i(1) - Y_i(0)\\right) = \\bar{Y}(1) - \\bar{Y}(0),\n\\tag{6.1}\\]\nwhere\n\\[\n\\bar{Y}(1) = \\frac{1}{N}\\sum_{i=1}^N Y_i(1) \\qquad \\bar{Y}(0) = \\frac{1}{N}\\sum_{i=1}^N Y_i(0).\n\\]\nThis is our estimand of interest.\nIf we have data from a completely randomised experiment in which \\(N_t = \\sum_{i=1}^N W_i\\) units are allocated to treatment and the remaining \\(N_c = \\sum_{i=1}^N (1-W_i)\\) to control, then a natural estimator for Equation 6.1 is the difference in the averages of the treatment and control units:\n\\[\n\\hat{\\tau}^{dif} = \\bar{Y}_t^{obs} - \\bar{Y}_c^{obs}.\n\\tag{6.2}\\]\nwhere\n\\[\n\\bar{Y}_t^{obs} = \\frac{1}{N_t}\\sum_{i:W_i=1} Y_i^{obs} \\qquad \\bar{Y}_c^{obs} = \\frac{1}{N_c}\\sum_{i:W_i=0} Y_i^{obs}\n\\]\nThis estimator is unbiased (see Proof of Theorem 6.1 in Imbens and Rubin (2015) for the proof)."
  },
  {
    "objectID": "chapters/neyman.html#estimator-of-the-variance-of-the-average-treatment-effect-estimator",
    "href": "chapters/neyman.html#estimator-of-the-variance-of-the-average-treatment-effect-estimator",
    "title": "4  Neyman’s repeated sampling approach",
    "section": "4.2 Estimator of the variance of the average treatment effect estimator",
    "text": "4.2 Estimator of the variance of the average treatment effect estimator\nEstimating the variance of the average treatment estimator \\(\\hat{\\tau}^{dif}\\) involves two steps:\n\nDerive the sampling variance of the estimator for the average treatment effect (i.e. for the estimator defined in Equation 4.2).\nDevelop estimators for this sampling variance.\n\nThe sampling variance of \\(\\hat{\\tau}^{dif}\\) is (see Theorem 6.2 in Imbens and Rubin (2015)):\n\\[\n\\mathbb{V}_W \\left(\\bar{Y}_t^{obs} - \\bar{Y}_c^{obs}\\right)\n= \\frac{S_c^2}{N_c} + \\frac{S_t^2}{N_t} - \\frac{S_{ct}^2}{N},\n\\tag{4.3}\\]\nwhere:\n\\[\n\\begin{align}\nS_c^2 &= \\frac{1}{N - 1}\\sum_{i=1}^{N}\\left(Y_i(0) - \\bar{Y}(0)\\right)^2 \\\\\nS_t^2 &= \\frac{1}{N - 1}\\sum_{i=1}^{N}\\left(Y_i(1) - \\bar{Y}(1)\\right)^2 \\\\\nS_{ct}^2 &= \\frac{1}{N - 1}\\sum_{i=1}^{N}\\left(Y_i(1) - Y_i(0) - (\\bar{Y}(1) - \\bar{Y}(0))\\right)^2 \\\\\n&= \\frac{1}{N - 1}\\sum_{i=1}^{N}\\left(Y_i(1) - Y_i(0) - \\tau_{fs}\\right)^2 \\\\\n\\end{align}\n\\]\nIf the unit level treatment effects \\(Y_(1) - Y_i(0)\\) are constant, then the below is an unbiased estimator for the sampling variance (see Theorem 6.3 in Imbens and Rubin (2015)):\n\\[\n\\hat{\\mathbb{V}}^{neyman} = \\frac{s_c^2}{N_c} + \\frac{s_t^2}{N_t},\n\\tag{4.4}\\]\nwhere:\n\\[\ns_c^2 = \\frac{1}{N_c - 1}\\sum_{i:W_i=0}\\left(Y_i(0) - \\bar{Y}_c^{obs}\\right)^2 \\qquad s_t^2 = \\frac{1}{N_t - 1}\\sum_{i:W_i=1}\\left(Y_i(1) - \\bar{Y}_t^{obs}\\right)^2\n\\]\nThere are other estimators, but \\(\\hat{\\mathbb{V}}^{neyman}\\) is the most commonly used because:\n\nIt is conservative (because it ignores the last term in Equation 4.3)\nIt is always an unbiased estimator of Equation 4.3 under the super-population perspective",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neyman's repeated sampling approach</span>"
    ]
  },
  {
    "objectID": "chapters/neyman.html#further-notes",
    "href": "chapters/neyman.html#further-notes",
    "title": "4  Neyman’s repeated sampling approach",
    "section": "4.3 Further notes",
    "text": "4.3 Further notes\n\nThe Neyman approach can incorporate discrete covariates by partitioning sample into subgroups, calculating subgroup treatment effects, and aggregating the subgroup treatment effects using a subgroup-size weighted average.\nHowever, it cannot handle cases where there are covariate values for which only treatment or control units are observed. In this case, we need to build a model for potential outcomes (e.g. by using regression analysis).\n\n\n\n\n\nImbens, Guido W, and Donald B Rubin. 2015. Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neyman's repeated sampling approach</span>"
    ]
  },
  {
    "objectID": "chapters/neyman.html#footnotes",
    "href": "chapters/neyman.html#footnotes",
    "title": "4  Neyman’s repeated sampling approach",
    "section": "",
    "text": "Note that this is the same setting as in Fisher’s approach↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neyman's repeated sampling approach</span>"
    ]
  },
  {
    "objectID": "chapters/guiding_principles.html",
    "href": "chapters/guiding_principles.html",
    "title": "7  Guiding principles",
    "section": "",
    "text": "Remember Twyman’s Law: if a result looks interesting it is probably a mistake – don’t invent a story, investigate!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Guiding principles</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_design.html#designs",
    "href": "chapters/experiment_design.html#designs",
    "title": "8  Experiment design",
    "section": "8.1 Designs",
    "text": "8.1 Designs\n\nSee BIT blue book section 5\n\nClassic RCT\nClustered\nStep-wedge / phase-in design\n\nMight allow us to estimate long-term effects. If we focus on a specific age-group or cohort. Because then certain people will never receive the treatment and can thus act as a long-run control group. (Example: if we focus on kids in their last year of school and phase-in periods are a year, then once we get to the third school, two cohorts will have graduated and will never receive the program.)\n\nWait-list design\nFactorial design\nEncouragement design\n\nIt measures the effect of those who took up the program because of the encouragement but would not have otherwise (LATE). So it’s particularly helpful if we are interested in the effect of encouragement on marginal participants.\nThe encouragement must not encourage some and discourage others (monotonicity assumption).\n\nWhat’s the difference between a “between-subjects design” and a “within-subjects design” and what are their advantages? What is a “mixed-design”?\n\nA between design compares the treatment group to a control group, a within design compares the treatment group to itself before the treatment, so that each participant acts as their own control group.\nThe strength of a between design is that it protects against confounding factors, that of a within design that it improves precision (because we have baseline data).\nA mixed design is where we have a control group and baseline data. If we use that baseline, the estimator is then a difference-in-difference estimator, which is statistically more efficient."
  },
  {
    "objectID": "chapters/experiment_design.html#choosing-number-of-units-per-arm",
    "href": "chapters/experiment_design.html#choosing-number-of-units-per-arm",
    "title": "8  Experiment design",
    "section": "8.5 Choosing number of units per arm",
    "text": "8.5 Choosing number of units per arm\n\nShow that equal split maximises power (use power formula and show that power is max for p = 1-p )\nIf we have many treatment groups, then more units should be allocated to control to estimate its effect more precisely (add formula)\nif sample variances differ, the ratio of the sample sizes should equal the ratio of the standard deviations."
  },
  {
    "objectID": "chapters/experiment_design.html#hypothesis-formulation",
    "href": "chapters/experiment_design.html#hypothesis-formulation",
    "title": "8  Experiment design",
    "section": "8.6 Hypothesis formulation",
    "text": "8.6 Hypothesis formulation"
  },
  {
    "objectID": "chapters/experiment_design.html#qa",
    "href": "chapters/experiment_design.html#qa",
    "title": "8  Experiment design",
    "section": "8.11 Q&A",
    "text": "8.11 Q&A\nQuestions:\n\nWhy don’t we include MDE in our hypothesis statement (i.e. “a will increase b by at least MDE for target population”)? Wouldn’t that be a good idea in a business context where we strongly care about an MDE? Don’t we just not do that because academics, who developed the methods, don’t usually care about an effect being of a certain size but only about learning what the effect is?\n\nAnswers:\n\nIncluding the MDE’s in the hypothesis would be incorrect. Why? Because what we formulate is the alternative hypothesis. Without altering the null hypothesis, which traditionally states the effect is zero, our two hypotheses would not be complete (H0 testing effect different from 0, HA asserting that effect is at least MDE means rejecting H0 would not imply support of HA). So why not adapt H0, too? Because the hypothesis is the thing we test when we calculate our test statistic (duh!), and so if included the MDE in the hypothesis formulation, then we’d have to include it in the calculation of the test statistic (subtract it from the observed difference), which would be incorrect. Why? The question confused the MDE with the effect size under the Null hypothesis, which are conceptually different things: the MDE is the smallest effect size we want to detect – we are interested in effects that are as large or larger than the MDES, we’re not testing whether the observed difference is significantly different from it, which is what the null hypothesis value is."
  },
  {
    "objectID": "chapters/experiment_stats.html",
    "href": "chapters/experiment_stats.html",
    "title": "9  Statistics of online experiments",
    "section": "",
    "text": "Our estimate of the average treatment effect is\n\\[\n\\hat{\\tau} = \\bar{Y}^{obs}_t - \\bar{Y}^{obs}_c,\n\\]\nwhere\n\\[\n\\bar{Y}_t^{obs} = \\frac{1}{N_t}\\sum_{i:W_i=1} Y_i^{obs} \\qquad \\bar{Y}_c^{obs} = \\frac{1}{N_c}\\sum_{i:W_i=0} Y_i^{obs}.\n\\]\nThis is an unbiased estimate of \\(\\tau = \\bar{Y(1)} - \\bar{Y(0)}\\) (see Proof of Theorem 6.1 in Imbens and Rubin (2015)).\nBecause treatment and control units are independently and randomly assigned, an unbiased estimate of the sampling variance of \\(\\hat{\\tau}\\) is given by[^vardetails]:\n\\[\nVar(\\hat{\\tau}) = \\frac{s^2_t}{N_t} + \\frac{s^2_c}{N_c},\n\\]\nwhere\n\\[\ns_c^2 = \\frac{1}{N_c - 1}\\sum_{i:W_i=0}\\left(Y_i(0) - \\bar{Y}_c^{obs}\\right)^2 \\qquad s_t^2 = \\frac{1}{N_t - 1}\\sum_{i:W_i=1}\\left(Y_i(1) - \\bar{Y}_t^{obs}\\right)^2.\n\\]\nTo test whether the observed treatment effect is significantly different from zero, we test:\n\\[\nH_0: \\bar{Y}^{obs}_t = \\bar{Y}^{obs}_c\nH_A: \\bar{Y}^{obs}_t \\neq \\bar{Y}^{obs}_c.\n\\]\nWe calculate the test-statistic\n\\[\nT = \\frac{\\hat{\\tau}}{\\sqrt{Var(\\hat{\\tau})}} \\sim t_{(N_t + N_c - 2)},\n\\]\nwhere \\(N_t + N_c - 2\\) is number of degrees of freedom[^tdetails].\ntodo:\n\nSee Rice 6.2 on why this follows t distribution\nFor complete treatment and derivation of sampling distributions (incl. discussion of all the approximations and sample corrections), see Rice sampling chapter and my ipad notes.\n\n[^vardetails] We gloss over a few details here. Depending on the treatment assignment mechanism, units are not independently assigned. In online experiments, we often use hash functions for treatment assignment, in which case assignment is independent. But if we use the assignment mechanism of a completely randomised experiment, where we perfectly balance the number of units assigned to each treatment, each unit being assigned to one variant lowers the probability of all other units being assigned to that same variant. In that case, the above variance estimator is unbiased only if the treatment effect is constant. However, it is usually used in practice even in cases where constant treatment effects are unlikely because it is conservative (i.e. at least as large as the true variance) and because it is always unbiased if the ATE is an estimate of the infinite super-population ATE.\n[^tdetails] Note that the test statistic follows a t-distribution because we have to estimate the variance (that is, if we replace the true variance with its estimate when standardising a normal variable, the result follows a Student’s t-distribution). So, this has nothing to do with the CLT. However, for the test statistic to follow a student distribution, the numerator has to follow a normal distribution. Often, though, the underlying data is not normal, so that its approximately normal only for large enough samples, due to the CLT. At the same time, the t-distribution also converges to normal as the sample size increases. Hence, one we have a sample size large enough to justify using the t-distribution, we might as well use a z-test. As pointed out in Chapter 9 in Rice (2006), the test statistic above only follows a t-distribution if we use the pooled variance, but for large sample sizes, the distribution is still approximately t or normal.\n\n\n\n\nImbens, Guido W, and Donald B Rubin. 2015. Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press.\n\n\nRice, John A. 2006. Mathematical Statistics and Data Analysis. Cengage Learning."
  },
  {
    "objectID": "chapters/metrics.html#why-good-metrics-matter",
    "href": "chapters/metrics.html#why-good-metrics-matter",
    "title": "5  Metrics",
    "section": "",
    "text": "Good metrics ensure that everyone works towards the same goal in a way that is reliable, transparent, and provides accountability – they ensure coherence across the company.\nGood metrics increase the probability that our evaluations detect a change if there is one – they have high sensitivity.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#metric-taxonomy",
    "href": "chapters/metrics.html#metric-taxonomy",
    "title": "5  Metrics",
    "section": "5.2 Metric taxonomy",
    "text": "5.2 Metric taxonomy\n\nDifferent contributions in the literature and different companies use different ways to classify metrics. So so the same type of metrics will have different names and the same name will be used for different types of metrics across contexts. What matters is less the labels, but an understanding of the different functions metric can serve and how different types of metric relate to one another.\nKohavi, Tang, and Xu (2020) classify metrics into goal metrics, driver metrics, and guardrail metrics. I find this useful and use it as the basis of how I think about metrics, but also add supporting and debug metrics.\nWhile I talk about these metrics below mainly as defined at the company level, they can be defined at each level within an organisation.\nThe challenge is to make sure that definitions across metric types and organisation levels cohere. Figure 6.1 in Kohavi, Tang, and Xu (2020) is a useful way to visualise this: it shows a large arrow containing many small arrows inside. This can represent goal metrics (large arrow) and driver metrics (small arrows) or organisational metrics (large arrows) and team metrics (small arrows). In each case, we want to make sure that the direction of the small arrows is as aligned as possible with the large arrow.\n\n\n\n\nUseful visual metaphor for aligning organisation level metrics with lower-level metrics\n\n\n\n5.2.1 Goal metrics\n\nAlso “success metrics” or “North Star” metrics\nDirectly aligned with the organisation’s mission and represents how it creates value for its customers\nThey are a quantitative definition of what success looks like\nTend to be long-term oriented and slow-moving\nAn organisation usually has only very few or even just one\nExamples: “average monthly purchases” for Amazon, MAP for Meta\n\n\n\n5.2.2 Driver metrics\n\nAlso “surrogate metrics”, “predictive metrics”, “proxy metrics”\nCapture the movement of factors contributing to the organisation’s goal\nThey are a quantitative representation of what drives success\nThey tend to be short-term oriented and fast-moving\nWill change over time as the service matures and as it becomes more closely aligned/correlated with the North Star\nDuan, Ba, and Zhang (2021) discuss lots of useful considerations for cases where we run experiments based on surrogate metrics\nExample for Amazon buyer focused team: number of high-quality sellers that join platform per month.\n\n\n\n5.2.3 Guardrail metrics\n\nKohavi, Tang, and Xu (2020) divide guardrails into two types: those protecting the business and those ensuring internal validity of experiment results\nThe main guardrail to ensure internal validity is smaple ratio mismatch (SRM). Others are discussed in chapter 21 in Kohavi, Tang, and Xu (2020)\nGuardrails that protect the business ensure that improving one part of the platform don’t come at the cost of quality/experience/something else – they basically try to guard against unintended consequences (an example would be site latency)\nDeng and Shi (2016) argue that the main feature of a good guardrail metric should be directionality, so that we can be sure that if we get a signal, it points in the right direction in terms of user experience (in contrast to debug metrics, which should have good sensitivity)\nAmazon example: average number of purchases per day (to check that influx of sellers doesn’t lead to paralysis for buysers)\n\n\n\n5.2.4 Supporting metrics\n\nIndicators that the primary or NS metric are moving in the right direction (particularly useful as leading indicators)\nAmazon example: emails sent to high-quality sellers, emails opened, etc.\n\n\n\n5.2.5 Debug metrics\n\nDeng and Shi (2016) mention debug metrics as a way to get additional informaiton about the movement of our primary metrics.\nThey can be useful when showing the individual components of combo metrics, or the numerator and denominator of ratio metrics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#what-makes-a-good-metric",
    "href": "chapters/metrics.html#what-makes-a-good-metric",
    "title": "5  Metrics",
    "section": "5.3 What makes a good Metric",
    "text": "5.3 What makes a good Metric\n\nWhat makes a good metric in general varies of the type of matric we’re talking about.\nKohavi, Tang, and Xu (2020) argues that a good goal or North Star metric is simple and stable. A good driver metrics, especially one which we would use for experimentation (is there ever an argument to be made for a driver metric that you wouldn’t want to use for experimentation?), we have a few more criteria.\nA good starting point to think about characteristics of a good metric for experimentation is the STEDII framework from Microsoft’s experimentation platform. But I find it quite incomplete, and augment it with other measures mentioned in Kohavi, Tang, and Xu (2020).\n\nKey:\n\nMeaningful: does it reflect the goal of the company/product (check is direction of change aligned with change in quality)\nMeasurable: not everything is measurable (e.g. post-purchase satisfaction)\nAttributable: we must be able to attribute changes in the metrics to experiment variants (not possible with data from third party data proviers)\nSensitive and timely: ensures we can detect a change in a timely manner (check that metric variance is low, and that we have historically observed change in the metric)\nTrustworthy: is the metric reliable for what we want to measure (is data collection reliable? is it not gameable1?)\nInterpretable and actionable: do we know what a change in the metric means? can we easily interpret it?\nDirectionality: does the measure consitently go in the same direction for a change that means the same?\n\nFurther conserns:\n\nEfficient: can we use the metric at scale? (check cost of metric use)\nDebuggable: can we investigate anomalies (can we decompose metric?)\nInclusive and fair: do we know blindspots and limitations? do we have segments to check impact of most vulnerable segments?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#developing-and-evaluating-good-metrics",
    "href": "chapters/metrics.html#developing-and-evaluating-good-metrics",
    "title": "5  Metrics",
    "section": "5.4 Developing and evaluating good metrics",
    "text": "5.4 Developing and evaluating good metrics\n\nSee Richardson et al. (2023) for a great paper on how to develop ideal proxy metrics (i.e. metrics that aren’t directly the North Star). Duan, Ba, and Zhang (2021) also seems promising.\nDeng and Shi (2016) suggest a three-step process to develop user-behaviour-driven metrics:\n\nFormulate a hypothesis based on a simple model of user behaviour (e.g. social network users who like, comment, and share more have a better user-experience)\nConduct user studies to create labelled data against which the original model can be tested (e.g. conduct user surveys and test whether users who like, comment, and share more report a better experience)\nDesign online metrics based on insights from step 2 and assess their directionality and sensitivity as discussed below (e.g. create a metric called “meaningful engagement” and test whether it has high directionality and sensitivity in past experiments)\n\nIn doing this, remember to 1) focus on behaviour patterns that are observed for most users, 2) collect labelled data from various sources to mitigate bias (surveys, lab studies, annotated logged data), and 3) use transparent models so that we can define and track debug metrics (e.g. create an online metric using a decision tree, rather than a complex ML model).\nDeng and Shi (2016) recommend using two criteria to evaluate the quality of a metric: directionality and sensitivity. Directionality requires that a move of the metric in one direction consistently captures the direction of the user experience. In practice, the direction of a metric, such as queries per user can be ambiguous. Sensitivity requires that the metric picks up changes in the user experience such that we can identify it as part of an experiment. We can think of them as the direction and the size of a vector: the more directly it points towards the North Star, and the closer it gets, the better. These two criteria also allow us to qualitatively compare different metrics and decide which one(s) to use as our OEC.\nDeng and Shi (2016) propose two ways to evaluate directionality and sensitivity. First, we can use a validation corpus: a collection of prior experiments for which we know the effect on user experience, and which we can then use to test sensitivity and directionality of our metrics. The second, if no validation corpus is available, is degeneration experiments, whereby we deliberately degenerate the user experience in a way that is acceptable and doesn’t harm long-term user experience, and then measure directionality and sensitivity of the metrics.\nDeng and Shi (2016) point out that ratio metrics (CTR or Success Query Rate) are often good candidates for OECs because they are bounded and have high sensitivity. However, they need to be interpreted carefully because a change can result from a change in the numerator, the denominator, of both, out of which only the first gives a clear signal. Hence, when relying on ratio metrics, they recommend two things: 1) rely on debug metrics to separately track numerator and denominator, 2) only rely on ratio metrics with a stable denominator. For example: Bing used Session Success Rate instead or Query Success Rate even though QSR was more sensitive because the denominator of SSR was more stable (QSR was used as a debug metric).\nFor cases where no single metric fits all scenarios, Deng and Shi (2016) recommend using a combo metric. To create such a metric, it is important to 1) understand the direction and interpretation of each metric and be aware of the scenarios when it fails to provide an accurate signal, 2) have metrics that cover all scenarios.\nOverall process\n\nDefine Goals for feature (e.g. improve efficiency)\nDefine Signals (fewer undos or erases)\nDefine Metrics (average number of undos per session)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#combining-metrics-into-a-signal-creating-an-oec",
    "href": "chapters/metrics.html#combining-metrics-into-a-signal-creating-an-oec",
    "title": "5  Metrics",
    "section": "5.5 Combining metrics into a signal / creating an OEC",
    "text": "5.5 Combining metrics into a signal / creating an OEC\n\nWe often consider multiple key metrics, and have a mental model of the trade-offs we are willing to accept (i.e. how much churn are we willing to accept as long as other users compensate for loss)?\nAn OEC is a way to formally state these trade-offs by creating a single metric as a weighted average of all key metrics.\nPossible approaches\n\nNormalise each metrics to between 0 and 1 and assign a weight to each",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#how-to-select-metrics",
    "href": "chapters/metrics.html#how-to-select-metrics",
    "title": "5  Metrics",
    "section": "5.6 How to select metrics",
    "text": "5.6 How to select metrics\n\nNSM – what is essence of company? How can we capture it?\nDriver metrics\n\nIn general, want product/service metrics that capture essence/goal/value proposition of product and drive company mission/NSM\nE.g. for mature product, might wanna focus on engagement rather than activation/growth\nIs there one single NSM for product/service (i.e. #of txns for Marketplace)\nQuestions to think about\n\nHow does the product work, exactly? (Do we know if transaction happens?)\nWhat is essence of product? (Give everyone opportunity to sell things and help locals find what they need)\nWhat are its goals? (Product pretty mature, so maybe engagement rather than growth)\nWhat would success look like? (We facilitate a lot of transactions)\nHow do users get value from it? (Sellers sell things quickly, buyers find things quickly)\n\n\nSupport and guardrails trickier. Use AAAERRR Framework\nTo ensure coherence across all workstreams in a company, metrics used at all levels have to contribute to the same overall goal, which is captured by the company’s North Star.\nPrimary metric something that directly captures what you wanna improve? Guardrails general health metrics you don’t want to go down (e.g. revenue, conversion)? Based on Kohavi anecdote below\nBojinov, Chen, and Liu (2020) mention that LinkedIn has four company wide success metrics and many product specific ones. So, presumably we’d use company-wide ones as guardrails. Question is, what are good product-specific metrics? Good in the sense that they have a positive impact on business-wide metrics? Can use causal inference (e.g. IV) to test effect (see section 2.1 in Bojinov, Chen, and Liu (2020))\nHave an OEC that directly captuers what you want to measure. Selecting the wrong metric can lead to misleading results. (kohavi2012trusworthy?) provide a memorable example from an experiment at Bing: the experiment increased revenue by user because search results were poorer, leading users to make more searches and lead them to click on more adds. This is good in the short-term. But in the long term, users will surely get frustrated by the poorer search results. A better metric would have been one that directly captures the quality of the search results, such as sessions per user. Lesson: have an OEC that directly captures the thing you want to improve, and use higher level-metrics such as revenue as guardrails.\nNormalise metrics by sample size. E.g. user revenue per user rather than variant-level revenue (since the latter is dependent on number of users, which can vary between variants).\nAdam D’Angelo on metrics here\n\nWhat to measure? Focus on users who are getting value today\n\nActive users (active if getting value)\nRevenue (implies they get value)\nTransactions (for marketplace)\n\nIn general, if you have two groups, focus on metric that unifies them (i.e. for marketplace, transactions captures value for sellers and buyers)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#good-use-of-guardrail-metrics",
    "href": "chapters/metrics.html#good-use-of-guardrail-metrics",
    "title": "5  Metrics",
    "section": "5.7 Good use of guardrail metrics",
    "text": "5.7 Good use of guardrail metrics\n\nBased on Airbnb’s Experiment Guardrail system as discussed in this post\nGoal is to have an automated process that ensures that no change as a negative impact on another part of the product without there having been an explicit discussion about it\nSelect guardrails for each experiement (based on types of guardrails above, and find balance between protecting all teams and moving fast – remember, as pointed out in the post, if you have 50 guardrail metrics and alert any significant degradation, then you have at least one false alert in 92% percent of experiments – given that \\(1 - (1-0.05)^50 = 0.92\\))\nFor each metric, have three types of guardrals: impact guardrail (catch experiments with high negative impact on metric), power guardrail (ensure impact guardrail has adequate significance and power levels) and stat. sig guardrail (catch even small impacts on key metrics if statsig)\nExperiments that raise a flag are being discussed among all stakeholders to make launch decision considering all trade-offs\n\n\n5.7.1 How to think about risks of a given set of metrics?\n\nThink about what I’ve focuse on (i.e. engagement for a mature product)\nThe risk is then that we neglect stages earlier in the funnel (e.g. acquisition in areas where the product doesn’t do as well) and later in the funnel (i.e. we might wanna think about monetisation)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#frameworks-for-creating-driver-metrics",
    "href": "chapters/metrics.html#frameworks-for-creating-driver-metrics",
    "title": "5  Metrics",
    "section": "5.8 Frameworks for creating driver metrics",
    "text": "5.8 Frameworks for creating driver metrics\nDriver metrics represent quantitative measures of the factors that drive an organisation’s success. The following frameworks help us think about those drivers of success.\n\n5.8.1 Pirate metrics (AARRR)\nDeveloped by Dave McClure, the pirate metrics (summary here, slides here) classify driver metrics into:\n\nAcquisition (the user comes to our site from various channels)\nActivation (has a good experience on the first visit)\nRetention (comes back to the site)\nReferral (likes the site enough to recommend it to others)\nRevenue (engages in a revenue generating behaviour)\n\nThe below Table provides a useful example of possible conversion metrics and associated conversion rates.\n\n\n\nAARRR example conversion metrics. Source: 500hats.typepad.com/500blogs/2007/09/startup-metrics.html\n\n\n\n\n5.8.2 AAAERRR\nAn extension of the pirate metrics.\n\nAwareness (how many aware of product)\nAcquisition (how many use product)\nActivation (how many are realizing value of product – e.g. 10 friends in 7 days on FB / stored at least 1 file on a device on Dropbox)\nEngagement (breath and frequencey of engagement)\nRevenue (how many are paying for product)\nRetention/renewal (how many are coming back)\nReferral (how many are becoming advocates)\n\n\n\n5.8.3 PULSE\n\nPage views\nUptime\nLatency\nSeven-day active users\nEarnings\n\n\n\n5.8.4 HEART\n\nDeveloped by Rodden, Hutchinson, and Fu (2010), the authors address shortcomings of the PULSE framework.\nKey challenge in CHI is creation of user-experience metrics based on large-scale data.\nTraditional PULSE metrics (Page views, Uptime, Latency, Seven-day active users, Earnings) are useful and related to user-experience, but limited because they are indirect and can be ambiguous (are more page views a sign of an increase in engagement or in confusion?) and provide limited insight (seven-day active users shows user-base volume but nothing about product commitment).\nAuthors propose HEART metrics (Happiness, Engagement, Adoption, Retention, Task success) to complement traditional metrics and remedy their shortcomings.\nHappiness\n\nMeasured using bipolar scale in-product survey\nShows that users liked redesign of personalised homepage after initial dip (also shows value of dynamic treatment effects)\n\nEngagement\n\nE.g. number of visits per user per week\nHelped Gmail team see proportion of users who visited more than 5 times per week.\n\nAdoption and retention\n\nE.g. how many new accounts created (adoption), how many users from seven-day active users 3 weeks ago are still in that set (retention).\nHelped Google Finance team distinguish between new and recurring users during 2008 meltdown.\n\nTask success\n\nE.g. progress in optimal path (signup)\nHelped Google maps team see that users could adopt search to single-box so they could drop double-box version.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#common-metrics",
    "href": "chapters/metrics.html#common-metrics",
    "title": "5  Metrics",
    "section": "5.9 Common metrics",
    "text": "5.9 Common metrics\n\nConversion rate\nNumber of bookings\nEngagement\n\nLikes, shares, comments, reactions\nPage views\nClick-through rates (CTR)\nTime spent per user per day\n\nRetention\n\nDaily active users (DAU, useful to measure intensity of usage)\nWeekly active users (WAU)\nMonthly active users (MAU, user engages frequently)\nStickiness (DAU / MAU)\nChurn rate (percentage of users who stop using the service within a given period)\nRetention rate (1 - churn rate)\n28-day retention rate (% who still use it 28-days after\nTime spent on product\nSession frequency\n\nRevenue\n\nAverage revenue per user (ARPE)\nCustomer lifetime value (CLV)\n\n\nGuardrails\n\nProtecting the business\n\nRevenue\nCancellation rate\nCannibalisation of similar products in our ecosystem (i.e. WhatsApp impact on Messenger)\n\nProtecting internal validity of experiment\nUser experience metrics\n\nBounce rate (proportion of site visitors who leave after seeing only the first page)\nLatency\nNumber of messages flagged as spam/harmful\n\nStrategic priority metrics",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#other-metric-taxonomies",
    "href": "chapters/metrics.html#other-metric-taxonomies",
    "title": "5  Metrics",
    "section": "5.10 Other metric taxonomies",
    "text": "5.10 Other metric taxonomies\nBusiness report vs heuristic vs user-behaviour\n\nDeng and Shi (2016) use a different classification altogether:\n\nType 1: Business Report Driven Metrics: Business report driven metrics, like Revenue per User and Monthly Active Users, focus on long-term goals of online services. These metrics are vital for business assessments but are less actionable for short-term product improvements. For example, improving search results in a service like Bing might decrease short-term revenue per user, highlighting the need for longer-term experimentation to truly assess impacts.\nType 2: Simple Heuristic Based Metrics: Simple heuristic based metrics, such as Click-Through Rate and user activity counts, offer direct insights into user interaction with online services. While actionable, they can be misleading in terms of user experience and business goals. For instance, higher CTR due to misleading content can negatively impact user experience and, subsequently, the service’s market share. These metrics are suitable for early-stage services but may not align with real user experience improvements in more mature stages.\nType 3: User-Behavior-Driven Metrics: User-behavior-driven metrics, derived from user satisfaction and frustration models, aim to directly measure user experience and its impact on long-term service success. They are complex, involving detailed analysis of user behavior, like considering both clicks and dwell time for assessing search satisfaction. These metrics are sensitive and actionable for agile experiments, offering a more nuanced understanding of user interaction than simpler metrics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#metric-sensitivity-decomposition",
    "href": "chapters/metrics.html#metric-sensitivity-decomposition",
    "title": "5  Metrics",
    "section": "5.11 Metric sensitivity decomposition",
    "text": "5.11 Metric sensitivity decomposition\n\nDeng and Shi (2016) point out that detecting a treatment effect has two components:\n\n\\[\nP(\\text{detecting treatment effect on the metric}) = P(H_1) \\times P(p \\leq \\alpha | H_1)\n\\]\n\n\\(P(H_1)\\) is moveability of the metric\n\\(P(p \\leq \\alpha | H_1)\\) is power\nThe authors point out that understanding which component produces a lack of sensitivity is crucial. Because if it’s movement probability, we might need a different metric, whereas with a lack of power, variance reduction might help.\nExamples for metrics with low movement probability might be “number of sessions per user” for a search engine, since daily search needs are limited, and changing user engagement is difficult in the short-term. An example for a metric with low power is “Revenue per user”, due to very high variance.\nSee also Richardson et al. (2023) for a more rigorous exposition of the point.\nThink about this more. I’m not convinced we don’t just care about power.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#metrics-vs-goals",
    "href": "chapters/metrics.html#metrics-vs-goals",
    "title": "5  Metrics",
    "section": "5.12 Metrics vs goals",
    "text": "5.12 Metrics vs goals\n\nEvery metric has its limitations.\nMeta’s goal is to have lots of users. But how do you measure this? As Alex Schultz explains here, the industry has moved from registered users to confirmed registered users to active confirmed registered users to monthly active users",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#footnotes",
    "href": "chapters/metrics.html#footnotes",
    "title": "5  Metrics",
    "section": "",
    "text": "A metric that isn’t gameable defies Goodhart’s law, which is what we want.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#theory",
    "href": "chapters/power.html#theory",
    "title": "6  Power",
    "section": "",
    "text": "Largely based on (duflo2007randomization?)\n\n\n\n\ntitle\n\n\n\n\nIn the simplest possible, we randomly draw a sample of size \\(N\\) from an identical population, so that our observations can be assumed to be i.i.d, and we allocate a fraction \\(P\\) of our sample to treatment. We can then estiamte the treatment effect using the OLS regression\n\n\n\nwhere the standard error of \\(\\beta\\) is given by \\(\\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}}\\).\nstd error derivation (from standard variance result of two independent samples, using population fractions):\n\n\n\nThe distribution on the left hand side below shows the distribution of our effect size estimator \\(\\hat{\\beta}\\) if the null hypothesis is true.\nWe reject the null hypothesis if the estimated effect size is larger than the critical value \\(t_{\\alpha}\\), determined by the significance level \\(\\alpha\\). Hence, for this to happen we need \\(\\hat{\\beta} &gt; t_{\\alpha} * SE(\\hat{\\beta})\\) (follows from rearranging the t-test formula).\nOn the right is the distribution of \\(\\hat{\\beta}\\) if the true effect size is \\(\\beta\\).\nThe power of the test for a true effect size of \\(\\beta\\) is the area under this curve that falls to the right of \\(t_{\\alpha}\\). This is the probability that we reject the null hypothesis given that it is false.\nHence, to attain a power of \\(\\kappa\\) it must be that \\(\\beta &gt; (t_a + t_{1-\\kappa}) * SE(\\hat{\\beta})\\), where \\(t_{1-\\kappa}\\) is the value from a t-distribution that has \\(1-\\kappa\\) of its probability mass to the left (for \\(\\kappa = 0.8\\), \\(t_{1-\\kappa} = 0.84\\)).\nThis means that the minimum detectable effect (\\(\\delta\\)) is given by:\n\n\n\nRearranding for the minimum required sample size we get:\n\n\n\nSo that the required sample size is inversely proportional to the minimal effect size we wish to detect. This makes sense, it means that the smaller an effect we want to detect, the larger the samle size we need. In particular, given that \\(N \\propto \\delta^{-2}\\), to detect an effect of half the size we need a sample four times the size.\nSE(\\(\\beta\\)) also includes measurement error, so this is also a determinant of power.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#what-determines-power",
    "href": "chapters/power.html#what-determines-power",
    "title": "6  Power",
    "section": "6.2 What determines power",
    "text": "6.2 What determines power\n\nSignificance level\nEffect size\nStandard error\n\nSample size\nVariant allocation proportion\nMetric variance",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#how-to-increase-power",
    "href": "chapters/power.html#how-to-increase-power",
    "title": "6  Power",
    "section": "6.3 How to increase power",
    "text": "6.3 How to increase power\n\nPower can be increased trivially by lowering the significance level, which we often don’t want to do, or by increasing sample size, which we’re often trying to avoid.\nIncrease effect size\n\nEnsure that only users who are exposed to the change are in the data to avoid dilution of the effect\n\nOptimally allocate variance proportions\n\nUsually equal for highest power\nShow why with many treatment variants, higher share in control is better\n\nReduce metric variance\n\nChoose metric with low variance (e.g. indicator)\nUse variance reduction technique\nOnly include triggered users",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#how-to-choose-key-parameters",
    "href": "chapters/power.html#how-to-choose-key-parameters",
    "title": "6  Power",
    "section": "6.4 How to choose key parameters",
    "text": "6.4 How to choose key parameters\n\n6.4.1 MDE\n\nWhat are you balancing here? The size of the effect you are able to identify and the time it takes to do it.\nAll else equal, the smaller a change you want to be able to detect, the longer it will take for the experiment to run because you need more sample size.\nThe relevant question to ask here is “what counts as a practically relevant change?”\nTo answer that, consider:\n\nMaturity of service (the more mature, the smaller a change can be expected)\nSize of service (the larger, the smaller a change still generates a lot of revenue)\nCost of change that need ot be covered\n\nCost of fully building out feature for launch (can be 0 when fully built out for experiment or high if we use painted door)\nCost of maintaining new code (new code has higher bugs, may increase code complexity and maintenance)\nOther costs: e.g. does CPU utilization increase?\n\n\n\n\n\n6.4.2 Significance level\n\nWhat are you balancing here? The probabilities of making a type I and type II error.\nThe higher significance level, the less likely we are to implement useless features (to make a Type I error) but the more likely we are to no implement useful features (to make a Type II error).\nHence, gotta balance cost of implementing useless feature and cost of not implementing useful feature.\nThings that play into this:\n\nHow long will feature be in effect (less long lowers risk of implementing)?\nHow widely will it be deployed (less widely lowers risk of implementing)?\nHow many users will see it / where in the funnel is it (later in funnel lowers risk of implementation)\n\nWhat to do in practice:\n\nStart from baseline values (\\(alpha = 0.05\\))\nAdjust depending on balance of risks\n\n\n\n\n6.4.3 Power\n\nWhat are you balancing here? The risk of making a Type II error and the time you have to wait for your results.\nAll else equal, the higher a level or power you want, the longer you’ll have to run the experiment to accumulate the requried sample size.\nFactors to consider:\n\nHow costly is it to not implement a useful feature.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#problems-with-low-power",
    "href": "chapters/power.html#problems-with-low-power",
    "title": "6  Power",
    "section": "6.5 Problems with low power",
    "text": "6.5 Problems with low power\n\nTruth inflation: underpowered studies only find a significant effect it the effect size is larger than the true effect size, leading to inflated claims of effect sizes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#power-in-online-experiments",
    "href": "chapters/power.html#power-in-online-experiments",
    "title": "6  Power",
    "section": "6.6 Power in online experiments",
    "text": "6.6 Power in online experiments\n\nKohavi et al. (2014) point out (in rule 7) that while general advice suggets that the CLT provides a good approximation for n larger than 30, the large skew in online metrics often requires many moer users. They recomment 355 * (skewness coefficient)^2.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#rule-of-thumb-for-sample-size",
    "href": "chapters/power.html#rule-of-thumb-for-sample-size",
    "title": "6  Power",
    "section": "6.7 Rule of thumb for sample size",
    "text": "6.7 Rule of thumb for sample size\nFor alpha = 0.05, power = 0.8, and a two-sided test with equal allocation to two variants, required sample size is approximately:\n\\[\nn \\approx \\frac{16\\sigma^2}{\\tau^2},\n\\]\nwhere \\(\\sigma^2\\) is the sample variance and \\(\\tau\\) is the tretment effect (this is not known, but we can use the MDES).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#best-practices",
    "href": "chapters/power.html#best-practices",
    "title": "6  Power",
    "section": "6.8 Best practices",
    "text": "6.8 Best practices\n\nWhen aiming to estimate a precise effect size rather than just being interested in statistical significance, use assurance instead of power: instead of choosing a sample size to attain a given level of power, choose sample size so that confidence interval will be suitably narrow 99 percent of the time (Sample-Size Planning for More Accurate Statistical Power: A Method Adjusting Sample Effect Sizes for Publication Bias and Uncertainty and Understanding the new statistics.)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#useful-resources",
    "href": "chapters/power.html#useful-resources",
    "title": "6  Power",
    "section": "6.9 Useful resources",
    "text": "6.9 Useful resources\n\nLarsen et al. (2023) for general overview\nZhou, Lu, and Shallah (2023) for comprehensive overview of how to calculate power\nBojinov, Simchi-Levi, and Zhao (2023), section 5, for simulation results for switchbacks and generally good approach to simulation to emulate\nReich et al. (2012) power calcs for cluster-randomised experiments",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#qa",
    "href": "chapters/power.html#qa",
    "title": "6  Power",
    "section": "6.10 Q&A",
    "text": "6.10 Q&A\nQuestions:\n\nLonger experiment duration generally increases power. Can you think of a scenario where this is not the case?\nAn online shopping site ranks products according to their average rating. Why might this be suboptimal? What could the site do instead?\n\nAnswers:\n\nWhen using a cumulative metric such as number of likes, the variance of which will increase the longer the experiment runs, which will increase the standard error of our treatment effect estimate and lower our power. Remember that \\(SE(\\hat{\\tau}) = \\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}}\\). So, whether this happens depends on what happens to \\(\\frac{\\sigma^2}{N}\\), as experiment duration increases. A decrease in power is plausible – likely, even! – because \\(N\\) will increase in a concave fashion over the course of the experiment duration (some users keep coming back), while \\(\\sigma^2\\) is likely to grow faster than linearly, which causes the ratio to increase and power to decrease.\nThe approach is suboptimal because products with few ratings will have much more variance than products with many ratings, and their average rating is thus less reliable. The problem is akin to small US states having the highest and lowest rates of kidney cancer, or small schools having highest and lowest average pupil performance. Fundamentally, it’s a problem of low power – the sample size is too low to reliably detect a true effect. The solution is to use a shrinkage method: use a weighted average of the product average rating and some global product rating, with the weight of the product average rating being proportional to the number of ratings. This way, products with few ratings will be average, while products with many ratings will reflect their own rating.\n\n\n\n\n\nBojinov, Iavor, David Simchi-Levi, and Jinglong Zhao. 2023. “Design and Analysis of Switchback Experiments.” Management Science 69 (7): 3759–77.\n\n\nKohavi, Ron, Alex Deng, Roger Longbotham, and Ya Xu. 2014. “Seven Rules of Thumb for Web Site Experimenters.” In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1857–66.\n\n\nLarsen, Nicholas, Jonathan Stallrich, Srijan Sengupta, Alex Deng, Ron Kohavi, and Nathaniel T Stevens. 2023. “Statistical Challenges in Online Controlled Experiments: A Review of a/b Testing Methodology.” The American Statistician, 1–15.\n\n\nReich, Nicholas G, Jessica A Myers, Daniel Obeng, Aaron M Milstone, and Trish M Perl. 2012. “Empirical Power and Sample Size Calculations for Cluster-Randomized and Cluster-Randomized Crossover Studies.” PloS One 7 (4): e35564.\n\n\nZhou, Jing, Jiannan Lu, and Anas Shallah. 2023. “All about Sample-Size Calculations for a/b Testing: Novel Extensions and Practical Guide.” arXiv Preprint arXiv:2305.16459.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/threats_to_validity.html#threats-to-reliability",
    "href": "chapters/threats_to_validity.html#threats-to-reliability",
    "title": "12  Threats to validity",
    "section": "12.1 Threats to reliability",
    "text": "12.1 Threats to reliability"
  },
  {
    "objectID": "chapters/threats_to_validity.html#threats-to-internal-validity",
    "href": "chapters/threats_to_validity.html#threats-to-internal-validity",
    "title": "7  Threats to validity",
    "section": "7.2 Threats to internal validity",
    "text": "7.2 Threats to internal validity\n\n7.2.1 Misc effects (some of them relevant only for field experiments)\n\nHawthorne effect: the treatment group works harder than normal.\nJohn Henry effect: the comparison group competes with the treatment group.\nResentment and demoralising effect: not getting the treatment changes behaviour negatively.\nDemand effect: treatment units more likely to do what they think is wanted from them.\nAnticipation effect: control group changes behaviour in anticipation of future treatment.\nSurvey effect: being surveyed changes behaviour.\n\n\n\n7.2.2 Interference\n\nBasically, all violations to SUTVA\nInterference can happen due to\n\nNetwork effects\nCannibalisation of resources in marketplaces\nShared resources (i.e. treatment slowing down site for everyone)\n\n\n\n\n7.2.3 Interaction effects\n\nUsers can be simultaneously part of multiple experiments, so that what we measure for reach of them is really the effect of the interaction of all of them. This means that, if only some features are implemented, the results after roll out could be different from those observed during the experiment period.\nHowever, with large sample sizes, this should not generally be a problem because effects of different experiments average out between treatment and control group.\nWhile the above may be true statistically, interaction effects can still lead to extremely poor user experiences (blue background interacted with blue font), which is why mature platforms aim to avoid them.\n\n\n\n7.2.4 Non-representative users\nPossible scenarios:\n\nOur marketing department launches an add campaign and attracts a lot of unusual users to the site temporarily.\nA competitor does the same and temporarily takes a ways users from our site.\nHeavy-user bias: heavy users are more likely to be bucketed in an experiment, biasing the results relative to the overall effect of a feature. Depending on the context, this can be an issue.\nSolution: run experiments for longer (thought this comes with opportunity costs, and will increase cookie churn)\n\n\n\n7.2.5 Survivor bias\n\nThis is really just a version of the above: if you select only users that have used the product for some time, your sample is not representative of all users. The classic demonstration of survivor bias is Abraham Wald’s insight in WWII that you want to put extra armour where returning plans got hit the least, since it’s presumable the planes that got hit there that didn’t make it back.\n\n\n\n\n7.2.6 Novelty and learning effects\n\nChallenge: behaviour might change abruptly and temporarily in response to a new feature (novelty or “burn in” effect) or it might take a while for behaviour fully adapt to a new feature (learning effects). In both cases, the results from a relatively short experiment will not provide a representative picture of the long-run effects of a feature.\nExamples: Increasing number of adds shows on Google led to increase in add revenue initially but then decrease of clicks in the long term because it increased add blindness Hohnhold, O’Brien, and Tang (2015)\nSolutions:\n\nMeasure long-term effects (by running experiments for longer)\nHave a “holdout” group of users that isn’t exposed to any changes for a pre-set period of time (a month, a quarter), to measure long-term effects\nEstimate dynamic treatment effects to see the evolution of the treatment effect\n\n\n\n\n7.2.7 Sample ration mismatch",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Threats to validity</span>"
    ]
  },
  {
    "objectID": "chapters/threats_to_validity.html#treats-to-external-validity",
    "href": "chapters/threats_to_validity.html#treats-to-external-validity",
    "title": "7  Threats to validity",
    "section": "7.3 Treats to external validity",
    "text": "7.3 Treats to external validity\n\n7.3.1 Budget effects in Ads\n\nOn an adds platform, a treatment might perform very well during an experiment in that it makes marketers launch more adds. But once scaled up may do less well because the increased traffic might exhaust marketer’s budgets, leading them to reduce adds launched.\n\n\n\n7.3.2 Feedback loops from personalisation\n\nTreatments might behave differently during experimentation and once they are scaled up if the performance of a feature is a function of the size of the audience it is exposed to (an example could be a recommendation algorithm, which performs better and better as it is being used more).\n\n\n\n7.3.3 Day-of-week effects\n\nSee below\n\n\n\n7.3.4 Seasonality\n\nSeasonality comes in many forms: day of week effects, week of year effects, season effects, holiday effects, etc.\nThe challenge is that, potentially, user behaviour might differ on certain days or over certain time periods either because we get different users or because users change their behaviour.\nWhether it is really a problem depends on the context. One aspect that is often forgotten here is that seasonality, first and foremost, is about a shift in levels – activity on LinkedIn might go down during the summer months. What we usually want to measure, however, is the difference between treatment and control units. Hence, if you don’t have reason to believe that the effect of the treatment is different during a particular season (e.g. because you think it’s additive), then seasonality might not be a problem for you.\nHaving said that, it’s actually quite likely that with either different users or different behaviour by the same users, users might react differently to featore on different days. So it really is a thread to external validity, and we thus should usually care about it.\nSolution: design your experiment so as to take seasonality into account. E.g. run your experiment for at least one week to account for day of week effects (that’s generally a good idea), don’t run crucial experiments during the holiday season or on major holidays or discard data from such periods, etc.\nWhat to take into account depends on your context. So understand the relevant seasonality for you (if you’re a travel app, consider seasonality of travel demand, if you’re an e-commerce site, consider seasonality of shopping behaviour)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Threats to validity</span>"
    ]
  },
  {
    "objectID": "chapters/threats_to_validity.html#differences-in-time-to-action-between-users",
    "href": "chapters/threats_to_validity.html#differences-in-time-to-action-between-users",
    "title": "7  Threats to validity",
    "section": "7.4 Differences in time-to-action between users",
    "text": "7.4 Differences in time-to-action between users\n\nSome users may engage with a new features immediately, others might take a while and then react differently to it.\nWhen running experiments for a very short time, we might thus get a biased picture of the overall effect of the feature.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Threats to validity</span>"
    ]
  },
  {
    "objectID": "chapters/threats_to_validity.html#resources",
    "href": "chapters/threats_to_validity.html#resources",
    "title": "7  Threats to validity",
    "section": "7.5 Resources",
    "text": "7.5 Resources\n\nForbes article on when not to trust your A/B tests\nDennis Meisner discussing threats to external validity\n\n\n\n\n\nHohnhold, Henning, Deirdre O’Brien, and Diane Tang. 2015. “Focusing on the Long-Term: It’s Good for Users and Business.” In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1849–58.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Threats to validity</span>"
    ]
  },
  {
    "objectID": "chapters/practical_issues.html#what-if-you-want-to-implement-a-feature-right-away",
    "href": "chapters/practical_issues.html#what-if-you-want-to-implement-a-feature-right-away",
    "title": "8  Practical issues",
    "section": "",
    "text": "There are situations where you feel very confident that a feature works well and you don’t want to wait a week or two to implement it.\nPinterest uses what they call “holdout experiments” in such cases, whereby 99% of users are exposed to the new feature and the remaining 1% act as the control (see King (2020) for more). In effect, this is a reversal of how experimentation typically happens.\nYou will want to use this approach sparingly, because the whole point of using experiments is that we usually can’t know the effect of a feature, even when we think we do (rember the CRASH trial).\nBut I really like the idea, because sometimes there really are cases in practice where you can be pretty certain that waiting for results is a waste of time (e.g. if you have an online shop and move from list of items without pictures to a grid-view with pictures). In such a situation, running a holdout experiment can help you be certain that the direction of the effect is what you expected, and also provide an estimate for the magnitude of the effect.\nFrom Forbes article on when not to trust your A/B tests",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Practical issues</span>"
    ]
  },
  {
    "objectID": "chapters/practical_issues.html#resources",
    "href": "chapters/practical_issues.html#resources",
    "title": "13  Practical issues",
    "section": "13.2 Resources",
    "text": "13.2 Resources\n\nForbes article on when not to trust your A/B tests\n\n\n\n\n\nKing, Jeremy. 2020. “The Power of These Techniques Is Only Getting Stronger.” Harvard Business Review. HARVARD BUSINESS SCHOOL PUBLISHING CORPORATION 300 NORTH BEACON STREET …."
  },
  {
    "objectID": "chapters/variance_reduction.html#why-reduce-variance",
    "href": "chapters/variance_reduction.html#why-reduce-variance",
    "title": "14  Variance reduction",
    "section": "14.1 Why reduce variance?",
    "text": "14.1 Why reduce variance?\nTODO: - Explain that we wanna increase power (refer to power section) - Reducing variance is one way to increase power (often the only feasible one)\nThe estimand of interest is the population average treatment effect:\n\\[\n\\tau = \\bar{y}_t - \\bar{y}_c.\n\\]\nUnder random treatment assignment, an unbiased estimator of \\(\\tau\\) is the difference in means between treatment and control units in the sample:\n\\[\n\\hat{\\tau}^{dif} = \\bar{y}_t^{obs} - \\bar{y}_c^{obs}.\n\\]\nThe variance of \\(\\hat{\\tau}^{dif}\\) is given by:\n\\[\n\\mathbb{V}(\\hat{\\tau}^{dif}) = \\frac{s_t}{N_t} + \\frac{s_c}{N_c},\n\\]\nwhere\n\\[\ns_t = \\frac{1}{N_t - 1}\\sum_{\\text{i:d=1}}(y_i - \\bar{y}_t^{obs})^2, \\quad s_c = \\frac{1}{N_c - 1}\\sum_{\\text{i:d=0}}(y_i - \\bar{y}_c^{obs})^2.\n\\]\nHence: for a given sample size, we can reduce \\(\\mathbb{V}(\\hat{\\tau}^{dif})\\) by reducing the variance in the outcome metric, \\(Y\\) – this is the variance we are trying to reduce."
  },
  {
    "objectID": "chapters/variance_reduction.html#what-about-the-variance-bias-trade-off",
    "href": "chapters/variance_reduction.html#what-about-the-variance-bias-trade-off",
    "title": "9  Variance reduction",
    "section": "9.2 What about the variance-bias trade-off?",
    "text": "9.2 What about the variance-bias trade-off?\nDiscuss why we can reduce variance without increasing bias.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variance reduction</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html#stratification",
    "href": "chapters/variance_reduction.html#stratification",
    "title": "9  Variance reduction",
    "section": "9.3 Stratification",
    "text": "9.3 Stratification\nTODO\n\n9.3.1 Useful resources\n\nDeng et al. 2013 – original CUPED paper – the original paper\nFive ways to reduce variance in A/B testing",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variance reduction</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html#regression-adjustment",
    "href": "chapters/variance_reduction.html#regression-adjustment",
    "title": "9  Variance reduction",
    "section": "9.4 Regression adjustment",
    "text": "9.4 Regression adjustment\nRegression adjustment reduces variance by adding additional regressors to the regression model used to evaluate an experiment in order to reduce residual variance.\nHow it works\nTo evaluate an experiment where we have, for each unit \\(i\\), an outcome metric \\(y_i\\) and a treatment assignment indicator \\(d_i\\) we would estimate the following linear regression model using OLS:\n\\[\ny_i = \\alpha + \\beta d_i + \\epsilon_i,\n\\]\nwhere \\(\\epsilon_i\\) is the error term, and where the estimate of \\(\\beta\\) is the estimate of our average treatment effect. If we have an additional variable, \\(x_i\\), that is correlated with \\(y_i\\) but uncorrelated with \\(d_i\\), we can add that to the right hand side of our regression model and estimate:\n\\[\ny_i = \\alpha + \\beta_1 d_i + \\beta_2 x_i + \\mu_i.\n\\]\nIf \\(x\\) is uncorrelated with the treatment assignment then \\(\\beta = \\beta_1\\), so the average treatment effect estimate will remain unchanged, which is good. And if \\(x\\) is correlated with \\(y\\), then the standard error of the average treatment effect estimate will again be lower, which increases power.\n\n9.4.1 Useful resources\n\nImbens & Rubin, Causal Inference for Statistics, Social, and Biomedical Sciences, Chapter 7 link",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variance reduction</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html#cupac",
    "href": "chapters/variance_reduction.html#cupac",
    "title": "9  Variance reduction",
    "section": "9.5 CUPAC",
    "text": "9.5 CUPAC\n\nCUPED was designed by folks at Doordash (Tang et al. 2000) to reduce the duration of their switchback experiments, which – according to their paper – it did by about 25 percent.\nCUPED can be extended to (make notation consistent with CUPED)\n\n\\[\ny' = y - \\theta \\bar{f(X)} + \\theta E(f(X))\n\\]\n\nIn above, it can be shown that optimal \\(f(X) = E(Y|X)\\) (show this).\nCUPED, which is effectively regression adjustment, uses best linear predictor.\nCUPAC extends this to non-linear predictors, generating \\(\\hat{y} = g(X) = E(Y|X)\\).\n\n\n9.5.1 Advantages\n\nCan be used for metrics that have no pre-experiment data (if you have variables that correlate with the metric and are unaffected by the treatment)\n\n\n\n9.5.2 Thoughts\n\nThere are two approaches: predict the experiment-time outcome or predict the pre-experiment value.\nIf you do the former, then you need features that are unaffected by the treatment. This means that for reach metric and each experiment, you need to collect features that meet this requirement. This will be different for each metric and experiment because for many features, whether or not the feature is impacted by the treatment depends on the specific experiment (i.e. where in the funnel the feature is active). This could be solved by finding, for each metric of interest, a set of features that are always independent of the treatment (such as distance between restaurant and customer, or the type of browser used by a customer), but it’s not at all clear that finding a set of features with good predictive properties is possible for all our metrics. Hence, building this into a pipeline in an automated way would be an enormous endeavour, if it’s possible at all.\nThis approach is thus really only viable in a very specialised setting where you keep running the same type of experiment, and thus have to build and train the model manually and only once (this is how it’s being used for courier experiments).\nTo ensure that features really are independent of treatment condition, they actively monitor correlations for each running experiment.\nTang et al. (2000) gives the example of reducing average daily delivery time, where a delivery-level covariate that is unaffected by treatment is distance between restaurant and customer.\n\n\n\n9.5.3 Useful resources\n\nControl Using Predictions as Covariates in Switchback Experiments",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variance reduction</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html#other-methods",
    "href": "chapters/variance_reduction.html#other-methods",
    "title": "9  Variance reduction",
    "section": "9.6 Other methods",
    "text": "9.6 Other methods\n\nVariance Reduction Using In-Experiment Data: Efficient and Targeted Online Measurement for Sparse and Delayed Outcomes",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variance reduction</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html#useful-resources-3",
    "href": "chapters/variance_reduction.html#useful-resources-3",
    "title": "9  Variance reduction",
    "section": "9.7 Useful resources",
    "text": "9.7 Useful resources\n\nFive ways to reduce variance in A/B testing\nOnline Experiments Tricks — Variance Reduction\nCUPED, CUPAC, and Other Ways to Reduce Variance in an Experiment\nDon’t use a t-test for A/B testing\nVariance Reduction in Experiments — Part 1: Intuition – see also part 2\n\n\n\n\n\nTang, Yixin, Caixia Huang, David Kastelman, and Jared Bauman. 2000. “Control Using Predictions as Covariates in Switchback Experiments.”",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variance reduction</span>"
    ]
  },
  {
    "objectID": "chapters/network_experiments.html#types-of-interferance",
    "href": "chapters/network_experiments.html#types-of-interferance",
    "title": "9  Network experiments",
    "section": "",
    "text": "Network interference. Suppose we have a network where all nodes are of the same type and where the behaviour of each node can affect the behaviour of the nodes it is directly connected to. An example is a social network, where all nodes are people, and where more activity of one person may plausibly lead to more activity among their connections, which may lead to more activity among their connections, and so on. Here, experimentation is difficult because if we randomly split units into treatment and control group, units in the treatment group that change their behaviour as a result of the treatment might interact with connections of theirs that are allocated to the control group and thus change their behaviour as a result. But now the units in the control group don’t behave any longer as they would under the absence of the treatment, and comparing the differences in behaviour between treatment and control is not an accurate representation of the different outcomes in two counterfactual states of the world, one where everyone is treated and one where nobody is.\nMarketplace interference. Suppose we have a network composed of two of more types of nodes which interact with each other, and where, from the perspective of each type of node, the number of other nodes is finite, so that each type of node competes for interactions with a fixed number of nodes of the other type. Examples include ride hailing, where drivers compete for customers and customers for available rides; ads marketplaces, where marketers compete for slots and platforms with slots for marketers; or food delivery services, where riders compete for orders and customers who place orders for riders (food delivery services, such as Deliveroo, are three-sided marketplaces, managing the interaction of restaurants, riders, and customers). In such a marketplace experimentation is challenging because treatment and control groups compete for a fixed pool of resources, so that an increase in behaviour for the treatment group must lead to a decrease in the decreate of the behaviour for the control group. This “cannibalisation” effect exaggerates the actual treatment effect. (See Liu, Mao, and Kang (2020) for more.)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Network experiments</span>"
    ]
  },
  {
    "objectID": "chapters/network_experiments.html#dealing-with-interference",
    "href": "chapters/network_experiments.html#dealing-with-interference",
    "title": "9  Network experiments",
    "section": "9.2 Dealing with interference",
    "text": "9.2 Dealing with interference\n\nThere are a number of different approaches (see, e.g. discussion in Larsen et al. (2023))\nWhat they share in common is that they all address the fundamental problem, which is that under the presence of interaction effects, the difference in outcomes between the treatment and the control group is not an accurate representation of the two counterfactual states of the world where everyone is and isn’t exposed to the treatment, which is what standard ATE estimators measure.\nThere are two broad ways to tackle the problem: we can adjust the design to limit interference in the first place, or we can model interference and adjust the analysis.\n\n\n9.2.1 Design-based approaches\n\n9.2.1.1 Split resources\n\nIf shared resources cause the issue (e.g. budget for ads), then splitting the resources is an option (i.e. 50% of resources for a variance that gets 50% of the traffig)\n\n\n\n9.2.1.2 Cluster-based randomisation approaches\n\nThis approache first splits the units into clusters that are disjoint (or as disjoint as possible). Randomisation then happens at the cluster level, and all units within a cluster receive the same treatment.\nBecause units will interact only (or mostly) with units that have the same treatment status as they themselves, this design provides a better counterfacturals for the all-treated and all-controlled alternatives.\nOne challenge is creating clusters: effectively, you need to find the clusters between which there is no interactions, so that externalities don’t spill across cluster boundaries. Examples:\n\nGeographical clustering:\n\nto experiment on currier behaviour in a food delivery network: randomise at a level at which curriers don’t interact (such as a part of a large city, or even between cities).\nIn a online-dating network, clusters may be geographical regions between which there is little interaction between users (i.e. locations to far away for people to wanting to find partners)\n\nsocial/interaction-graph-clustering:\n\nTo test a feature in a social-network app: create graphs of user interactions, and partition the graph such that there is maximal within-group interaction and minimal between group interaction\n\n\nAnother challenge is ensuring you have enough effective sample size: if the outcomes of users within a group are correlated, then randomising at a level higher than the unit reduces effective sample size. In the extreme case, where within group correlation is perfect, your effective sample size is the number of groups, not the number of units.\nEffectively, we face a variance-bias tradeoff: the fewer clusters we have the larger and more isolated they are, which leads to low bias but large variance and vice versa.\n\nImplications for analysis\n\nHow to adjust standard errors? See Abadie et al. (2023) and Chaisemartin and Ramirez-Cuellar (2024)\nIf we have randomised at the group level: either analyse the data at the group level, or, if analysing the data at the individual level, use clustered standard errors.\n\nEffect on power\n\nReduces power because observations within clusters are almost surely correlated. Intuition:\n\nIt effectively reduces sample size (if all observations within a cluster are identical, then our number of observations equals the number of clusters).\nIt makes the sample less diversified and hence less representative of the population so that different estimates based on different samples will vary more widely, which leads to a larger standard error.\n\nThe effect depends on the number of clusters (the more the more power we have) and on the intra-cluster correlation (the less correlation the higher the power).\n\n\n\n9.2.1.3 Ego-clusters\n\nInstead of traditional clusters, create much smaller, unit-based clusters. See Saint-Jacques et al. (2019)\nBasically: you create clusters based on a central node (the ego) and its direct links (the alters), and randomise egos and alters seprately.\n\n\n\n9.2.1.4 Switchback experiments\n\nAn approach often used to deal with marketplace interferance, whereby all units are frequently switched from being in control to being in treatment.\nThere could be carry-over issues with this, but this can be dealt with by using “burn-in” periods.\nAnother issue is that, like cluster-randomisation, switchbacks also suffer from lower power.\n\n\n\n\n9.2.2 Analysis-based approaches\n\nAnalysis based approaches model the interference and then adjust the analysis accordingly. See last paragraph of section 6 in Larsen et al. (2023) for details. Also, Bojinov, Simchi-Levi, and Zhao (2023), in their introduction, differentiate between approaches that rely on game theoretic, Markov Chain, and network approachs to model interference.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Network experiments</span>"
    ]
  },
  {
    "objectID": "chapters/network_experiments.html#measuring-interaction-effects",
    "href": "chapters/network_experiments.html#measuring-interaction-effects",
    "title": "9  Network experiments",
    "section": "9.3 Measuring interaction effects",
    "text": "9.3 Measuring interaction effects\nOverall approach: if there are no interaction effects, different estimands should be the same\nDesign side\n\nrun clauster and unit-level experiment side-by-side. Different results imply presence of interaction effects\nWe can design differences in treatment density by varying the allocation fraction for different groups.\nWe can use a multistage randomisation to allocate treatment first at the group level and then, within the group, at the individual level. This creates a treatment group (the treated in the treatment group), a spillover control group (the untreated in the treatment group), and a pure control group (those in the nontreatment group).\n\nAnalysis side\n\nexposure modelling / reweighting\nuse of focal units\nWe can exploit natural variation in treatment density (number of children eligible for deworming program within a geographic area).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Network experiments</span>"
    ]
  },
  {
    "objectID": "chapters/network_experiments.html#useful-resources",
    "href": "chapters/network_experiments.html#useful-resources",
    "title": "9  Network experiments",
    "section": "9.4 Useful resources",
    "text": "9.4 Useful resources\n\nLarsen et al. (2023), in section 6, provides a recent review of the literature.\nKarrer et al. (2021) describe network experimentation at Meta.\n\n\n\n\n\nAbadie, Alberto, Susan Athey, Guido W Imbens, and Jeffrey M Wooldridge. 2023. “When Should You Adjust Standard Errors for Clustering?” The Quarterly Journal of Economics 138 (1): 1–35.\n\n\nBojinov, Iavor, David Simchi-Levi, and Jinglong Zhao. 2023. “Design and Analysis of Switchback Experiments.” Management Science 69 (7): 3759–77.\n\n\nChaisemartin, Clément de, and Jaime Ramirez-Cuellar. 2024. “At What Level Should One Cluster Standard Errors in Paired and Small-Strata Experiments?” American Economic Journal: Applied Economics 16 (1): 193–212. https://doi.org/https://doi.org/10.1257/app.20210252.\n\n\nKarrer, Brian, Liang Shi, Monica Bhole, Matt Goldman, Tyrone Palmer, Charlie Gelman, Mikael Konutgan, and Feng Sun. 2021. “Network Experimentation at Scale.” In Proceedings of the 27th Acm Sigkdd Conference on Knowledge Discovery & Data Mining, 3106–16.\n\n\nLarsen, Nicholas, Jonathan Stallrich, Srijan Sengupta, Alex Deng, Ron Kohavi, and Nathaniel T Stevens. 2023. “Statistical Challenges in Online Controlled Experiments: A Review of a/b Testing Methodology.” The American Statistician, 1–15.\n\n\nLiu, Min, Jialiang Mao, and Kang Kang. 2020. “Trustworthy Online Marketplace Experimentation with Budget-Split Design.” arXiv Preprint arXiv:2012.08724.\n\n\nSaint-Jacques, Guillaume, Maneesh Varshney, Jeremy Simpson, and Ya Xu. 2019. “Using Ego-Clusters to Measure Network Effects at LinkedIn.” arXiv Preprint arXiv:1903.08755.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Network experiments</span>"
    ]
  },
  {
    "objectID": "chapters/switchbacks.html#notes-on-bojinov2023design",
    "href": "chapters/switchbacks.html#notes-on-bojinov2023design",
    "title": "10  Switchback experiments",
    "section": "",
    "text": "AB tests have two main challenges in practice: dealing with interaction effects, and estimating heterogeneous treatment effects.\nSwitchback experiments sequentially assign units to a random treatment, measure the response, and repeat the proceedure for a fixed period of time.\nThe approach can thus deal with both limitations above: it limits interference, and it can estimate individual-level causal effects, thus providing the ability to estiamte heterogeneous treatment effects.\nInstead of making assumptions on the outcome model under interference, switchback experiments require assumptions on the duration of carryover effects, the duration for which the effects of one treatment during a particular period affects outcomes in subsequent periods (the authors call this carryover duration the “order of carryover effects”).\nThere are two main areas of application: to deal with interference in a network (either in the context of network or marketplace interference) and to deal with situations where we have a limited number of units in the experiment and where we expect heterogeneous treatment effects.\nThere are three main challenges when running switchback experiments:\n\nATE estimators from switchback experiments have high variance because the precision is a function of the total number of assignments.\nOne has to deal with the existence of carryover effects.\nSuper-population inference requires unrealistic assumptions.\n\nThe paper provides solutions for all of them:\n\nIt provides an optimal design approach that reduces variance\nIt assumes carryover effects and shows that estimation and inference is valid both when they are correctly and incorrectly specified, though in the latter case estimation variance is higher. The authors also provide a method for practitioners to measure the duraton of carryover effects.\nIt takes a purely design-based perspective on inference by assuming that outcomes are unknown but fixed, which means that findings are wholy non-parametric and robust to model misspecification (akin to the approach of Fisher’s exact P-value approach)\n\nAssumptions:\n\n\n\n\nThis says that potential outcomes don’t depend on future assignments. Given that we control the assignment mechanisms, this holds by design (i.e. units can’t adapt their behaviour in a given period based on future assignments because these assignments are random).\n\n\n\n\nThis says that outcomes at time \\(t\\) are independent of assignenments more than \\(m\\) periods in the past.\nTogether, the two assumption imply that for any \\(t \\in \\{m + 1, \\ldots, T\\}\\) and any two assignment paths \\(w, w' \\in \\{0, 1\\}^{m+1}\\), whenever \\(w = w'\\) this leads to:\n\n\n\nWhich is a rigorous way of waying that all that matters do determine potential outcomes at time \\(t\\) is the assignment history of the previous \\(m\\) periods.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Switchback experiments</span>"
    ]
  },
  {
    "objectID": "chapters/switchbacks.html#useful-resources",
    "href": "chapters/switchbacks.html#useful-resources",
    "title": "10  Switchback experiments",
    "section": "10.2 Useful resources",
    "text": "10.2 Useful resources\n\nBojinov, Simchi-Levi, and Zhao (2023)\nBojinov and Shephard (2019)\nExperiment Rigor for Switchback Experiment Analysis\n\n\n\n\n\nBojinov, Iavor, and Neil Shephard. 2019. “Time Series Experiments and Causal Estimands: Exact Randomization Tests and Trading.” Journal of the American Statistical Association 114 (528): 1665–82.\n\n\nBojinov, Iavor, David Simchi-Levi, and Jinglong Zhao. 2023. “Design and Analysis of Switchback Experiments.” Management Science 69 (7): 3759–77.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Switchback experiments</span>"
    ]
  },
  {
    "objectID": "chapters/heterogeneous_treatment_effects.html#good-segments",
    "href": "chapters/heterogeneous_treatment_effects.html#good-segments",
    "title": "17  Heterogeneous treatment effects",
    "section": "17.1 Good segments",
    "text": "17.1 Good segments\n\nMarket or country\nDevice or platform\nTime of day or day of week\nUser type (new vs existing, high-use vs low-use)\nAccount characteristics (single vs joint account)"
  },
  {
    "objectID": "chapters/heterogeneous_treatment_effects.html#issues-to-be-aware-of",
    "href": "chapters/heterogeneous_treatment_effects.html#issues-to-be-aware-of",
    "title": "17  Heterogeneous treatment effects",
    "section": "17.2 Issues to be aware of",
    "text": "17.2 Issues to be aware of\n\nUsers changing segment can lead to misleading results: if I make changes on feature F, and compare users of F with those who don’t use F. Suppose avg of sessions per wk is 20 for users, 10 for non-users. Now suppose that my change makes everyone with 15 sessions per wk who previously used F to stop using F. Avg for users of F will increase, that for non-users will also increase. But this has nothing to do with overall avg sessions per wk, which could go in any direction. – Solution: if possible, only look at segments that are fixed before the start of the experiment. If you cannot do that, then beware of this issue, and look at aggregate metrics.\nSimpson’s paradox"
  },
  {
    "objectID": "chapters/heterogeneous_treatment_effects.html#simpsons-paradox",
    "href": "chapters/heterogeneous_treatment_effects.html#simpsons-paradox",
    "title": "17  Heterogeneous treatment effects",
    "section": "17.3 Simpson’s paradox",
    "text": "17.3 Simpson’s paradox\n\nSimpson’s paradox basically happens due to a kind of omitted variable bias.\nIt can occus if pooling data overlooks a confounding driver of the data generation that affects the data in specific ways.\nIn particular, the underlying data needs to be such that:\n\nOutcomes differ by the level of the confounding variable\nThe difference in outcome between these levels is larger than the differences in outcome within each level\n\nThis is very abstract – I’m aiming to rewrite this over time.\nThe paradox has two classic manifestations: frequency Tables and correlations.\nFor frequency Tables, classic examples are listed on Wikipedia. Notice that in all the examples, the difference in outcomes between levels of the confounder (departments, stone size, and year) is much larger than the differences between the groups of interest (men and women, treatments, batters) at any given level of the confounder.\nFor correlations, the below gif makes things very clear.\n\n\n\n\nPace~svwiki, CC BY-SA 4.0 https://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons"
  },
  {
    "objectID": "chapters/heterogeneous_treatment_effects.html#resources",
    "href": "chapters/heterogeneous_treatment_effects.html#resources",
    "title": "17  Heterogeneous treatment effects",
    "section": "17.4 Resources",
    "text": "17.4 Resources\n\nBojinov, Saint-Jacques, and Tingley (2020)\n\n\n\n\n\nBojinov, Iavor, Guillaume Saint-Jacques, and Martin Tingley. 2020. “Avoid the Pitfalls of a/b Testing.” Harvard Business Review 98 (2): 48–53."
  },
  {
    "objectID": "chapters/ethics.html",
    "href": "chapters/ethics.html",
    "title": "10  Ethics",
    "section": "",
    "text": "Some useful principles to consider, based on reading chapter 9 in Kohavi, Tang, and Xu (2020)\n\nOnly test changes that Company policy would allow you to roll out to 100 percent of users i.e. for Meta, deliberately showing them negative content wouldn’t qualify. Personally I’m not sure I agree. I think in the Meta example, it would be useful to understand the effect of exposure to negative content. I’m not gonna think about this deeply now, but I think adapting the rule to not showing levels of content users couldn’t organically be exposed to on the platform might be more useful. In the Meta example, this would allow for studying the effect of negative content in a systematic way without making the experience worse for treatment users than it actually is for some users (and could well be for treatment users, too). The reason for my willingness to entertain to go further is that there is a potentially large benefit to understanding harm. Yes, there might be some cost in the very short term (and you’d obviously want ot bound that cost somehow, as I proposed above, in order to guard against slippery slopes), but if the insights gained allow you to prevent large harm indefinitely later, then that’s worth considering. Two other thoughts: motivation clearly matters here. And: the possibility of a slippery slope is not generally an argument to not do something – often, as here – there are quite natural and objective ways to draw a line on how far one would be willing to go.\nAim for equipoise: this is a situation where, ex-ante, there is no grounds to favour one variant over another. This is the normal case. (The term is borrowed from clinical trial, where clinical equipoise is the assumption in an RCT that no drug is ex-ante better than another).\nSome worthwhile experiments violate equipoise: increasing latency, disabling feature, showing more adds, all aim to help us collect data which can be useful to make tradeoffs later on, which, ultimately, can benefit users.\nBeware of behavioural experiments and deception.\nPresumptive consent: ask a small subset of users whether they would be okay participating and if they do, assume that the sentiment would generalise.\nDifferent from clinical trials, subjects in online trials usually have the opportunity to switch service (for sth like FB, this might be difficult).\n\nWhat are Salganik (2018)’s three principles of ethical design? Why should we care?\n\nReplace: use quasi-experiments whenever possible.\nRefine: make interventions as harmless as possible.\nReduce: use the minimum necessary number of participants (use power analysis to make sure you’re not over-powered).\nWhy all this? Because we can never know whether an intervention is not harmful to at least some participants.\n\n\n\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to a/b Testing. Cambridge University Press.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "chapters/power.html",
    "href": "chapters/power.html",
    "title": "6  Power",
    "section": "",
    "text": "6.1 Theory\nPower basics\n\\[ y = \\alpha + \\beta T + \\epsilon\\]\n\\[\nstd = \\sqrt{\\frac{\\sigma^2}{N_t} + \\frac{\\sigma^2}{N_c}} = \\sqrt{\\frac{\\sigma^2}{PN} + \\frac{\\sigma^2}{(1-P)N}} = ... = \\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}}\n\\]\n\\[ \\delta = (t_a + t_{1-\\kappa}) * \\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}} \\]\n\\[ N =  \\frac{(t_a + t_{1-\\kappa})^2}{P(1-P)}\\left(\\frac{\\sigma}{\\delta}\\right)^2 \\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/stats_foundations.html",
    "href": "chapters/stats_foundations.html",
    "title": "2  Statistics foundation",
    "section": "",
    "text": "2.1 Sampling",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics foundation</span>"
    ]
  },
  {
    "objectID": "chapters/regression.html",
    "href": "chapters/regression.html",
    "title": "3  Regression",
    "section": "",
    "text": "3.1 Glossary\nTODO: - Linear regression vs OLS",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_design.html",
    "href": "chapters/experiment_design.html",
    "title": "8  Experiment design",
    "section": "",
    "text": "8.1 Designs\nClassic RCT\nClustered\nStep-wedge / phase-in design\nWait-list design\nFactorial design\nEncouragement design\nWhat’s the difference between a “between-subjects design” and a “within-subjects design” and what are their advantages? What is a “mixed-design”?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Experiment design</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_design.html#choosing-unit-of-randomisation",
    "href": "chapters/experiment_design.html#choosing-unit-of-randomisation",
    "title": "8  Experiment design",
    "section": "8.2 Choosing unit of randomisation",
    "text": "8.2 Choosing unit of randomisation\nFactors to consider: - Spillovers - The level of treatment administration - The level of measurement (randomisation level needs to be the same or higher) - Power - Attrition and compliance (within-group randomisation may lead to resentment among participants and staff) - Feasibility (what is easiest, cheapest, politically possible, …)"
  },
  {
    "objectID": "chapters/experiment_design.html#dimensions-along-which-we-can-vary-randomisation",
    "href": "chapters/experiment_design.html#dimensions-along-which-we-can-vary-randomisation",
    "title": "8  Experiment design",
    "section": "8.3 Dimensions along which we can vary randomisation",
    "text": "8.3 Dimensions along which we can vary randomisation\nAspect: - access - access around cutoff - timing - encouragement\nLevel: - individual - group"
  },
  {
    "objectID": "chapters/experiment_design.html#the-assignment-mechanism",
    "href": "chapters/experiment_design.html#the-assignment-mechanism",
    "title": "8  Experiment design",
    "section": "8.4 The assignment mechanism",
    "text": "8.4 The assignment mechanism\n\n8.4.1 Bernoulli trials\n\n\n8.4.2 Completely randomised experiments\n\n\n8.4.3 Stratified randomised experiments\nStratification is a treatment allocation method in which the pool of eligible units is first divided into strata (or groups or blocks) so as to ensure balance along certain variables. Random assignment is then performed within each group.\nEffect: Random allocation of treatment ensures that control and treatment group will be similar in expectation, stratification ensures that along certain dimensions they will be in practice. If we stratify by gender and language, for instance, we make sure that both control and treatment group have the same proportion of women who speak a given language.\nAdvantages: 1.) It can increase precision of the estimator by lowering residual variance (intuitively, it has the opposite effect of clustering: it makes samples more representative of the overall population, which means that repeated samples will be more similar, which means that the standard error of the estimator is smaller). 2.) It allows analysis by subgroups.\nDisadvantages: it can lead to very different sample sizes in different strata.\nWhen to stratify? - We should stratify if we want to: - increase power - achieve balance - analyse data by subgroups\n…and what variables to use? Use variables that - are highly correlated with the outcome variable - are discrete (can discretise by creating buckets) - are the subgroups of interest\nHow to adjust analysis? Include dummy for each variable used for stratification in the regression. (explain why…)\n\n\n8.4.4 Paired randomised experiments\nPaired randomisation is an extreme form of stratification where the size of each stratum equals the number of treatment cells.\nIf we have three cells (two treatments and one control) and we pair based on test scores, we pick groups of three students, starting with the three highest scoring ones, and randomly assign one of them to each cell.\nThe aim is to increase power and achieve balance, and it’s particularly useful if we have small samples.\nThe danger is that if we have attrition and one of the three students drops out, we lose all three observations (OLS will drop all observations with the relevant dummy because there is a missing value). One remedy is to have strata of size double or triple the cell number.\n\n\n8.4.5 Matching\nHow to adjust analysis? Matching is an extreme form of stratifying, and so the approach is the same: we include a dummy for each (but one) matched pair. (If we have matched on test scores, we include a dummy for all but one possible test scores)."
  },
  {
    "objectID": "chapters/experiment_design.html#treatment-effects",
    "href": "chapters/experiment_design.html#treatment-effects",
    "title": "8  Experiment design",
    "section": "8.7 Treatment effects",
    "text": "8.7 Treatment effects\nATE is the average difference in potential outcomes of all subjects. E[y1 - y0]\nATE is a weighted average of ATET and average treatment effect of untreated.\nATE is also a weighted average of the compliers, always-takers, and never-takers.\nATE = ATET if treatment effects are homogenous (if they are not, then experiment measures ATET).\nATE = ITT under homogenous effects or under perfect compliance.\nATET (or TOT) is the average difference in potential outcomes of treated subjects. E[y1 - y0 | D=1] (D is treatment)\nATET = LATE in an experiment with no always-takers. This is relevant because there are many cases of onesided non-compliance (if participants who are randomised to treatment do not take it up but nobody who wasn’t randomised to treatment has access).\nITT is the average difference in potential outcomes of those assigned to treatment (a weighted average between those who accepted treatment and those who didn’t).\nLATE is the average difference in potential outcomes of compliers."
  },
  {
    "objectID": "chapters/experiment_design.html#imperfect-compliance",
    "href": "chapters/experiment_design.html#imperfect-compliance",
    "title": "8  Experiment design",
    "section": "8.8 Imperfect compliance",
    "text": "8.8 Imperfect compliance\nPartial complience\n\nPartial compliance occurs if, for some reason, some people in the treatment group are not treated or some people in the control group are.\nIt’s problematic because it reduces the difference in treatment exposure between treatment and control group (in the extreme, if they are equal, we learn nothing), and because they might make treatment takeup non-random.\nA few things can help to limit the problem: make takeup easy and/or incentivise it, randomise at a higher level, and provide a treatment to everyone.\nWe can adjust our analysis by calculating LATE, either using the Wald estimator (ITT / difference in take-up between treatment and control groups) or 2SLS where we instrument the behaviour we want to encourage with the treatment dummy (and possibly other covariates).\nDo not drop non-compliers or re-assign them to the control group – compliers and non-compliers are different so that dropping or reclassifying non-compliers would re-introduce self-selection into out two samples, which defeats the whole point of the randomisation.\n\nDefiers\n\nThey are the opposite of compliers: they either take up the treatment because they were assigned to control or the other way around.\nThey might occur in an encouragement design if they overestimated the benefit of treatment and got discouraged by the information provided in the treatment.\nThey might make us significantly misinterpret the true effect (RRE page 303, or, better, MHE p 156).\nWe can deal with them only if they form an identifiable subgroup, in which case we can estimate the treatment effect on defiers and compliers separately and calculate an average treatment effect.\n\nIn an experiment with one-sided non-compliance what does IV estimate 1. if there are no always-takers and 2. if there are not never-takers?\n\nIn general, IV estimates LATE, the effect on compliers, and the treated consist of compliers and always-takers, while the non-treated consist of compliers and never-takers.\nIf there are no always-takers, the population of compliers is the same as the population of the treated, so that LATE = ATET.\nIf there are no never-takers, the population of compliers is the same as that of the untreated, so LATE = average treatment effect of the untreated."
  },
  {
    "objectID": "chapters/experiment_design.html#attrition",
    "href": "chapters/experiment_design.html#attrition",
    "title": "8  Experiment design",
    "section": "8.9 Attrition",
    "text": "8.9 Attrition\nAttrition is when, for some reason, we cannot collect endline data on some units.\nIt’s a problem because when it’s systematic rather than random, treatment and control group are no longer comparable, which is a threat to internal validity. (Even if the same number of people drop out, if they are different type, the problem is the same.) Also, it reduces sample size and thus power.\nWe can limit it if we promise access to the program to everyone (phase-in design), change the level of randomisation, and improve data collection.\nIn our analysis we should 1. Report the extent of attrition, 2. Check for differential attrition (between groups, and within groups based on observable characteristics), and 3. Determine the range of estimates given attrition using a selection model or bounds.\nOne method is to use Heckman’s selection model: we look at the characteristics of those who attrite, assume that they have the same outcomes as those with the same characteristics for which we have data, and then fill in their outcome variables accordingly.\nAnother method is to use bounds. There are two types.\nManski-Horowitz bounds: Lower bound: replace all missing values in treatment with the least favourable outcome value from the observed sample and replace all missing values in control with most favourable value in the observed sample. Upper bound created in a reverse way. Unless attrition is low and the outcome variable is tightly bounded, this tends to lead to very large bounds.\nLee bounds: We treat the estimate from the available data as an upper bound, and construct the lower bound by trimming from the sample with less attrition the observations that most contribute to the treatment effect."
  },
  {
    "objectID": "chapters/experiment_design.html#interpretation",
    "href": "chapters/experiment_design.html#interpretation",
    "title": "8  Experiment design",
    "section": "8.10 Interpretation",
    "text": "8.10 Interpretation\n\nWe often think of experiment as answering: “Does the intervention work?”, but what it really answers is “What is the average effect of the intervention with this specific implementation for this particular population at this time?”\nWhen we have multiple treatments or analyse subgroups separately, we need to test whether different estimates are significantly different from each other (can’t just look at whether some are significantly different from zero while some aren’t)"
  },
  {
    "objectID": "chapters/threats_to_validity.html",
    "href": "chapters/threats_to_validity.html",
    "title": "7  Threats to validity",
    "section": "",
    "text": "7.1 Threats to reliability",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Threats to validity</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html",
    "href": "chapters/variance_reduction.html",
    "title": "9  Variance reduction",
    "section": "",
    "text": "9.1 Why reduce variance?\nTODO: - Explain that we wanna increase power (refer to power section) - Reducing variance is one way to increase power (often the only feasible one)\nThe estimand of interest is the population average treatment effect:\n\\[\n\\tau = \\bar{y}_t - \\bar{y}_c.\n\\]\nUnder random treatment assignment, an unbiased estimator of \\(\\tau\\) is the difference in means between treatment and control units in the sample:\n\\[\n\\hat{\\tau}^{dif} = \\bar{y}_t^{obs} - \\bar{y}_c^{obs}.\n\\]\nThe variance of \\(\\hat{\\tau}^{dif}\\) is given by:\n\\[\n\\mathbb{V}(\\hat{\\tau}^{dif}) = \\frac{s_t}{N_t} + \\frac{s_c}{N_c},\n\\]\nwhere\n\\[\ns_t = \\frac{1}{N_t - 1}\\sum_{\\text{i:d=1}}(y_i - \\bar{y}_t^{obs})^2, \\quad s_c = \\frac{1}{N_c - 1}\\sum_{\\text{i:d=0}}(y_i - \\bar{y}_c^{obs})^2.\n\\]\nHence: for a given sample size, we can reduce \\(\\mathbb{V}(\\hat{\\tau}^{dif})\\) by reducing the variance in the outcome metric, \\(Y\\) – this is the variance we are trying to reduce.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variance reduction</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html",
    "href": "chapters/metrics.html",
    "title": "5  Metrics",
    "section": "",
    "text": "5.1 Why good metrics matter",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/switchbacks.html",
    "href": "chapters/switchbacks.html",
    "title": "10  Switchback experiments",
    "section": "",
    "text": "10.1 Notes on Bojinov, Simchi-Levi, and Zhao (2023)\nAssumption 1 (Non-anticipating Potential Outcomes).\nFor any \\(t \\in [T]\\), \\(w_{1:t} \\in \\{0,1\\}^t\\), and for any \\(w', w'' \\in \\{0,1\\}^{T-t}\\), \\[ Y_t(w_{1:t}, w'_{t+1:T}) = Y_t(w_{1:t}, w''_{t+1:T})\\]\nAssumption 2 (m-Carryover Effects).\nThere exists a fixed and given m, such that for any \\(t \\in \\{m+1, m+2, \\ldots, T\\}\\), \\(w \\in \\{0,1\\}^{T-t+m+1}\\), and for any \\(w', w'' \\in \\{0,1\\}^{t-m-1}\\), \\[ Y_t(w'_{1:t-m-1}, w_{t-m:T}) = Y_t(w''_{1:t-m-1}, w{t-m:T})\\]\n\\[\nY_t(w_{1:T})=Y_t(w'_{1:T})\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Switchback experiments</span>"
    ]
  },
  {
    "objectID": "chapters/projection.html",
    "href": "chapters/projection.html",
    "title": "18  Projection",
    "section": "",
    "text": "18.1 Projecting from 2-D onto 1-D\nProjecting a vector in two-dimensional space onto a line that goes through the origin is a nice way to build an understanding for what a projection does.2\nSay we want to orthogonally project the vector \\(b\\) onto a line defined by another vector, \\(a\\), and we will call the resulting projection \\(p\\). Hence, \\(p\\) is the point on the line defined by \\(a\\) that is nearest to the (tip of) the vector \\(b\\). (Note, though, that \\(p\\) is a vector, not a point. Calling it a point just clarifies what we’re doing here.)\nWe can think of the line as being generated by scaling vector \\(a\\) with a scalar \\(x\\), so that choosing a suitable \\(x\\) allows us to reach any point on the line. Finding \\(p\\) then boils down to finding the value of \\(x\\) that gets us to that point of the line that is closest to \\(b\\). We can thus write \\(p = ax\\).\nLet’s start by finding \\(p\\). We can find it in different ways.\nUsing calculus:\nGiven that we define closeness based on the Euclidean distance, minimising the distance between the (tip of) the vector, \\(b\\), and the projection, \\(p\\), is akin to solving the following problem:\n\\[\n\\begin{aligned}\nargmin_{x} \\sqrt{\\sum_{i=1}^2{(b_i - p_i)^2}} &= argmin_{x} \\sum_{i=1}^2{(b_i - p_i)^2} \\\\\n&= argmin_{x} \\sum_{i=1}^2{(b_i - xa_i)^2} \\\\\n&= argmin_{x} (b - xa)'(b - xa),\n\\end{aligned}\n\\]\nCalculating the derivative with respect to \\(x\\) we get:\n\\[\n\\begin{aligned}\n\\frac{d}{dx} (b - xa)'(b - xa) &= (-a)'(b - xa) + (b - xa)'(-a) & \\\\\n&= -a'b + xa'a - a'b + xa'a \\\\\n&= -2a'b + 2xa'a\n\\end{aligned}\n\\]\nFinlly, setting the result to 0 and solving for \\(x\\) we get:\n\\[\n\\begin{aligned}\n-2a'b + 2xa'a &= 0 \\\\\nxa'a &= a'b \\\\\nx &= (a'a)^{-1}a'b\n\\end{aligned}\n\\]\nHence, given that \\(p = ax\\), we have:\n\\[\n\\begin{aligned}\np = ax = \\underbrace{a(a'a)^{-1}a'}_\\text{$P_a$}b,\n\\end{aligned}\n\\]\nwhere \\(P_a\\) is the projection matrix.\nLet’s reflect for a moment what this all means. In general, pre-multiplying a vector by a matrix transforms the vector in a particular way. When we perform orthogonal projection, we pre-multiply a vector by a matrix that transforms the vector into that point on a subspace that it closest to the original vector. In our case here, pre-multiplying our initial vector \\(b\\) by the projection matrix \\(P_a\\) transforms \\(b\\) into that point on \\(a\\) that is closest to \\(b\\), which we call \\(p\\). Given that we define “nearest” using the Euclidean distance, it makes sense that the projection matrix would emerge out of the solution to the minimisation problem of finding the point on the subspace that minimises the Euclidean distance to the original vector.\nUsing basic geometry:\nWe could also find \\(p\\) using our understanding of basic geometry. Looking at the figure above, it is intuitively obvious that the shortest path between the tip of \\(b\\) and the line \\(a\\) is that which is perpendicular to \\(a\\) – the point at which the perpendicular line meets \\(a\\) is thus \\(p\\). The path between \\(b\\) and \\(p\\) is simply \\(b - p\\). In linear algebra terms, we thus want that path to be orthogonal to the line \\(a\\).\nHence, we want:\n\\[\n\\begin{aligned}\na'(b - p) &= 0 \\\\\na'(b - xa) &= 0 \\\\\na'b - xa'a &= 0 \\\\\nxa'a &= a'b \\\\\nx &= (a'a)^{-1}a'b\n\\end{aligned}\n\\]\nSo that, again, we have:3\n\\[\np = ax = \\underbrace{a(a'a)^{-1}a'}_\\text{$P_a$}b.\n\\]\nThis also makes clear why this type of projection is called “orthogonal projection”: we want to project a vector onto a subspace in such a way that the distance between the original vector and the subspace is minimal. The resulting projection will be a point on the subspace such that a vector from that point to the original vector is orthogonal to the subspace. Intuitively, this is the case because the shortest path between the vector and the subspace will be that which is perpendicular to the subspace, and orthogonality is the generalisation of the notion of perpendicularity.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Projection</span>"
    ]
  },
  {
    "objectID": "chapters/projection.html#projecting-from-2-d-onto-1-d",
    "href": "chapters/projection.html#projecting-from-2-d-onto-1-d",
    "title": "18  Projection",
    "section": "",
    "text": "Projecting a 2d vector onto a line\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrthogonal vectors\n\n\n\n\n\nOrthogonality is a generalisation of perpendicularity; two vectors that are orthogonal are perpendicular to each other. The word comes from the Greek “orthos”, meaning “straight” or “right”, and “gonia”, meaning “angle”. To test whether two vectors \\(x\\) and \\(y\\) are perpendicular, we check whether their dot product, \\(x'y\\) equals zero.\nBut why do we know that the dot-product of orthogonal vectors is zero? The answer follows from the Pythagorean theorem, which states that for a right triangle with sides \\(a\\) and \\(b\\) and hypotenuse \\(c\\) we have \\(a^2\\) + \\(b^2\\) = \\(c^2\\). Now, if vectors \\(x\\) and \\(y\\) are orthogonal, they form the sides of a right triangle, where the hypotenuse is given by \\(x + y\\). By the Pythagorean theorem, we thus have\n\\[\n||x||^2 + ||y||^2 = ||x + y||^2,\n\\]\nwhich equals\n\\[\n\\left(\\sqrt{x'x}\\right)^2 + \\left(\\sqrt{y'y}\\right)^2 = \\left(\\sqrt{(x + y)'(x + y)}\\right)^2,\n\\]\nand simplifies to\n\\[\n\\begin{aligned}\nx'x + y'y &= (x + y)'(x + y) \\\\\nx'x + y'y &= x'x + y'y + x'y + y'x \\\\\nx'x + y'y &= x'x + y'y + 2x'y,\n\\end{aligned}\n\\]\nfrom which it is clear that \\(x'y = 0\\).",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Projection</span>"
    ]
  },
  {
    "objectID": "chapters/projection.html#why-project",
    "href": "chapters/projection.html#why-project",
    "title": "18  Projection",
    "section": "18.2 Why project?",
    "text": "18.2 Why project?\nOne reason projection is useful is because it allows us to approximately solve systems of linear equations that have no exact solution. Imagine we have the following system of equations:\n\\[\n\\begin{align*}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1k}x_k &= b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2k}x_k &= b_2 \\\\\n\\vdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nk}x_k &= b_n\n\\end{align*}\n\\]\nwhich we can write more compactly in matrix form as:\n\\[\nAx = b,\n\\]\nwhere\n\\[\nA = \\begin{pmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  a_{n1} & a_{n2} & \\cdots & a_{nk}\n\\end{pmatrix},\n\\quad\nx =\n\\begin{pmatrix}\n  x_{1} \\\\\n  x_{2} \\\\\n  \\vdots \\\\\n  x_{k}\n\\end{pmatrix},\n\\quad\nb =\n\\begin{pmatrix}\n  b_{1} \\\\\n  b_{2} \\\\\n  \\vdots \\\\\n  b_{n}\n\\end{pmatrix}.\n\\]\nIf \\(n &gt; k\\), the system is overdetermined – it has more constarints (equations) than degrees of freedom (variables) – and might not have a solution. In this case, it can be useful to solve\n\\[\nAx = \\hat{b},\n\\]\nwhere \\(\\hat{b}\\) is the orthogonal projection of \\(b\\) onto the vector space spanned by \\(A\\). This vector space is also called the span or column space of \\(A\\). Using orthogonal projection achieves two things: first, it guarantees a solution because \\(\\hat{b}\\) lies on the same space as \\(Ax\\) – they both lie on the subspace spanned by the columns of \\(A\\) – and, second, it makes \\(\\hat{p}\\) the closest approximation to \\(b\\) in terms of the Euclidean distance.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Projection</span>"
    ]
  },
  {
    "objectID": "chapters/projection.html#projection-from-3-d-onto-2-d-and-projecting-onto-n-d",
    "href": "chapters/projection.html#projection-from-3-d-onto-2-d-and-projecting-onto-n-d",
    "title": "18  Projection",
    "section": "18.3 Projection from 3-D onto 2-D and projecting onto N-D",
    "text": "18.3 Projection from 3-D onto 2-D and projecting onto N-D\nAs in the 2-D onto 1-D example above, we’re going to project the vector \\(b\\) onto a subspace. But \\(b\\) is now a 3-D vector, and instead of projecting onto a line characterised by vector \\(a\\), we’re projecting onto a 2-D plane characterised by the 2 x 2 matrix \\(A\\). Hence, our projection \\(p\\) is now a vector in the 3-dimensional space that lies on the 2-dimensional subspace. Similarly to above, \\(p\\) is defined by \\(p = Ax\\), where \\(x\\) is a 2-D vector of scalars.\nWe can still go about finding \\(p\\) in the same way as above:\n\\[\n\\begin{aligned}\nA'(b - p) &= 0 \\\\\nA'(b - Ax) &= 0 \\\\\nA'b - A'Ax &= 0 \\\\\nA'Ax &= A'b \\\\\nx &= (A'A)^{-1}A'b\\\\\n\\\\\np = Ax = \\underbrace{A(A'A)^{-1}A'}_\\text{$P_a$}b.\n\\end{aligned}\n\\]\nThe last step above works only if \\(A'A\\) is actually invertible, which is the case if \\(A\\) has full rank.\n\n\n\n\n\n\nFull rank\n\n\n\n\n\nA matrix is said to have full rank if none of its columns can be constructed from a linear combination of any other columns.\nWe can think of the columns of a matrix are the basis vectors of its column space, and the rank of the matrix is the number of dimensions of that column space.\nFull rank means thus means that the column space has as many dimensions as there are columns.\n\n\n\nThe math above generalises directly to projections onto N-dimensional space. All that changes is that \\(b\\), \\(A\\), \\(x\\), and \\(p\\) are higher-dimensional objects, and that visualising what’s happening becomes rather mind-bending.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Projection</span>"
    ]
  },
  {
    "objectID": "chapters/projection.html#useful-references",
    "href": "chapters/projection.html#useful-references",
    "title": "18  Projection",
    "section": "18.4 Useful references",
    "text": "18.4 Useful references\n\nGilbert Strang’s linear algebra lectures at MIT\n10 Fundamental Theorems for Econometrics",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Projection</span>"
    ]
  },
  {
    "objectID": "chapters/projection.html#footnotes",
    "href": "chapters/projection.html#footnotes",
    "title": "18  Projection",
    "section": "",
    "text": "The Euclidean distance between two points \\(x\\) and \\(\\bar{x}\\) in \\(\\mathbb{R}^N\\) is defined as \\(\\sqrt{\\sum_{i=1}^N{(\\bar{a_i} - a_i})^2}\\).↩︎\nA line through the origin is a subspace of a two-dimensional vector because it is 1-dimensional (and thus a subset of the 2-dimensional vector) and because all possible linear combinations of vectors on the line will also lie on the line (see box on subspaces for more details).↩︎\nOne thing I used to wonder about was whether the order of the vectors in the scalar product \\(ax\\) matters. That is, whether we could also write \\(p = xa\\). Once you think about the rules of linear algebra, the answer becomes quite clear. For a scalar product, the order doesn’t matter. However, \\(p = ax\\) is preferred because it naturally generalises to the higher-dimensional case where instead of a scalar \\(a\\), we have a matrix \\(A\\). In this case, \\(p = Ax\\), which is different from \\(p = xA\\), which is generally not defined, and \\(p = x'A\\), which leads to a different result.↩︎",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Projection</span>"
    ]
  },
  {
    "objectID": "chapters/neyman_rubin_causal_model.html#how-does-randomisation-help-us-estimate-ates",
    "href": "chapters/neyman_rubin_causal_model.html#how-does-randomisation-help-us-estimate-ates",
    "title": "2  Neyman-Rubin causal model",
    "section": "2.5 How does randomisation help us estimate ATEs?",
    "text": "2.5 How does randomisation help us estimate ATEs?\nBy ensuring that the treatment - the causal variable of interest - is independent of potential outcomes, so that control and treatment group are comparable.\nMore technically, it ensures that expected outcomes are the same in both groups, which means that the selection bias - the difference of expected potential outcomes without treatment - disappears. When using regression, this is equivalent to the error term being uncorrelated with treatment status.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Neyman-Rubin causal model</span>"
    ]
  },
  {
    "objectID": "chapters/practical_issues.html#overlapping-vs-isoldated-experiments",
    "href": "chapters/practical_issues.html#overlapping-vs-isoldated-experiments",
    "title": "8  Practical issues",
    "section": "8.2 Overlapping vs isoldated experiments",
    "text": "8.2 Overlapping vs isoldated experiments\n\nUseful post here: https://blog.statsig.com/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments-cb0a69e09d3\n\n\n\n\n\nKing, Jeremy. 2020. “The Power of These Techniques Is Only Getting Stronger.” Harvard Business Review. HARVARD BUSINESS SCHOOL PUBLISHING CORPORATION 300 NORTH BEACON STREET ….\n\n\nKohavi, Ron, Randal M Henne, and Dan Sommerfield. 2007. “Practical Guide to Controlled Experiments on the Web: Listen to Your Customers Not to the Hippo.” In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 959–67.\n\n\nKohavi, Ron, Roger Longbotham, Dan Sommerfield, and Randal M Henne. 2009. “Controlled Experiments on the Web: Survey and Practical Guide.” Data Mining and Knowledge Discovery 18: 140–81.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Practical issues</span>"
    ]
  },
  {
    "objectID": "chapters/triggered_analysis.html",
    "href": "chapters/triggered_analysis.html",
    "title": "10  Triggered analysis",
    "section": "",
    "text": "(inproceedings?){deng2015diluted, title={Diluted treatment effect estimation for trigger analysis in online controlled experiments}, author={Deng, Alex and Hu, Victor}, booktitle={Proceedings of the Eighth ACM International Conference on Web Search and Data Mining}, pages={349–358}, year={2015} }\n\nFeature coverage: proportion of traffic that triggers a feature (e.g. improvement in checkout page only seen by those sessions that go to checkout)\n(deng2015diluted?) recommend using triggered analysis for features with coverage of less than 20 percent",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Triggered analysis</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#levels-at-which-to-calculate-metrics",
    "href": "chapters/metrics.html#levels-at-which-to-calculate-metrics",
    "title": "5  Metrics",
    "section": "5.13 Levels at which to calculate metrics",
    "text": "5.13 Levels at which to calculate metrics\n\n“Conversion rate” can be calculated at a numa number of different ways:\n\nAt the variant level:\n\n\\[\nCR_{variant\\_level} = \\frac{\\text{Successful sessions across variant}}{\\text{Total sessions across variant}}\n\\]\nUser level: \\[\nCR_{User\\_level} = \\sum_{i=1}^{N_v}frac{\\text{Successful sessions across user}}{\\text{Total sessions across user}}\n\\]\nAdvantage of user level is that they are more robust to outliers, since each user has weight 1, instead of weight being proportional to their number of sessions (both total and successful).\n\n\n\n\n\nBojinov, Iavor, Albert Chen, and Min Liu. 2020. “The Importance of Being Causal.” Harvard Data Science Review 2 (3): 6.\n\n\nDeng, Alex, and Xiaolin Shi. 2016. “Data-Driven Metric Development for Online Controlled Experiments: Seven Lessons Learned.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 77–86.\n\n\nDuan, Weitao, Shan Ba, and Chunzhe Zhang. 2021. “Online Experimentation with Surrogate Metrics: Guidelines and a Case Study.” In Proceedings of the 14th ACM International Conference on Web Search and Data Mining, 193–201.\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to a/b Testing. Cambridge University Press.\n\n\nRichardson, Lee, Alessandro Zito, Dylan Greaves, and Jacopo Soriano. 2023. “Pareto Optimal Proxy Metrics.” arXiv Preprint arXiv:2307.01000.\n\n\nRodden, Kerry, Hilary Hutchinson, and Xin Fu. 2010. “Measuring the User Experience on a Large Scale: User-Centered Metrics for Web Applications.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 2395–98.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/practical_issues.html",
    "href": "chapters/practical_issues.html",
    "title": "8  Practical issues",
    "section": "",
    "text": "8.1 What if you want to implement a feature right away?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Practical issues</span>"
    ]
  },
  {
    "objectID": "chapters/neyman_rubin_causal_model.html",
    "href": "chapters/neyman_rubin_causal_model.html",
    "title": "2  Neyman-Rubin causal model",
    "section": "",
    "text": "2.1 Potential outcomes",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Neyman-Rubin causal model</span>"
    ]
  },
  {
    "objectID": "chapters/fisher.html",
    "href": "chapters/fisher.html",
    "title": "3  Fisher’s exact P-value approach",
    "section": "",
    "text": "3.1 A simple example\nWe have a sample of six units, three assigned to treatment (\\(W_i = 1\\)) and three to control (\\(W_i = 0\\)). The first two columns in the table show the (unobserved) potential outcomes under control and treatment status, the third and fourth column show the observed treatment status and outcome. For each of the six units the outcome value we observe is the potential outcome corresponding to the treatment status.\nThis table highlights the fundamental problem of causal inference – we can only ever observe one potential outcome for each unit.\nHowever, Fisher’s sharp null hypothesis asserts that:\n\\[\nH_0: Y_i(1) = Y_i(0) \\quad \\text{for $i = 1, \\dots, 6$},\n\\]\nwhich makes filling in the missing values trivial:\nThere are a number of different test statistics we could use (and Imbens and Rubin (2015) discuss and compare an number of them). Following the book, for this example I use the absolute value of the difference in average outcome by treatment status, \\(T(W, Y^{obs}) = |\\bar{Y}_t^{obs} - \\bar{Y}_c^{obs}|\\), which is a function of the random assignment \\(W\\) and the observed values \\(Y^{obs}\\).\nFor our little experiment above, we can calculate the test statistic easily as:\n\\[\n\\begin{aligned}\nT^{obs} &= |(Y_1^{obs} + Y_2^{obs} + Y_3^{obs})/3 - (Y_4^{obs} + Y_5^{obs} + Y_6^{obs})/3| \\\\\n&= |(3 + 5 + 0)/3 - (4 + 0 + 1)/3| \\\\\n&= 8/3 - 5/3 \\\\\n&= 1\n\\end{aligned}\n\\]\nTo calculate the P-value, we need the distribution of test statistics under all possible random assignments. There are \\(\\begin{psmallmatrix}6\\\\3\\end{psmallmatrix} = 20\\) different assignments, and we can calculate the distribution using Python:\nimport numpy as np\nfrom itertools import combinations\nimport seaborn as sns\n\ny = np.array([3, 5, 0, 4, 0, 1])\nidx = range(len(y))\n\nts = []\nfor w in combinations(idx, 3):\n    w0, w1 = list(set(idx) - set(w)), list(w)\n    y0, y1 = y[w0], y[w1]\n    t = abs(np.mean(y1) - np.mean(y0))\n    ts.append(t)\n\nsns.histplot(ts);\nWith that distribution in hand, we can easily calculate the P-value:\np = np.mean([t &gt;= 1 for t in ts])\np\n\n0.5\nThe P-value indicates that if there is no treatment effect, we’d expect a value of the test statistic equal to 1 or even larger in 50 out of 100 random experiments, which does not provide any evidencen against the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fisher's exact P-value approach</span>"
    ]
  },
  {
    "objectID": "chapters/neyman.html",
    "href": "chapters/neyman.html",
    "title": "4  Neyman’s repeated sampling approach",
    "section": "",
    "text": "4.1 Estimator for the average treatment effect\nIn a setting with \\(i = 1, \\dots, N\\) units with fixed potential outcomes \\(Y_i(0)\\) and \\(Y_i(1)\\), where the only random component is the random assignment, captured by the assignment vector \\(W\\)1, Neyman was interested in the population average treatment effect:\n\\[\n\\tau_{fs} = \\frac{1}{N}\\sum_{i=1}^N \\left(Y_i(1) - Y_i(0)\\right) = \\bar{Y}(1) - \\bar{Y}(0),\n\\tag{4.1}\\]\nwhere\n\\[\n\\bar{Y}(1) = \\frac{1}{N}\\sum_{i=1}^N Y_i(1) \\qquad \\bar{Y}(0) = \\frac{1}{N}\\sum_{i=1}^N Y_i(0).\n\\]\nThis is our estimand of interest.\nIf we have data from a completely randomised experiment in which \\(N_t = \\sum_{i=1}^N W_i\\) units are allocated to treatment and the remaining \\(N_c = \\sum_{i=1}^N (1-W_i)\\) to control, then a natural estimator for Equation 4.1 is the difference in the averages of the treatment and control units:\n\\[\n\\hat{\\tau}^{dif} = \\bar{Y}_t^{obs} - \\bar{Y}_c^{obs}.\n\\tag{4.2}\\]\nwhere\n\\[\n\\bar{Y}_t^{obs} = \\frac{1}{N_t}\\sum_{i:W_i=1} Y_i^{obs} \\qquad \\bar{Y}_c^{obs} = \\frac{1}{N_c}\\sum_{i:W_i=0} Y_i^{obs}\n\\]\nThis estimator is unbiased (see Proof of Theorem 6.1 in Imbens and Rubin (2015) for the proof).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neyman's repeated sampling approach</span>"
    ]
  },
  {
    "objectID": "chapters/network_experiments.html",
    "href": "chapters/network_experiments.html",
    "title": "9  Network experiments",
    "section": "",
    "text": "9.1 Types of interferance\nLarsen et al. (2023) differentiate between:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Network experiments</span>"
    ]
  }
]