{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Another post on CUPED\"\n",
    "date: \"2023-10-10\"\n",
    "tags:\n",
    "    - datascience, stats\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_price</th>\n",
       "      <th>order_price_pre</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319393</th>\n",
       "      <td>JE:IE:1000007</td>\n",
       "      <td>23.146154</td>\n",
       "      <td>21.619444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319399</th>\n",
       "      <td>JE:IE:1000063</td>\n",
       "      <td>13.440000</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319400</th>\n",
       "      <td>JE:IE:1000064</td>\n",
       "      <td>19.245001</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_id  order_price  order_price_pre  t\n",
       "319393  JE:IE:1000007    23.146154        21.619444  0\n",
       "319399  JE:IE:1000063    13.440000        21.250000  1\n",
       "319400  JE:IE:1000064    19.245001        14.700000  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generally write the linear regression model as\n",
    "\n",
    "$$ y = X\\beta + \\epsilon, $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ y = \n",
    " \\begin{pmatrix}\n",
    "  y_{1}\\\\\n",
    "  y_{2}\\\\\n",
    "  \\vdots \\\\\n",
    "  y_{k} \n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "is a vector with $n$ outcome variables, one for each unit in the data, $X$ is an $n \\times k$ matrix of the form\n",
    "\n",
    "$$ X = \n",
    " \\begin{pmatrix}\n",
    "  x_{1,1} & x_{1,2} & \\cdots & x_{1,k} \\\\\n",
    "  x_{2,1} & x_{2,2} & \\cdots & x_{2,k} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  x_{n,1} & x_{n,2} & \\cdots & x_{n,k} \n",
    " \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "which we can think of as a matrix composed of $n$ row vectors stacked on top of each other, with each row vector corresponding to to the $k$ covariate values for a single unit in the data.\n",
    "\n",
    "$$ \\beta = \n",
    " \\begin{pmatrix}\n",
    "  \\beta_{1}\\\\\n",
    "  \\beta_{2}\\\\\n",
    "  \\vdots \\\\\n",
    "  \\beta_{k} \n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "is a column vector of $k$ coefficients, and\n",
    "\n",
    "$$ \\epsilon = \n",
    " \\begin{pmatrix}\n",
    "  \\epsilon_{1}\\\\\n",
    "  \\epsilon_{2}\\\\\n",
    "  \\vdots \\\\\n",
    "  \\epsilon_{k} \n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "a column vector containing the $n$ error terms.\n",
    "\n",
    "In our example data, we have $y$ contains order prices, and $X$ the treatment indicator and the pre-experiment period order price. We can estimate the model using OSL. Let's first estimate a model without the pre-experiment data. Then we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     23.6222      0.024    972.818      0.000      23.575      23.670\n",
      "t              1.1629      0.034     33.865      0.000       1.096       1.230\n",
      "==============================================================================\n",
      "1.162933033\n"
     ]
    }
   ],
   "source": [
    "# Simple regression model\n",
    "\n",
    "formula = 'order_price ~ t'\n",
    "result = smf.ols(formula, data=df).fit()\n",
    "print(result.summary().tables[1])\n",
    "print(f\"{result.params.t:.10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that treatment increase order price by about £1.3, which corresponds to the 5 percent treatment effect we created above. So all is well. We also know how to interpret the coefficient estimate of $t$: in general, the interpretation is that \"for each one unit increase in t, the order price increases by £1.3 on average\". In this case, this is equivalent to saying that if treatment status switches from 0 to 1 -- if a unit is treated -- the order price increases by £1.3 on average.\n",
    "\n",
    "Now let's add the pre-experiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           9.5653      0.042    228.211      0.000       9.483       9.647\n",
      "t                   1.1693      0.029     40.341      0.000       1.113       1.226\n",
      "order_price_pre     0.6033      0.002    384.477      0.000       0.600       0.606\n",
      "===================================================================================\n",
      "1.16933865\n"
     ]
    }
   ],
   "source": [
    "# Full model\n",
    "formula = 'order_price ~ t + order_price_pre'\n",
    "result = smf.ols(formula, data=df).fit()\n",
    "print(result.summary().tables[1])\n",
    "print(f\"{result.params.t:.10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The treatment effect estimate is very similar, which is not surprising given that treatment assignment is random and thus uncorrelated with pre-experiment order prices. Notice, however, that the standard error of the treatment effect estimate has decreased, because the the pre-experiment data accounts for some of the variance in the outcome, which means that including it reduces the variance of the residuals.\n",
    "\n",
    "Usually, we'd now interpret the coefficient on the treatment dummy as \"given the value of pre-experiment order price, a change in treatment status from 0 to 1 increases order price by about £1.3 on average\".\n",
    "\n",
    "One way to think about where the 1.3 comes from is to think of it as a (variance) weighted average of the treatment effects of each subgroup with a given pre-experiment order price. But the Frisch-Waugh-Lovell theorem provides us with another useful way to think about this. Let's have a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Frisch-Waugh-Lovell theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the theorem says and what it means\n",
    "\n",
    "For what follows, it will be useful to partition the covariate matrix and coefficient vector from our linear regression model, so that we can write\n",
    "\n",
    "$$ y = X_{1}\\beta_{1} + X_{2}\\beta_{2} + \\epsilon. $$\n",
    "\n",
    "This simply says that we now have two sets of covariates. In our example above, for instance, we could include the intercept and the treatment indicator in the first group, and the pre-experiment data in the second group.\n",
    "\n",
    "Now imagine that we instead estimate \n",
    "\n",
    "$$ \\tilde{y} = \\tilde{X}_{1}\\beta_{1}^* + \\epsilon^*, $$\n",
    "\n",
    "where $y$ and $X_1$ have been residualised so that $\\tilde{y}$ is a vector of residuals from regressing $y$ on $X_2$, and $\\tilde{X_1}$ is a vector of residuals from regressing $X_1$ on $X_2$.\n",
    "\n",
    "The Frisch-Waugh-Lovell theorem (FWL) states that:\n",
    "\n",
    "$$ \\beta_1 = \\beta_1^* \\hspace{0.5cm}\\text{and}\\hspace{0.5cm} \\epsilon = \\epsilon^*. $$\n",
    "\n",
    "Before we go on, let's verify this.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   8.482e-14      0.014   5.85e-12      1.000      -0.028       0.028\n",
      "t_res          1.1693      0.029     40.341      0.000       1.113       1.226\n",
      "==============================================================================\n",
      "1.16933865\n"
     ]
    }
   ],
   "source": [
    "# Verifying FWL -- residualised outcome and treatment\n",
    "\n",
    "order_price_res = smf.ols('order_price ~ order_price_pre', data=df).fit().resid\n",
    "t_res = smf.ols('t ~ order_price_pre', data=df).fit().resid\n",
    "\n",
    "formula = 'order_price_res ~ t_res'\n",
    "result = smf.ols(formula, data=df).fit()\n",
    "print(result.summary().tables[1])\n",
    "print(f\"{result.params.t_res:.10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FWL works -- the treatment estimate is identical to that estimated in the full model above, and so is the standard error.\n",
    "\n",
    "But what does it mean? It tells us that when estimating the coefficient of a given covariate, multiple regression only considers the variation in that covariate and the outcome that is not explained by any of the other covariates in the data -- the effect of other covariates are \"partialled out\". We can thus think about $\\beta_1$ as capturing the correlation between the outcome and the $X_1$ covariates when the effects of all $X_2$ covariates on all these variables have been removed -- for a given $X_1$ covariate, think of a scatterplot with a residualised outcome on the y-axis and a residualised covariate on the x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand what FWL says, have convinced ourselves that it works, and understand what it means, let's dig deeper and understand the multiple algebra of the theorem, which will then help us see its connection to CUPED in a transparent way.\n",
    "\n",
    "A more formal way to state FWL is that if we have the regression of interest that contains two separate sets of predictors\n",
    "\n",
    "$$ y = X_{1}\\beta_{1} + X_{2}\\beta_{2} + \\epsilon, $$\n",
    "\n",
    "we can estimate $\\beta_{1}$ using\n",
    "\n",
    "$$ M_{2}y = M_{2}X_{1}\\beta_{1} + \\epsilon, $$\n",
    "\n",
    "where \n",
    "\n",
    "$$M_2 = I - X_2(X_2'X_2)^{-1}X_2' = I - P_2$$\n",
    "\n",
    "is the residual-maker matrix.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUPED\n",
    "\n",
    "$$\n",
    "\\tilde{y} = y - \\theta x,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\theta = \\frac{cov(y, X_2)}{var(X_2)}\n",
    "$$\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$$ M_{2}y = X_{1}\\beta_{1} + \\mu, $$\n",
    "\n",
    "In other words, CUPED only residualises the outcome variables instead of the outcome and the treatment indicator. Then why are the results almost identical?\n",
    "\n",
    "If treatment is perfectly random, then\n",
    "\n",
    "$$M_{2}T = T$$\n",
    "\n",
    "Because, in practice, it isn't perfectly random, the results are slighlty different.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     23.6190      0.020   1152.333      0.000      23.579      23.659\n",
      "t              1.1693      0.029     40.341      0.000       1.113       1.226\n",
      "==============================================================================\n",
      "1.169338264\n"
     ]
    }
   ],
   "source": [
    "# Traditional CUPED\n",
    "\n",
    "def cuped_adjust_y(df, y, x):\n",
    "    data = df.dropna(subset=[y, x])\n",
    "    cv = np.cov([data[y], data[x]])\n",
    "    theta = cv[0, 1] / cv[1, 1]\n",
    "    y, x = data[y], data[x]\n",
    "    return (y - (x - x.mean()) * theta).fillna(y)\n",
    "\n",
    "df['order_price_cuped'] = cuped_adjust_y(df, 'order_price', 'order_price_pre')\n",
    "\n",
    "formula = 'order_price_cuped ~ t'\n",
    "result = smf.ols(formula, data=df).fit()\n",
    "print(result.summary().tables[1])\n",
    "print(f\"{result.params.t:.10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.5847      0.020    -28.525      0.000      -0.625      -0.544\n",
      "t              1.1693      0.029     40.341      0.000       1.113       1.226\n",
      "==============================================================================\n",
      "1.169338264\n"
     ]
    }
   ],
   "source": [
    "# Regression CUPED\n",
    "formula = 'order_price_res ~ t'\n",
    "result = smf.ols(formula, data=df).fit()\n",
    "print(result.summary().tables[1])\n",
    "print(f\"{result.params.t:.10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get an identical coefficient estimate, and the standard error is identical, too. However, the intercept is effectively zero. This is because we have included an intercept when we created the residualised outcomes, meaning the outcome residuals have mean zero. We can recover the intercept value by adding the variables mean values to the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     25.9690      0.023   1152.903      0.000      25.925      26.013\n",
      "t_res          1.2855      0.032     40.355      0.000       1.223       1.348\n",
      "==============================================================================\n",
      "1.285500636\n"
     ]
    }
   ],
   "source": [
    "# Add mean values\n",
    "\n",
    "order_price_res = smf.ols('order_price ~ order_price_pre', data=df).fit().resid + df.order_price.mean()\n",
    "t_res = smf.ols('t ~ order_price_pre', data=df).fit().resid + df.t.mean()\n",
    "\n",
    "formula = 'order_price_res ~ t_res'\n",
    "result = smf.ols(formula, data=df).fit()\n",
    "print(result.summary().tables[1])\n",
    "print(f\"{result.params.t_res:.10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUPED is the same for all practical purposes, but note that the coefficient estimate is not identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     25.9690      0.023   1152.903      0.000      25.925      26.013\n",
      "t              1.2855      0.032     40.355      0.000       1.223       1.348\n",
      "==============================================================================\n",
      "1.285500216\n"
     ]
    }
   ],
   "source": [
    "# Residualised outcome and full treatment (CUPED)\n",
    "formula = 'order_price_res ~ t'\n",
    "result = smf.ols(formula, data=df).fit()\n",
    "print(result.summary().tables[1])\n",
    "print(f\"{result.params.t:.10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, residualised treatment estimate on full outcome has same coefficient estimate, but higher standard error because the outcome variance is larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     25.9690      0.027    972.999      0.000      25.917      26.021\n",
      "t_res          1.2855      0.038     34.058      0.000       1.212       1.359\n",
      "==============================================================================\n",
      "1.285500636\n"
     ]
    }
   ],
   "source": [
    "# Full outcome and residualised treatment\n",
    "formula = 'order_price ~ t_res'\n",
    "result = smf.ols(formula, data=df).fit()\n",
    "print(result.summary().tables[1])\n",
    "print(f\"{result.params.t_res:.10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix algebra\n",
    "\n",
    "We can think of a projection as an operation that maps a vector onto a subspace. The result of a projection is a vector that lies within the subspace and is the closest point in the subspace to the original vector. \n",
    "\n",
    "Where does the projection matrix come from? [10 Fundamental Theorems for Econometrics](https://bookdown.org/ts_robinson1994/10EconometricTheorems/linear_projection.html#linear_projection) provides a nice example that I'm going to borrow here (the site has a useful visualisation that I'm not going to redraw here). If we have a point $x$ in two dimensional space and a line $L$ in that same space, then the projection of $x$ onto $L$, $\\bar{x}$, is the point on $L$ that is closest to $x$. If we think of $L$ as being formed by a vector $v$ and a set of scalar multiples $c$, we want to find the one scalar multiple $c$ for which the Euclidian distance between $x$ and $\\bar{x}$, $\\sqrt{\\sum_i{(\\bar{x_i} - x})^2}$ (the index $i$ ranges over all dimensions), is minimised -- we're looking for $\\bar{x} = c^*v$, where $c^*$ is the optimal c. Formally, we want to find\n",
    "\n",
    "$$\n",
    "arg min_c \\sqrt{\\sum_i{(\\bar{x_i} - x})^2} \\\\\n",
    "\n",
    "= arg min_c \\sum_i{(\\bar{x_i} - x})^2 \\\\\n",
    "\n",
    "= arg min_c \\sum_i{(cv_{i} - x})^2,\n",
    "$$\n",
    "\n",
    "where the first equality holds because the square root is a monotonic transformation and the second because we have defined as $\\bar{x} = c^*v$. To find $c^*$, we can differentiate and setting the result equal to zero\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d}{dc} \\sum_i{(cv_{i} - x})^2 &= \\sum_i{2v_{i}(cv_{i} - x}) \\\\\n",
    "& = 2\\left(\\sum_i{cv_{i}^2} - \\sum_i{v_{i}x}\\right) \\\\\n",
    "&= 2(cv'v - v'x) & \\text{using vector notation} \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and solve for $c$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "2(cv'v - v'x) &= 0 \\\\\n",
    "cv'v - v'x &= 0 \\\\\n",
    "cv'v &= v'x \\\\\n",
    "c &= (v'v)^{-1}v'x\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Remembering that $\\bar{x} = vc$, we get:\n",
    "\n",
    "$$\n",
    "\\bar{x} = vc = \\underbrace{v(v'v)^{-1}v'}_\\text{$P_v$}x,\n",
    "$$\n",
    "\n",
    "where $P_v = v(v'v)^{-1}v'$ is the projection matrix of $x$ onto $v$. Once we understand what the projection matrix is -- the function we apply to $x$ to find the nearest point on $v$ -- and know that we define \"rearest\" as minimising the Euclidean distance, this intimate link between minimising the Euclidean distance and the projection matrix is no surprise.\n",
    "\n",
    "What does this mean in the context of linear regression? In the context of linear regression, with a covariance matrix $X$, the projection matrix is $P = X(X'X)^{-1}X'$. The coefficient estimates are given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X'X)^{-1}X'y.\n",
    "$$\n",
    "\n",
    "and the predicted values are given by:\n",
    "\n",
    "$$\n",
    "\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y = Py.\n",
    "$$\n",
    "\n",
    "This tells us that the fitted values in a linear regression are a projection of the vector of observed outcomes, $y$, onto the subspace spanned by $X$.\n",
    "\n",
    "Finally, the residuals of the linear model are:\n",
    "\n",
    "$$\n",
    "\\hat{\\epsilon} = y - \\hat{y} = y - X\\hat{\\beta} = y - X(X'X)^{-1}X'y = My,\n",
    "$$\n",
    "\n",
    "where $M = I - X(X'X)^{-1}X'$. Hence, $M$ is called the residual-maker matrix because it is the matrix that, when pre-multiplied to the vector $y$, returns the vector of residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
