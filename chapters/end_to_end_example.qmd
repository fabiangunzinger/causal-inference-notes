# End-to-end example

For now, this is heavily based on Chapter 2 in @kohavi2020trustworthy.


## Pre-design

Define the proposed change

- We want to test the effect of adding coupon field to the checkout page

- For ideas and possible concerns of certain ideas, you should check the literature and have a look at [GoodUI](https://goodui.org)


Define how you'd implement the change

- We use painted door approach, and design two different treatment (one with additional field, one with popup)


Define your goal metric (OEC)

- Revenue per user

- In general, this depends on:

  - What precisely we want to test (e.g. does feature hurt amount spent or whether user abandons checkout?)

  - Power (indicator has lower variance than revenue and thus higher power)

- See @sec-metrics for more details


Decide which users to focus on 

- We use users who start the checkout process (i.e. land on the checkout page)

- In general, it's useful to think about the user experience funnel, and then think about where in the funnel the change is visible. More often than not, this is the relevant step at which to trigger users into the experiment.


Formulate hypothesis

- "Adding a coupon field to the checkout page will reduce revenue per user for users who start the checkout process".

- General template: "[proposed change] will [hypothesised effect] on [OEC] for [selected sample]".


Decide on statistical significance level, desired level of power, and MDE

- They are the key parameters to our experiment

- We use conventioal values of 0.05 and 0.8 for stat sifnificance and power, and 1 percent change for MDE.

- To determine MDE, consider

  - Cost of launching the change

    - Cost of fully building out feature (can be 0 when fully built out for experiment or high if we use painted door)

    - Cost of maintaining new code (new code has higher bugs, may increase code complexity and maintenance)

    - Other costs: e.g. does CPU utilization increase?

- To determine significance level and power, consider:

  - Cost of not launching positive feature vs cost of launching harmful feature

  - How long will feature be in effect?

  - How widely will it be deployed?

  - How many users will see it (i.e. where in funnel is it?)


Decide on the MDE 

- We use a 1 percent change

- This is equivalent to asking "what counts as a practically relevant change?" To answer that, consider:

  - Maturity of service (the more mature, the smaller a change can be expected)

  - Size of service (the larger, the smaller a change still generates a lot of revenue)

  - Cost of implementing/maintaining the feature (e.g. feature development, code maintenance, etc.)






## Experiment design

Decide on randomisation

  - We use users

Decide on target population

  - We target all users

  - Could target only users in some countries or other subgroups


Decide on variant split

- We use 34/33/33 split

- Generally, equal proportions have highest power

- But if we have many treatments, allocating more users to control is advisable (describe why)


Decide experiment size (number of unique units)

  - We want 80 percent power to detect at least a 1 percent change in revenue per user -- we'll use a power analysis to determine required size.

  - Factors that determine length

    - High vs low variance metric (e.g. binary has higher power due to lower variance)

    - MDES

    - Significance level

  - Other factors to consider

    - How save is experiment? If risky, ramp-up more slowly

    - Do we need to share traffic with other experiments?

Decide experiment duration

  - We'll make sure to run the experiment for at least a week to guard against day-of-week effects, and we'll check for novely and learning effects and run the experiment for longer if needed

  - Factors to consider

    - More units generally leads to higher power (unless with cumulative metrics such as counts where variance increases with duration), though in a concave way -- this is related to size requirement above

    - Day of week effect (run for at least a week)

    - Seasonality

      - Need to ensure results not driven by reasonality in any of the places experiment is run

      - One aspect of external validity (in terms of time)

    - Novelty and learning effects (run experiment for longer until we see stable effects)

## Running experiment

- We need

  - Instrumentation (lots of how users behave on the site and what experiment variant they were assigned to)

  - Infrastructure to implement experiment (feature flagging and variant assignment)

## Interpreting results

- Sanity checks

  - Before looking at results, we check guardrail and invariant metrics to be confident about internal validity

  - Trust-related guardrails

    - SRM

  - Organisational guardrails

    - E.g. latency

- Experiment results

  - We find that both treatments significantly reduce revenue per user

  - Digging shows that it's because of fewer checkouts (we could see that by having indicator and revenue conditional on spend as debug metrics)

## Decision making

- Adding coupon field reduces revenue per user by 7 percent

- Hence, marketing campaign of coupons would not only have to cover campaign costs, but also reduction due to additional coupon field

- Marketing study found that sending coupons increases revenue per user by about 3 percent

- Hence, we won't run the campaign

Different hypothetical cases:

- Neither statistically nor practically significant

  - Iterate or abandon

- Statistically and practically significant

  - Launch

- Stat sig but not practically sig

  - Benefits don't outweigh costs (or else practical sig boundary would be lower) -- don't launch

- Not stat or practically significant

  - Try to rerun/run for longer to get more power
  - If need to, launch if statsig and point estimate is also practically sig but with ci beyond practically sig boundary



- Case 4:

  - Not enough data to draw conclusion
  - Run follow up test with higher power -- run for longer

- Case 5:

  - Repeat with more power if possible -- run for longer

- Case 6:

  - Run for longer to get more power
  - Launch would be reasonable, if you can't wait





