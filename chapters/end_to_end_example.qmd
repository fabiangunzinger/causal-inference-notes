# End-to-end example

- This is my framework for how I drive a discussion with a PM who want to test a new feature.

- I use it as a checklist for consistency, as a way to build in best-practices and lessons learned over the years, and to ensure I don't forget anything (which I'm prone to!).

- Obviously, I adapt the framework depending on the context -- depending on what precisely is needed, how much I know about the product and the feature.

- I also use this in interviews because, really, these are about showing how you do the job.

- The experiment-centered part of this is based on Chapter 2 in @kohavi2020trustworthy.



## Clarify

Understand the ask

- What does each term mean?

- What is our objective here?

Understand the product

- What does the product do?

- What is its purpose?

- How precisely does it work?

Describe my approach

- In order to design an evaluation approach, we'll first have to make a few decisions together about what we care about as part of the test.

- Based on these, I'll then propose a design and evaluation approach.



## Pre-design

Define the proposed change

- We want to test the effect of adding coupon field to the checkout page

- For ideas and possible concerns of certain ideas, you should check the literature and have a look at [GoodUI](https://goodui.org)


Define how you'd implement the change

- We use painted door approach, and design two different treatment (one with additional field, one with popup)


Define your goal metric (OEC)

- Revenue per user

- In general, this depends on:

  - What precisely we want to test (e.g. does feature hurt amount spent or whether user abandons checkout?)

  - Power (indicator has lower variance than revenue and thus higher power)

- See @sec-metrics for more details


Decide which users to focus on 

- We use users who start the checkout process (i.e. land on the checkout page)

- In general, it's useful to think about the user experience funnel, and then think about where in the funnel the change is visible. More often than not, this is the relevant step at which to trigger users into the experiment.


Formulate hypothesis

- "Adding a coupon field to the checkout page will reduce revenue per user for users who start the checkout process".

- General template: "[proposed change] will [hypothesised effect] on [OEC] for [selected sample]".


Decide on statistical significance level, desired level of power, and MDE

- They are the key parameters to our experiment

- We use conventioal values of 0.05 and 0.8 for stat sifnificance and power, and 1 percent change for MDE.

- To determine MDE, consider

  - Cost of launching the change

    - Cost of fully building out feature (can be 0 when fully built out for experiment or high if we use painted door)

    - Cost of maintaining new code (new code has higher bugs, may increase code complexity and maintenance)

    - Other costs: e.g. does CPU utilization increase?

- To determine significance level and power, consider:

  - Cost of not launching positive feature vs cost of launching harmful feature

  - How long will feature be in effect?

  - How widely will it be deployed?

  - How many users will see it (i.e. where in funnel is it?)


Decide on the MDE 

- We use a 1 percent change

- This is equivalent to asking "what counts as a practically relevant change?" To answer that, consider:

  - Maturity of service (the more mature, the smaller a change can be expected)

  - Size of service (the larger, the smaller a change still generates a lot of revenue)

  - Cost of implementing/maintaining the feature (e.g. feature development, code maintenance, etc.)


Agree on decision-making process:

- Find a criteria that completely determines the launch/no-launch decision, i.e. such that meeting the criteria is a clear launch.

- For experiment, the criteria is the relationship between the OEC and the MDE.

- Validate these criteria with PM/leadership.

- Use traffic light system to classify your results: green is launch, red is no-launch, amber is need more info/data (see below)


## Experiment design

Decide on randomisation

  - We use users

Decide on target population

  - We target all users

  - Could target only users in some countries or other subgroups


Decide on variant split

- We use 34/33/33 split

- Generally, equal proportions have highest power

- But if we have many treatments, allocating more users to control is advisable (describe why)


Decide experiment size (number of unique units)

  - We want 80 percent power to detect at least a 1 percent change in revenue per user -- we'll use a power analysis to determine required size.

  - Factors that determine length

    - High vs low variance metric (e.g. binary has higher power due to lower variance)

    - MDES

    - Significance level

  - Other factors to consider

    - How save is experiment? If risky, ramp-up more slowly

    - Do we need to share traffic with other experiments?

Decide experiment duration

  - We'll make sure to run the experiment for at least a week to guard against day-of-week effects, and we'll check for novely and learning effects and run the experiment for longer if needed

  - Factors to consider

    - More units generally leads to higher power (unless with cumulative metrics such as counts where variance increases with duration), though in a concave way -- this is related to size requirement above

    - Day of week effect (run for at least a week)

    - Seasonality

      - Need to ensure results not driven by reasonality in any of the places experiment is run

      - One aspect of external validity (in terms of time)

    - Novelty and learning effects (run experiment for longer until we see stable effects)

## Running experiment

- We need

  - Instrumentation (lots of how users behave on the site and what experiment variant they were assigned to)

  - Infrastructure to implement experiment (feature flagging and variant assignment)

## Interpreting results

- Sanity checks

  - Before looking at results, we check guardrail and invariant metrics to be confident about internal validity

  - Trust-related guardrails

    - SRM

  - Organisational guardrails

    - E.g. latency

- Experiment results

  - We find that both treatments significantly reduce revenue per user

  - Digging shows that it's because of fewer checkouts (we could see that by having indicator and revenue conditional on spend as debug metrics)

## Decision making

- Adding coupon field reduces revenue per user by 7 percent

- Hence, marketing campaign of coupons would not only have to cover campaign costs, but also reduction due to additional coupon field

- Marketing study found that sending coupons increases revenue per user by about 3 percent

- Hence, we won't run the campaign


Different hypothetical cases:

![Possible decision scenarios. Source: @kohavi2020trustworthy](../inputs/decision_scenarios.png){width=75%}

- Scenario 1: Neither SS nor PS -- iterate or abandon

- Scenario 2: SS and PS -- launch

- Scenario 3: SS but not PS with narrow CI -- no lunch

- Scenario 4-6: CI so wide that SS and possibly even PS point estimate coule be null, or where small point estimate could still be SS and PS -- run for longer or rerun with higher power ideally, else make decision based on risk/reward.