# Metrics

The choice of metrics to consider when evaluating an experiment is not trivial. In field experiments, collecting data can be very costly, which puts an automatic limit to the number of metrics collected. In online experiments, the number of metrics we could look at is vast.

## Types of metrics

- Primary

- Secondary

- Guardrail: ensures that improvements in primary metrics don't come at the cost of quality/experience/something else -- basically to avoid unintended consequences


## How to select metrics

- Look at BIT bluebook for overall framework for how to think about metrics (specificity etc)

- Look at Spotify confidence

- Primary metric something that directly captures what you wanna improve? Guardrails general health metrics you don't want to go down (e.g. revenue, conversion)? Based on Kohavi anecdote below

- @bojinov2020importance mention that LinkedIn has four company wide success metrics and many product specific ones. So, presumably we'd use company-wide ones as guardrails. Question is, what are good product-specific metrics? Good in the sense that they have a positive impact on business-wide metrics? Can use causal inference (e.g. IV) to test effect (see section 2.1 in @bojinov2020importance)


## Common metrics

- Conversion rate
- Number of bookings
- Engagement
  - Likes, shares, comments, reactions
  - Page views
  - Click-through rates (CTR)
  - Time spent per user per day

- Retention
  - Daily active users (DAU)
  - Churn rate (percentage of users who stop using the service within a given period)

- Revenue
  - Average revenue per user (ARPE)
  - Customer lifetime value (CLV)


Guardrail metrics
- Bounce rate (proportion of site visitors who leave after seeing only the first page)
- Cancellation rate
- 


## Ways to think about Metrics

- From Meta 
  - Topline metric: e.g. daily active users
  - Feature/product team northstar: e.g. total buyers for buyer-facing side of marketplace
  - Guardrail metric: number of messages flagges as spam or harmful in msg app



## Misc issues

- Selecting the wrong metric can lead to misleading results. @kohavi2012trusworthy provide a memorable example from an experiment at Bing: the experiment increased revenue by user because search results were poorer, leading users to make more searches and lead them to click on more adds. This is good in the short-term. But in the long term, users will surely get frustrated by the poorer search results. A better metric would have been one that directly captures the quality of the search results, such as sessions per user. Lesson: have a primary metric that directly captures the thing you want to improve. Use higher level-metrics such as revenue as guardrails.
