# Metrics


## Why good metrics matter

- Good metrics ensure that everyone works towards the same goal in a way that is reliable, transparent, and provides accountability -- they ensure coherence across the company.

- Good metrics increase the probability that our evaluations detect a change if there is one -- they have high sensitivity.


## Metric taxonomy

Different contributions in the literature and different companies use different ways to classify metrics. So so the same type of metrics will have different names and the same name will be used for different types of metrics across contexts. What matters is less the labels, but an understanding of the different functions metric can serve and how different types of metric relate to one another.

@kohavi2020trustworthy classify metrics into goal metrics, driver metrics, and guardrail metrics. I find this useful and use it as the basis of how I think about metrics.


### Goal metrics

- Also "success metrics" or "North Star" metrics

- Directly aligned with the organisation's mission and represents how it creates value for its customers

- They are a quantitative definition of what success looks like

- Tend to be long-term oriented and slow-moving

- An organisation usually has only very few or even just one

- Examples: "average monthly purchases" for Amazon.


### Driver metrics

- Also "surrogate metrics", "predictive metrics"

- Capture the movement of factors contributing to the organisation's goal

- They are a quantitative representation of what drives success

- They tend to be short-term oriented and fast-moving 

- Will change over time as the service matures and as it becomes more closely aligned/correlated with the North Star

- Example for Amazon buyer focused team: number of high-quality sellers that join platform per month.


### Guardrail metrics

- @kohavi2020trustworthy divide guardrails into two types: those protecting the business and those ensuring internal validity of experiment results

- The main guardrail to ensure internal validity is smaple ratio mismatch (SRM). Others are discussed in chapter 21 in @kohavi2020trustworthy

- Guardrails that protect the business ensure that improving one part of the platform don't come at the cost of quality/experience/something else -- they basically try to guard against unintended consequences

- @deng2016data argue that the main feature of a good guardrail metric should be directionality, so that we can be sure that if we get a signal, it points in the right direction in terms of user experience (in contrast to debug metrics, which should have good sensitivity)

- Amazon example: average number of purchases per day (to check that influx of sellers doesn't lead to paralysis for buysers)


### Other metrics

**Supporting metrics**

- Indicators that the primary or NS metric are moving in the right direction (particularly useful as leading indicators)

- Amazon example: emails sent to high-quality sellers, emails opened, etc.

**Debug metrics**:

- @deng2016data mention debug metrics as a way to get additional informaiton about the movement of our primary metrics.

- They can be useful when showing the individual components of combo metrics, or the numerator and denominator of ratio metrics.


### Metrics at different levels of the organisation

- Above, I have defined goal, driver, and guardrail metrics in reference to the organisation.

- But they can be defined at each level within an organisation, too.

- The challenge is to make sure that definitions across metric types and organisation levels cohere.

- Figure 6.1 in @kohavi2020trustworthy is a useful way to visualise this: it shows a large arrow containing many small arrows inside. This can represent goal metrics (large arrow) and driver metrics (small arrows) or organisational metrics (large arrows) and team metrics (small arrows). In each case, we want to make sure that the direction of the small arrows is as aligned as possible with the large arrow.


## What makes a good Metric


### STEDII


### Other characteristics

- Meaningful (reflect goals of company, product)

- Measurable ()

- Moveable (with low delay)

- Interpretable (not too complicated, easy to communicate and understand)

- Not gameable (violate Goodhart's law)


### Different priorities for different metric types

- @kohavi2020trustworthy

  - For goals: simple and stable 
  - For drivers: aligned with goal, actionable and relevant, sensitive, resistant to gaming 


## Developing and evaluating good metrics

- @deng2016data suggest a three-step process to develop user-behaviour-driven metrics:

  1. Formulate a hypothesis based on a simple model of user behaviour (e.g. social network users who like, comment, and share more have a better user-experience)

  2. Conduct user studies to create labelled data against which the original model can be tested (e.g. conduct user surveys and test whether users who like, comment, and share more report a better experience)

  3. Design online metrics based on insights from step 2 and assess their directionality and sensitivity as discussed below (e.g. create a metric called "meaningful engagement" and test whether it has high directionality and sensitivity in past experiments)

  In doing this, remember to 1) focus on behaviour patterns that are observed for most users, 2) collect labelled data from various sources to mitigate bias (surveys, lab studies, annotated logged data), and 3) use transparent models so that we can define and track debug metrics (e.g. create an online metric using a decision tree, rather than a complex ML model).

- @deng2016data recommend using two criteria to evaluate the quality of a metric: directionality and sensitivity. Directionality requires that a move of the metric in one direction consistently captures the direction of the user experience. In practice, the direction of a metric, such as queries per user can be ambiguous. Sensitivity requires that the metric picks up changes in the user experience such that we can identify it as part of an experiment. We can think of them as the direction and the size of a vector: the more directly it points towards the North Star, and the closer it gets, the better. These two criteria also allow us to qualitatively compare different metrics and decide which one(s) to use as our OEC.

- @deng2016data propose two ways to evalute directionality and sensitivity. First, we can use a validation corpus: a collection of prior experiments for which we know the effect on user experience, and which we can then use to test sensitivity and directionality of our metrics. The second, if no validation corpus is available, is degeneration experiments, whereby we deliberately degenerate the user experence in a way that is acceptable and doesn't harm long-term user experience, and then measure directionality and sensitivity of the metrics.

- @deng2016data point out that ratio metrics (CTR or Success Query Rate) are often good candidates for OECs because they are bounded and have high sensitivity. However, they need to be interpreted carefully because a change can result from a change in the numerator, the denominator, of both, out of which only the first gives a clear signal. Hence, when relying on ratio metrics, they recommend two things: 1) rely on debug metrics to separately track numerator and denominator, 2) only rely on ratio metrics with a stable denominator. For example: Bing used Session Success Rate instead or Query Success Rate even though QSR was more sensitive because the denominator of SSR was more stable (QSR was used as a debug metric).

- For cases where no single metric fits all scenarios, @deng2016data recommend using a combo metric. To create such a metric, it is important to 1) understand the direction and interpretation of each metric and be aware of the scenarios when it fails to provide an accurate signal, 2) have metrics that cover all scenarios.


- Overall process
  - Define Goals for feature (e.g. improve efficiency)
  - Define Signals (fewer undos or erases)
  - Define Metrics (average number of undos per session)


## Combining metrics into a signal / creating an OEC



## Understanding metric sensitivity

- @deng2016data point out that detecting a treatment effect has two components:

$$
P(\text{detecting treatment effect on the metric}) = P(H_1) \times P(p \leq \alpha | H_1) 
$$

- $P(H_1)$ is the probability the movement probability -- the probability that the metric moves

- $P(p \leq \alpha | H_1)$ is power

- The authors point out that understanding which component produces a lack of sensitivity is crucial. Because if it's movement probability, we might need a different metric, whereas with a lack of power, variance reduction might help.

- Examples for metrics with low movement probability might be "number of sessions per user" for a search engine, since daily search needs are limited, and changing user engagement is difficult in the short-term. An example for a metric with low power is "Revenue per user", due to very high variance.


## How to select metrics

- NSM and primary based on main company/product goal, focus on criteria above

- Support and guardrails trickier. Use AAAERRR Framework

- To ensure coherence across all workstreams in a company, metrics used at all levels have to contribute to the same overall goal, which is captured by the company's North Star.

- Primary metric something that directly captures what you wanna improve? Guardrails general health metrics you don't want to go down (e.g. revenue, conversion)? Based on Kohavi anecdote below

- @bojinov2020importance mention that LinkedIn has four company wide success metrics and many product specific ones. So, presumably we'd use company-wide ones as guardrails. Question is, what are good product-specific metrics? Good in the sense that they have a positive impact on business-wide metrics? Can use causal inference (e.g. IV) to test effect (see section 2.1 in @bojinov2020importance)




## Frameworks for creating driver metrics

Driver metrics represent quantitative measures of the factors that drive an organisation's success. The following frameworks help us think about those drivers of success.


### Pirate metrics (AARRR)

Developed by Dave McClure, the pirate metrics (summary [here](https://500hats.typepad.com/500blogs/2007/09/startup-metrics.html), slides [here](https://www.slideshare.net/dmc500hats/startup-metrics-for-pirates-long-version)) classify driver metrics into:

- Acquisition (the user comes to our site from various channels)

- Activation (has a good experience on the first visit)

- Retention (comes back to the site)

- Referral (likes the site enough to recommend it to others)

- Revenue (engages in a revenue generating behaviour)

The below Table provides a useful example of possible conversion metrics and associated conversion rates. 

![AARRR example conversion metrics. Source: 500hats.typepad.com/500blogs/2007/09/startup-metrics.html](inputs/aarrr-example-conv-metrics.png)


### AAAERRR

An extension of the pirate metrics.

- Awareness (how many aware of product)

- Acquisition (how many use product)

- Activation (how many are realizing value of product -- e.g. 10 friends in 7 days on FB / stored at least 1 file on a device on Dropbox)

- Engagement (breath and frequencey of engagement)

- Revenue (how many are paying for product)

- Retention/renewal (how many are coming back)

- Referral (how many are becoming advocates)


### PULSE

- Page views

- Uptime

- Latency

- Seven-day active users

- Earnings


### HEART

- Developed by @rodden2010measuring, the authors address shortcomings of the PULSE framework.

- Key challenge in CHI is creation of user-experience metrics based on large-scale data.

- Traditional PULSE metrics (Page views, Uptime, Latency, Seven-day active users, Earnings) are useful and related to user-experience, but limited because they are indirect and can be ambiguous (are more page views a sign of an increase in engagement or in confusion?) and provide limited insight (seven-day active users shows user-base volume but nothing about product commitment).

- Authors propose HEART metrics (Happiness, Engagement, Adoption, Retention, Task success) to complement traditional metrics and remedy their shortcomings.

- Happiness
  - Measured using bipolar scale in-product survey
  - Shows that users liked redesign of personalised homepage after initial dip (also shows value of dynamic treatment effects)

- Engagement
  - E.g. number of visits per user per week
  - Helped Gmail team see proportion of users who visited more than 5 times per week.

- Adoption and retention
  - E.g. how many new accounts created (adoption), how many users from seven-day active users 3 weeks ago are still in that set (retention).
  - Helped Google Finance team distinguish between new and recurring users during 2008 meltdown.

- Task success
  - E.g. progress in optimal path (signup)
  - Helped Google maps team see that users could adopt search to single-box so they could drop double-box version.


## Common metrics

- Conversion rate
- Number of bookings
- Engagement
  - Likes, shares, comments, reactions
  - Page views
  - Click-through rates (CTR)
  - Time spent per user per day

- Retention
  - Daily active users (DAU)
  - Churn rate (percentage of users who stop using the service within a given period)

- Revenue
  - Average revenue per user (ARPE)
  - Customer lifetime value (CLV)


Guardrail metrics
- Bounce rate (proportion of site visitors who leave after seeing only the first page)
- Cancellation rate
- 


## Ways to think about Metrics

- From Meta 
  - Topline metric: e.g. daily active users
  - Feature/product team northstar: e.g. total buyers for buyer-facing side of marketplace
  - Guardrail metric: number of messages flagges as spam or harmful in msg app



## Misc issues

- Selecting the wrong metric can lead to misleading results. @kohavi2012trusworthy provide a memorable example from an experiment at Bing: the experiment increased revenue by user because search results were poorer, leading users to make more searches and lead them to click on more adds. This is good in the short-term. But in the long term, users will surely get frustrated by the poorer search results. A better metric would have been one that directly captures the quality of the search results, such as sessions per user. Lesson: have a primary metric that directly captures the thing you want to improve. Use higher level-metrics such as revenue as guardrails.





## Other metric taxonomies 

**Business report vs heuristic vs user-behaviour**

- @deng2016data use a different classification altogether:

  - Type 1: Business Report Driven Metrics: Business report driven metrics, like Revenue per User and Monthly Active Users, focus on long-term goals of online services. These metrics are vital for business assessments but are less actionable for short-term product improvements. For example, improving search results in a service like Bing might decrease short-term revenue per user, highlighting the need for longer-term experimentation to truly assess impacts.

  - Type 2: Simple Heuristic Based Metrics: Simple heuristic based metrics, such as Click-Through Rate and user activity counts, offer direct insights into user interaction with online services. While actionable, they can be misleading in terms of user experience and business goals. For instance, higher CTR due to misleading content can negatively impact user experience and, subsequently, the service's market share. These metrics are suitable for early-stage services but may not align with real user experience improvements in more mature stages.

  - Type 3: User-Behavior-Driven Metrics: User-behavior-driven metrics, derived from user satisfaction and frustration models, aim to directly measure user experience and its impact on long-term service success. They are complex, involving detailed analysis of user behavior, like considering both clicks and dwell time for assessing search satisfaction. These metrics are sensitive and actionable for agile experiments, offering a more nuanced understanding of user interaction than simpler metrics.


## Case study: Meta

Metrics in quarterly earnings presentation:

- Advertising revenue (by user geography) -- $33bn
- Revenue (by user geography) -- $34bn
- Net income -- $11bn

- Family daily active people (DAP) -- 3.1bn
- Family monthly active people (MAP) -- 3.9bn
- DAP / MAP -- 80%

- Family average revenue per person per quarter (ARPP) -- $8.7

- Facebook daily active users (DAU) -- 2bn
- Facebook monthly active users (MAU) -- 3bn
- DAU / MAU -- 68%
- Facebook average regenue per user (ARPU) -- $11


Definitions: 
- We define a daily active person (DAP) as a registered and logged-in user of Facebook, Instagram, Messenger, and/or WhatsApp (collectively, our "Family" of products) who visited at least one of these Family products through a mobile device application or using a web or mobile browser on a given day.

- We define average revenue per person (ARPP) as our total revenue during a given quarter, divided by the average of the number of MAP at the beginning and end of the quarter. While ARPP includes all sources of revenue, the number of MAP used in this calculation only includes users of our Family products as described in the definition of MAP in the previous slide.

Limitations of key data

- The numbers for our key metrics are calculated using internal company data based on the activity of user accounts.

- We report our estimates of the numbers of our daily active people (DAP), monthly active people (MAP), and average revenue per person (ARPP) (collectively, our "Family metrics") based on the activity of users who visited at least one of Facebook, Instagram, Messenger, and WhatsApp (collectively, our "Family" of products) during the applicable period of measurement.

- We have historically reported the numbers of our daily active users (DAUs), monthly active users (MAUs), and average revenue per user (ARPU) (collectively, our "Facebook metrics") based on user activity only on Facebook and Messenger and not on our other products.

- We believe our Family metrics better reflect the size of our community and the fact that many people are using more than one of our products. As a result, over time we intend to report our Family metrics as key metrics in place of DAUs, MAUs, and ARPU.

- While these numbers are based on what we believe to be reasonable estimates of our user base for the applicable period of measurement, there are inherent challenges in measuring usage of our products across large online and mobile populations around the world. The methodologies used to measure these metrics require significant judgment and are also susceptible to algorithm or other technical errors. In addition, we are continually seeking to improve our estimates of our user base, and such estimates may change due to improvements or changes in our methodology. We regularly review our processes for calculating these metrics, and from time to time we discover inaccuracies in our metrics or make adjustments to improve their accuracy, which can result in adjustments to our historical metrics. Our ability to recalculate our historical metrics may be impacted by data limitations or other factors that require us to apply different methodologies for such adjustments. We generally do not intend to update previously disclosed Family metrics for any such inaccuracies or adjustments that are within the error margins disclosed below.

- In addition, our Family metrics and Facebook metrics estimates will differ from estimates published by third parties due to differences in methodology or other factors such as data limitations or other challenges in measuring large online and mobile populations. For example, our Family metrics estimates in some instances exceed estimates of addressable online and mobile populations that are based on data published by third parties.

Family Metrics

- Many people in our community have user accounts on more than one of our products, and some people have multiple user accounts within an individual product. Accordingly, for our Family metrics, we do not seek to count the total number of user accounts across our products because we believe that would not reflect the actual size of our community. Rather, our Family metrics represent our estimates of the number of unique people using at least one of Facebook, Instagram, Messenger, and WhatsApp.

- We do not require people to use a common identifier or link their accounts to use multiple products in our Family, and therefore must seek to attribute multiple user accounts within and across products to individual people. To calculate these metrics, we rely upon complex techniques, algorithms and machine learning models that seek to count the individual people behind user accounts, including by matching multiple user accounts within an individual product and across multiple products when we believe they are attributable to a single person, and counting such group of accounts as one person. These techniques and models require significant judgment, are subject to data and other limitations discussed below, and inherently are subject to statistical variances and uncertainties.

- We estimate the potential error in our Family metrics primarily based on user survey data, which itself is subject to error as well. While we expect the error margin for our Family metrics to vary from period to period, we estimate that such margin generally will be approximately 3% of our worldwide MAP.

- At our scale, it is very difficult to attribute multiple user accounts within and across products to individual people, and it is possible that the actual numbers of unique people using our products may vary significantly from our estimates, potentially beyond our estimated error margins. As a result, it is also possible that our Family metrics may indicate changes or trends in user numbers that do not match actual changes or trends.

- To calculate our estimates of Family DAP and MAP, we currently use a series of machine learning models that are developed based on internal reviews of limited samples of user accounts and calibrated against user survey data. We apply significant judgment in designing these models and calculating these estimates. For example, to match user accounts within individual products and across multiple products, we use data signals such as similar device information, IP addresses, and user names. We also calibrate our models against data from periodic user surveys of varying sizes and frequency across our products, which are inherently subject to error. The timing and results of such user surveys have in the past contributed, and may in the future contribute, to changes in our reported Family metrics from period to period. In addition, our data limitations may affect our understanding of certain details of our business and increase the risk of error for our Family metrics estimates. Our techniques and models rely on a variety of data signals from different products, and we rely on more limited data signals for some products compared to others. For example, as a result of limited visibility into encrypted products, we have fewer data signals from WhatsApp user accounts and primarily rely on phone numbers and device information to match WhatsApp user accounts with accounts on our other products. Similarly, although Messenger Kids users are included in our Family metrics, we do not seek to match their accounts with accounts on our other applications for purposes of calculating DAP and MAP. 

- Any loss of access to data signals we use in our process for calculating Family metrics, whether as a result of our own product decisions, actions by third-party browser or mobile platforms, regulatory or legislative requirements, or other factors, also may impact the stability or accuracy of our reported Family metrics, as well as our ability to report these metrics at all.

- Our estimates of Family metrics also may change as our methodologies evolve, including through the application of new data signals or technologies, product changes, or other improvements in our user surveys, algorithms, or machine learning that may improve our ability to match accounts within and across our products or otherwise evaluate the broad population of our users. In addition, such evolution may allow us to identify previously undetected violating accounts (as defined below).

- We regularly evaluate our Family metrics to estimate the percentage of our MAP consisting solely of "violating" accounts. We define "violating" accounts as accounts which we believe are intended to be used for purposes that violate our terms of service, including bots and spam. In the fourth quarter of 2022, we estimated that approximately 3% of our worldwide MAP consisted solely of violating accounts. Such estimation is based on an internal review of a limited sample of accounts, and we apply significant judgment in making this determination. For example, we look for account information and behaviors associated with Facebook and Instagram accounts that appear to be inauthentic to the reviewers, but we have limited visibility into WhatsApp user activity due to encryption. In addition, if we believe an individual person has one or more violating accounts, we do not include such person in our violating accounts estimation as long as we believe they have one account that does not constitute a violating account. From time to time, we disable certain user accounts, make product changes, or take other actions to reduce the number of violating accounts among our users, which may also reduce our DAP and MAP estimates in a particular period. We intend to disclose our estimates of the percentage of our MAP consisting solely of violating accounts on an annual basis. Violating accounts are very difficult to measure at our scale, and it is possible that the actual number of violating accounts may vary significantly from our estimates.

- The numbers of Family DAP and MAP discussed in this presentation, as well as ARPP, do not include users on our other products, unless they would otherwise qualify as DAP or MAP, respectively, based on their other activities on our Family products.

Facebook Metrics

- We regularly evaluate our Facebook metrics to estimate the number of "duplicate" and "false" accounts among our MAUs. A duplicate account is one that a user maintains in addition to his or her principal account. We divide "false" accounts into two categories: (1) user-misclassified accounts, where users have created personal profiles for a business, organization, or non- human entity such as a pet (such entities are permitted on Facebook using a Page rather than a personal profile under our terms of service); and (2) violating accounts, which represent user profiles that we believe are intended to be used for purposes that violate our terms of service, such as bots and spam. The estimates of duplicate and false accounts are based on an internal review of a limited sample of accounts, and we apply significant judgment in making this determination. For example, to identify duplicate accounts we use data signals such as identical IP addresses and similar user names, and to identify false accounts we look for names that appear to be fake or other behavior that appears inauthentic to the reviewers. Any loss of access to data signals we use in this process, whether as a result of our own product decisions, actions by third-party browser or mobile platforms, regulatory or legislative requirements, or other factors, also may impact the stability or accuracy of our estimates of duplicate and false accounts. Our estimates also may change as our methodologies evolve, including through the application of new data signals or technologies or product changes that may allow us to identify previously undetected duplicate or false accounts and may improve our ability to evaluate a broader population of our users. Duplicate and false accounts are very difficult to measure at our scale, and it is possible that the actual number of duplicate and false accounts may vary significantly from our estimates.

- In the fourth quarter of 2022, we estimated that duplicate accounts may have represented approximately 11% of our worldwide MAUs. We believe the percentage of duplicate accounts is meaningfully higher in developing markets such as the Philippines and Vietnam, as compared to more developed markets. In the fourth quarter of 2022, we estimated that false accounts may have represented approximately 4-5% of our worldwide MAUs. Our estimation of false accounts can vary as a result of episodic spikes in the creation of such accounts, which we have seen originate more frequently in specific countries such as Indonesia, Nigeria, and Vietnam. From time to time, we disable certain user accounts, make product changes, or take other actions to reduce the number of duplicate or false accounts among our users, which may also reduce our DAU and MAU estimates in a particular period. We intend to disclose our estimates of the number of duplicate and false accounts among our MAUs on an annual basis.

- The numbers of DAUs and MAUs discussed in this presentation, as well as ARPU, do not include users on Instagram, WhatsApp, or our other products, unless they would otherwise qualify as DAUs or MAUs, respectively, based on their other activities on Facebook.

User Geography
- Our data regarding the geographic location of our users is estimated based on a number of factors, such as the user's IP address and self-disclosed location. These factors may not always accurately reflect the user's actual location. For example, a user may appear to be accessing Facebook from the location of the proxy server that the user connects to rather than from the user's actual location. The methodologies used to measure our metrics are also susceptible to algorithm or other technical errors, and our estimates for revenue by user location and revenue by user device are also affected by these factors.