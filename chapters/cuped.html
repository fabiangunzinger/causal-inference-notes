<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>cuped</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="cuped_files/libs/clipboard/clipboard.min.js"></script>
<script src="cuped_files/libs/quarto-html/quarto.js"></script>
<script src="cuped_files/libs/quarto-html/popper.min.js"></script>
<script src="cuped_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="cuped_files/libs/quarto-html/anchor.min.js"></script>
<link href="cuped_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="cuped_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="cuped_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="cuped_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="cuped_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="cuped" class="level1">
<h1>CUPED</h1>
<p>todo:</p>
<ul>
<li>Incorporate Rice content on prediction and MSE (chapter 4.4 and ipad notes), which exactly gets us the CUPED result, again showing that CUPED is a re-invention of linear regression and standard stats</li>
</ul>
<p>CUPED stands for “Controlled experiments Using Pre-Experiment Data”, and reduces variance by partialling out outcome metric variance based on pre-experiment data.</p>
<p>Questions: - [ ] Why does fillna(y) not increase variance - [ ] What happens if pre and post perfectly correlated</p>
<section id="how-cuped-works" class="level2">
<h2 class="anchored" data-anchor-id="how-cuped-works">How CUPED works</h2>
<p>CUPED supposes that in addition to an outcome metric <span class="math inline">\(y\)</span>, we also have access to another variable, <span class="math inline">\(x\)</span>, which is correlated with <span class="math inline">\(y\)</span> but uncorrelated with the treatment assignment of our experiment. Pre-experiment data of the outcome metric is a good candidate for <span class="math inline">\(x\)</span> when it is available – it’s what’s commonly used, and where the method gets its name from. With that in hand, notice that we can write</p>
<p>– the most obvious candidate that has been found to work well is pre-experiment data of the metric of interest.</p>
<p>We can then create a new variable</p>
<p><span class="math display">\[
\tilde{y} = y - \theta x,
\]</span></p>
<p>where – it turns out – the optimal choice for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\frac{cov(y, x)}{var(x)}\)</span>, which we can easily calculate from the available data.</p>
<p>This is useful because it can be shown that if we now evaluate our experiment using <span class="math inline">\(\tilde{y}\)</span> instead of <span class="math inline">\(y\)</span>, the treatment estimate will be the same but it’s standard error will be lower, which will increase power. The standard error of the treatment effect estimate is lower because the variance of <span class="math inline">\(\tilde{y}\)</span> is lower than that of <span class="math inline">\(y\)</span> whenever <span class="math inline">\(cov(y, x) \neq 0\)</span>, that is, whenever <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are indeed correlated. To be precise, we have:</p>
<p><span class="math display">\[
var(\tilde{y}) = var(y)(1 - \rho^2),
\]</span></p>
<p>where <span class="math inline">\(\rho\)</span> is the Pearson correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\rho = \frac{cov(y, x)}{var(x)var(y)}.
\]</span></p>
<p>Presentation notes:</p>
<ul>
<li><p>Assume that in addition to our outcome variable <span class="math inline">\(Y\)</span>, we have access to another random variable, <span class="math inline">\(X\)</span>, which is independent of the treatment assignment and has known expectation <span class="math inline">\(\mathbb{E}X\)</span>.</p></li>
<li><p>We can then define <span class="math inline">\(\bar{Y}^{cuped} = \bar{Y} - \theta \bar{X} + \theta \mathbb{E}X\)</span> for both treatment and control groups, and compare avearge outcomes in this adjusted metric.</p></li>
<li><p>Our new estimator is: <span class="math inline">\(\hat{\tau}^{cuped} = \bar{Y}^{cuped}_t - \bar{Y}^{cuped}_c\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{\tau}^{cuped}\)</span> is unbiased since <span class="math inline">\(\mathbb{E}\left[\hat{\tau}^{cuped}\right] = \bar{Y}_t - \bar{Y}_c = \tau\)</span> (proof in appendix below).</p></li>
<li><p>If treatment effects are small (which, in practice, they usually are) and for an optimal choice of <span class="math inline">\(\theta\)</span>: <span class="math inline">\(\mathbb{V}\left(\hat{\tau}^{cuped}\right) \simeq \left(\frac{s_t}{N_t} + \frac{s_c}{N_c}\right)\left(1 - \rho^2\right)\)</span>, where <span class="math inline">\(\rho\)</span> is the correlation coefficient of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>.</p></li>
<li><p>Hence: <span class="math inline">\(\frac{\mathbb{V}(\hat{\tau}^{cuped})}{\mathbb{V}(\hat{\tau}^{dif})} = 1 - \rho^2\)</span> – the higher the correlation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, the more CUPED reduces the variance of our treatment estimate.</p></li>
</ul>
<p><span class="math inline">\(\hat{\tau}^{cuped}\)</span> is unbiased:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}\left[\hat{\tau}^{cuped}\right] &amp;= \mathbb{E}\left[\bar{Y}^{cuped}_t - \bar{Y}^{cuped}_c\right] \\
&amp;= \mathbb{E}\left[\left(\bar{Y}_t - \theta \bar{X}_t + \theta \mathbb{E}X\right) - \left(\bar{Y}_c - \theta \bar{X}_c + \theta \mathbb{E}X\right)\right] \\
&amp;= \mathbb{E}\left[\left(\bar{Y}_t - \theta \bar{X}_t\right) - \left(\bar{Y}_c - \theta \bar{X}_c\right)\right] \\
&amp;= \mathbb{E}\left[\bar{Y}_t - \bar{Y}_c\right] \\
&amp;= \mathbb{E}\left[\frac{1}{N_t}\sum_{\text{i:T=1}}Y_i- \frac{1}{N_c}\sum_{\text{i:T=0}}Y_i\right] \\
&amp;= \frac{1}{N_t} N_t \mathbb{E}Y_t - \frac{1}{N_c} N_c \mathbb{E}Y_c \\
&amp;= \mathbb{E}Y_t - \mathbb{E}Y_c \\
&amp;= \bar{Y}_t - \bar{Y}_c \\
&amp;= \tau
\end{align*}
\]</span></p>
<p><span class="math inline">\(\hat{\tau}^{cuped}\)</span> has variance:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{V}\left(\hat{\tau}^{cuped}\right) &amp;= \mathbb{V}\left(\bar{Y}^{cuped}_t - \bar{Y}^{cuped}_c\right) \\
&amp;= \mathbb{V}\left(\bar{Y}^{cuped}_t\right) + \mathbb{V}\left(\bar{Y}^{cuped}_c\right) \\
&amp;= \mathbb{V}\left(\bar{Y}_t - \theta \bar{X}_t + \theta \mathbb{E}X\right) + \mathbb{V}\left(\bar{Y}_c - \theta \bar{X}_c + \theta
\mathbb{E}X\right) \\
&amp;= \mathbb{V}\left(\bar{Y}_t - \theta \bar{X}_t\right) + \mathbb{V}\left(\bar{Y}_c - \theta \bar{X}_c\right) \\
&amp;= \frac{1}{N_t}\mathbb{V}\left(Y_t - \theta X_t\right) + \frac{1}{N_c}\mathbb{V}\left(Y_c - \theta X_c\right) \\
&amp;= \frac{1}{N_t}\left[\mathbb{V}(Y_t) + \theta^2 \mathbb{V}(X_t) - 2\theta Cov(Y_t, X_t)\right] + \frac{1}{N_c}\left[\mathbb{V}(Y_c) + \theta^2 \mathbb{V}(X_c) - 2\theta Cov(Y_c, X_c)\right]
\end{align*}
\]</span></p>
<p>This is minimised for:</p>
<p><span class="math display">\[
\theta^* = \frac{Cov(Y_t, X_t) + Cov(Y_c, X_c)}{\mathbb{V}(X_t) + \mathbb{V}(X_c)}
\]</span></p>
<p>In practice, a common approach is to pool the data to get:</p>
<p><span class="math display">\[
\begin{align*}
\theta^*_p &amp;= \frac{Cov(Y, X) + Cov(Y, X)}{\mathbb{V}(X) + \mathbb{V}(X)}\\
&amp;= \frac{Cov(Y, X)}{\mathbb{V}(X)},
\end{align*}
\]</span></p>
<p>and to assume that <span class="math inline">\(\mathbb{V}(X_t) \simeq \mathbb{V}(X_t)\)</span>, and <span class="math inline">\(Cov(Y_t, X_t) \simeq Cov(Y_c, X_c)\)</span>, which is reasonable as long as the treatment effect is not too large (see discussion towards the end <a href="https://alexdeng.github.io/causal/sensitivity.html#vrreg">here</a>). If, in addition, we let <span class="math inline">\(\rho = Cor(X, Y)\)</span>, then we have</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{V}\left(\hat{\tau}^{cuped}\right) &amp;\simeq \frac{1}{N_t}\left[\mathbb{V}(Y_t) + (\theta^*_p)^2 \mathbb{V}(X) - 2 \theta^*_p Cov(Y, X)\right] + \frac{1}{N_c}\left[\mathbb{V}(Y_c) + (\theta^*_p)^2 \mathbb{V}(X) - 2 \theta^*_p Cov(Y, X)\right]\\
&amp;= \frac{1}{N_t}\left[\mathbb{V}(Y_t) + \left(\frac{Cov(Y, X)}{\mathbb{V}(X)}\right)^2 \mathbb{V}(X) - 2 \left(\frac{Cov(Y, X)}{\mathbb{V}(X)}\right) Cov(Y, X)\right] + \frac{1}{N_c}\left[\mathbb{V}(Y_c) + \left(\frac{Cov(Y, X)}{\mathbb{V}(X)}\right)^2 \mathbb{V}(X) - 2 \left(\frac{Cov(Y, X)}{\mathbb{V}(X)}\right) Cov(Y, X)\right]\\
&amp;= \frac{1}{N_t}\left[\mathbb{V}(Y_t) - \frac{Cov(Y, X)^2}{\mathbb{V}(X)}\right] + \frac{1}{N_c}\left[\mathbb{V}(Y_c) - \frac{Cov(Y, X)^2}{\mathbb{V}(X)}\right]\\
&amp;= \frac{1}{N_t}\left[\mathbb{V}(Y_t) - \frac{\left(\rho\sqrt{\mathbb{V}(X)}\sqrt{\mathbb{V}(Y)}\right)^2}{\mathbb{V}(X)}\right] + \frac{1}{N_c}\left[\mathbb{V}(Y_c) - \frac{\left(\rho\sqrt{\mathbb{V}(X)}\sqrt{\mathbb{V}(Y)}\right)^2}{\mathbb{V}(X)}\right]\\
&amp;= \frac{1}{N_t}\left[\mathbb{V}(Y_t) - \rho^2\mathbb{V}(Y)\right] + \frac{1}{N_c}\left[\mathbb{V}(Y_c) - \rho^2\mathbb{V}(Y)\right]\\
&amp;= \frac{\mathbb{V}(Y_t)}{N_t}(1 - \rho^2) + \frac{\mathbb{V}(Y_c)}{N_c}(1 - \rho^2)\\
&amp;= \left[\frac{\mathbb{V}(Y_t)}{N_t} + \frac{\mathbb{V}(Y_c)}{N_c}\right]\left(1 - \rho^2\right)
\end{align*}
\]</span></p>
<p>In practice, we use the sample variances <span class="math inline">\(s_t = \frac{1}{N_t - 1}\sum_{\text{i:T=1}}\left(Y_i - \bar{Y}_t^{obs}\right)^2\)</span> and <span class="math inline">\(s_c = \frac{1}{N_c - 1}\sum_{\text{i:T=0}}\left(Y_i - \bar{Y}_c^{obs}\right)^2\)</span> as unbiased estimators for the variances of treatment and control outcomes, and a sample estimate of the correlation coefficient, <span class="math inline">\(\rho\)</span>.</p>
<p><strong>Things to notice</strong></p>
<ul>
<li><p>The main “trick” CUPED relies on for unbiasedness is the fact that we don’t actually have to know <span class="math inline">\(\mathbb{E}X\)</span> to obtain an unbiased estimator since it cancels out when we take the difference of two CUPED-adjusted variables.</p></li>
<li><p>Any fixed value of <span class="math inline">\(\theta\)</span> will give us an unbiased estimator of <span class="math inline">\(\tau\)</span>, so pooling the data and assuming equal variances and covariances in the treatment and control groups, as we did to calculate the variance, effect the degree of variance reduction only. If we didn’t make these assumptions, the factor by which CUPED reduces variance would be a more complicated term than <span class="math inline">\(\left(1 - \rho^2\right)\)</span>, involving separte variances and covariances from the treatment and control groups.</p></li>
</ul>
</section>
<section id="features-of-cuped" class="level2">
<h2 class="anchored" data-anchor-id="features-of-cuped">Features of CUPED</h2>
<ul>
<li><p>Also permits non-linear adjustments (i.e.&nbsp;not reliant on linearity assumptions in OLS)</p></li>
<li><p>Reduces biase (see statsic post)</p></li>
</ul>
</section>
<section id="is-cuped-regression-adjustment" class="level2">
<h2 class="anchored" data-anchor-id="is-cuped-regression-adjustment">Is CUPED regression adjustment?</h2>
<p>todo: - Is CUPED regression adjustment? Identical to regression only in simple case (show FWL link – have separate post on understanding FWL with relevant regression examples)</p>
<ul>
<li><p>Take inclusion of constant into accound (centering variables, but doesn’t change correlations)</p></li>
<li><p>Show that in multiple regression, var(Y_adjusted) = var(y_unadjusted)(1 - R^2) – so, CUPED result is just a specieal case where k = 1, in which case R^2 = rho.</p></li>
<li><p>I think it really is. CUPED can be extended to (make notation consistent with CUPED)</p></li>
</ul>
<p><span class="math display">\[
y' = y - \theta \bar{f(X)} + \theta E(f(X))
\]</span></p>
<ul>
<li><p>In above, it can be shown that optimal <span class="math inline">\(f(X) = E(Y|X)\)</span> (show this). CUPED uses best linear predictor. CUPAC extends this to non-linear predictors, generating <span class="math inline">\(\hat{y} = g(X) = E(Y|X)\)</span>. This is an example of using a non-linear function of X as Deng points out is possible. But if you do this, theta is still the OLS coefficient, so it’s still equivalent to just sticking g(X) into the regression as a covariate.</p></li>
<li><p>The motivation for CUPED seems to be a bit of a strawman: that regression adjustment relies on assumption that expectation of Y conditional on X is linear. Imbens and Rubin in 7.5 show that this is not required. Neither is homoskedasticity, or, rather, this should hold anyways given randomisation. Study chapter 7.</p></li>
<li><p>As pointed out in <span class="citation" data-cites="tang2000control">@tang2000control</span>, the Deng argument that you don’t need linear assumptions in CUPED seems to be inspired by the Friedman critique of OLS for experiments.</p></li>
<li><p>So the CUPED motivation might not be a strawem as much as a result of the debate in statistics about whether you should use OLS for experiment analysis or not. This is probs the way I want to frame the entire issue – review debate (key point of Friedman critique, Lin’s repsonse, Imbens and Rubin’s take, and then discuss cuped and regression adjustment in that light). Basically, if you follow Lin, then cuped is just regression adjustment. If you follow Friedman, then you shouldn’t do regression adjustment and use cuped. But then do practical comparisons and show that you get the same results in practice, which seems to show that using linear regression clearly works.</p></li>
</ul>
<p>It turns out that in the simple cases discussed above, it doesn’t – the two approaches are identical! Seeing why requires a few steps.</p>
<p>First, we know (from the Frisch-Waugh-Lowell <a href="https://en.wikipedia.org/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem">theorem</a>) that if we were to estimate the alternative model</p>
<p><span class="math display">\[
\tilde{y}_i = \alpha + \beta_1^* \tilde{d}_i + \epsilon_i,
\]</span></p>
<p>where <span class="math inline">\(\tilde{y}_i\)</span> is the residual from regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span>, and <span class="math inline">\(\tilde{d}_i\)</span> the residual from regressing <span class="math inline">\(d\)</span> on <span class="math inline">\(x\)</span>, we would find that <span class="math inline">\(\beta_1^* = \beta_1\)</span>. That is, the two models are identical.</p>
<p>Second, to obtain <span class="math inline">\(\tilde{y}\)</span>, we first estimate</p>
<p><span class="math display">\[
y = \alpha + \delta x_i + u_i,
\]</span></p>
<p>and then calculate <span class="math inline">\(\tilde{y} = y - \delta x\)</span> (the calculation of <span class="math inline">\(\tilde{d}_i\)</span> works analogously). Given that this is a simple regression model, we know that <span class="math inline">\(\delta = \frac{cov(y, x)}{var(x)}\)</span> so that</p>
<p><span class="math display">\[
\tilde{y} = y - \frac{cov(y, x)}{var(x)}x.
\]</span></p>
<p>Finally, above in our discussion of CUPED we have seen that the CUPED-adjusted outcome metric <span class="math inline">\(\tilde{y}\)</span> is defined in exactly the same way. Hence, to evaluate an experiment with a CUPED-adjusted outcome, we would estimate the model:</p>
<p><span class="math display">\[
\tilde{y}_i = \alpha + \beta^* d_i + \epsilon_i^*,
\]</span></p>
<p>Notice that the only difference to model (2) is that we don’t adjust the treatment assignment – we use <span class="math inline">\(d_i\)</span> instead of <span class="math inline">\(\tilde{d}_i\)</span>. But if the treatment assignment is random, then <span class="math inline">\(cov(d, x) = 0\)</span> so that the adjustment has no effect and we have <span class="math inline">\(\tilde{d}_i = d_i\)</span>. Hence, the two approaches are the same.</p>
<p>In general, regression adjustment and CUPED are identical if two conditions hold: (1) the treatment indicator is independent of <span class="math inline">\(x\)</span>, and (2) we use a linear CUPED adjustment. In the context of experimentation, where treatment is random, and with the classical (linear) CUPED adjustment discussed above, this is always the case.</p>
<p>The reason why in practice we get a don’t get the exact same result is that the covariance of <span class="math inline">\(d\)</span> and <span class="math inline">\(x\)</span> is only approximately 0, hence providing very similar, but not identical results.</p>
<section id="is-cuped-did" class="level3">
<h3 class="anchored" data-anchor-id="is-cuped-did">Is CUPED DiD?</h3>
<ul>
<li>Is CUPED DiD? (based on Courthoud) – same if theta = 1</li>
</ul>
</section>
<section id="non-linear-extensions" class="level3">
<h3 class="anchored" data-anchor-id="non-linear-extensions">Non-linear extensions</h3>
</section>
</section>
<section id="blog-post-notes" class="level2">
<h2 class="anchored" data-anchor-id="blog-post-notes">Blog post notes</h2>
<p>CUPED is a re-invention of multiple linear regression. Evan Miller (<a href="https://www.evanmiller.org/you-cant-spell-cuped-without-frisch-waugh-lovell.html">here</a>) and Matteo Courthoud (<a href="https://towardsdatascience.com/understanding-cuped-a822523641af">here</a>) make similar points in their excellent posts on the topic, but – given my starting point – neither quite helped me fully understand what is going on. This post is my attempt to do that.</p>
<p>In particular, I think that to really understand the connection between multiple linear regression and CUPED, you have to understand the linear algebra of the Frisch-Waugh-Lowell theorem (FWL) rather than just knowing that that theorem says, and to understand that, you have to understand the concept of a projection.</p>
<section id="cuped-implementation-detail" class="level3">
<h3 class="anchored" data-anchor-id="cuped-implementation-detail">CUPED implementation detail</h3>
<ul>
<li>Because we show control, treatment, difference, and CIs in our dashboard, we need to display unadjusted values.</li>
<li>This means our tests and CI’s have to be based on the unadjusted delta and the adjusted standard error</li>
</ul>
<p><em>Test statistic for unadjusted metric</em> <span class="math display">\[
Z_u = \frac{\bar{Y}^{obs}_{t} - \bar{Y}^{obs}_{c}}{\sqrt{\frac{s^2_t}{N_t} + \frac{s^2_c}{N_c}}}
\]</span></p>
<p><em>Test statistic for CUPED-adjusted metric</em> <span class="math display">\[
Z_a = \frac{\left(\bar{Y}^{obs}_{t} - \bar{Y}^{obs}_{c}\right) - \theta \left(\bar{X}^{obs}_{t} - \bar{X}^{obs}_{c}\right)}{\sqrt{\left(\frac{s^2_t}{N_t} + \frac{s^2_c}{N_c}\right) (1 - \rho^2)}}
\]</span></p>
<p><em>Implemented test statistic</em> <span class="math display">\[
Z_i = \frac{\bar{Y}^{obs}_{t} - \bar{Y}^{obs}_{c}}{\sqrt{\left(\frac{s^2_t}{N_t} + \frac{s^2_c}{N_c}\right) (1 - \rho^2)}}
\]</span></p>
<p>Critical assumption:</p>
<p><span class="math display">\[
\bar{X}^{obs}_{t} = \bar{X}^{obs}_{c}
\]</span></p>
<p>###&nbsp;Useful resources</p>
<ul>
<li><p><a href="https://www.exp-platform.com/Documents/2013-02-CUPED-ImprovingSensitivityOfControlledExperiments.pdf">Deng et al.&nbsp;2013 – original CUPED paper</a> – the original paper</p></li>
<li><p><a href="https://alexdeng.github.io/causal/sensitivity.html#vrreg">Variance reduction section of Deng’s causal inference book</a> – more in-depth discussion of some aspects of CUPED and its link to regression adjustment</p></li>
<li><p><a href="https://www.kdd.org/kdd2016/papers/files/adp0945-xieA.pdf">Improving the Sensitivity of Online Controlled Experiments: Case Studies at Netflix</a></p></li>
<li><p><a href="https://booking.ai/how-booking-com-increases-the-power-of-online-experiments-with-cuped-995d186fff1d">How Booking.com increases the power of online experiments with CUPED</a></p></li>
<li><p><a href="https://www.statsig.com/blog/cuped">CUPED explained – statsig post</a></p></li>
<li><p><a href="https://www.evanmiller.org/you-cant-spell-cuped-without-frisch-waugh-lovell.html">You can’t spell CUPED without Frisch-Waugh-Lovell</a> – good post exploring link to FWL theorem</p></li>
<li><p><a href="https://towardsdatascience.com/understanding-cuped-a822523641af">Understanding CUPED</a> – good post exploring link to multiple regression (also using FWL theorem) and DiD.</p></li>
<li><p><a href="https://bytepawn.com/reducing-variance-in-ab-testing-with-cuped.html#reducing-variance-in-ab-testing-with-cuped">Reducing variance in A/B testing with CUPED</a></p></li>
<li><p><a href="https://blog.statsig.com/cuped-on-statsig-d57f23122d0e">CUPED on Statsig</a></p></li>
</ul>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>