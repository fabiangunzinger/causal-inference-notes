# Experiment design


## Designs
- See BIT blue book section 5

Classic RCT

Clustered

Step-wedge / phase-in design

- Might allow us to estimate long-term effects. If we focus on a specific age-group or cohort. Because then certain people will never receive the treatment and can thus act as a long-run control group. (Example: if we focus on kids in their last year of school and phase-in periods are a year, then once we get to the third school, two cohorts will have graduated and will never receive the program.)

Wait-list design

Factorial design 

Encouragement design

- It measures the effect of those who took up the program because of the encouragement but would not have otherwise (LATE). So it's particularly helpful if we are interested in the effect of encouragement on marginal participants. 

- The encouragement must not encourage some and discourage others (monotonicity assumption).


What's the difference between a "between-subjects design" and a "within-subjects design" and what are their advantages? What is a "mixed-design"?

- A between design compares the treatment group to a control group, a within design compares the treatment group to itself before the treatment, so that each participant acts as their own control group.
- The strength of a between design is that it protects against confounding factors, that of a within design that it improves precision (because we have baseline data).
- A mixed design is where we have a control group and baseline data. If we use that baseline, the estimator is then a difference-in-difference estimator, which is statistically more efficient.


## Choosing unit of randomisation 

Factors to consider: 
- Spillovers
- The level of treatment administration
- The level of measurement (randomisation level needs to be the same or higher)
- Power
- Attrition and compliance (within-group randomisation may lead to resentment among participants and staff)
- Feasibility (what is easiest, cheapest, politically possible, ...)



## Dimensions along which we can vary randomisation 

Aspect: 
- access
- access around cutoff
- timing
- encouragement

Level: 
- individual 
- group



## The assignment mechanism

### Bernoulli trials

### Completely randomised experiments

### Stratified randomised experiments

Stratification is a treatment allocation method in which the pool of eligible units is first divided into strata (or groups or blocks) so as to ensure balance along certain variables. Random assignment is then performed within each group.

Effect: Random allocation of treatment ensures that control and treatment group will be similar in expectation, stratification ensures that along certain dimensions they will be in practice. If we stratify by gender and language, for instance, we make sure that both control and treatment group have the same proportion of women who speak a given language.

Advantages:  1.) It can increase precision of the estimator by lowering residual variance (intuitively, it has the opposite effect of clustering: it makes samples more representative of the overall population, which means that repeated samples will be more similar, which means that the standard error of the estimator is smaller). 2.) It allows analysis by subgroups.

Disadvantages: it can lead to very different sample sizes in different strata.

When to stratify?
- We should stratify if we want to:
- increase power
- achieve balance
- analyse data by subgroups

...and what variables to use? Use variables that
- are highly correlated with the outcome variable
- are discrete (can discretise by creating buckets)
- are the subgroups of interest


How to adjust analysis? Include dummy for each variable used for stratification in the regression. (explain why...)

### Paired randomised experiments

Paired randomisation is an extreme form of stratification where the size of each stratum equals the number of treatment cells.

If we have three cells (two treatments and one control) and we pair based on test scores, we pick groups of three students, starting with the three highest scoring ones, and randomly assign one of them to each cell.

The aim is to increase power and achieve balance, and it's particularly useful if we have small samples.

The danger is that if we have attrition and one of the three students drops out, we lose all three observations (OLS will drop all observations with the relevant dummy because there is a missing value). One remedy is to have strata of size double or triple the cell number.

### Matching

How to adjust analysis? Matching is an extreme form of stratifying, and so the approach is the same: we include a dummy for each (but one) matched pair. (If we have matched on test scores, we include a dummy for all but one possible test scores).


## Choosing number of units per arm

- Show that equal split maximises power (use power formula and show that power is max for p = 1-p )

- If we have many treatment groups, then more units should be allocated to control to estimate its effect more precisely (add formula)

- if sample variances differ, the ratio of the sample sizes should equal the ratio of the standard deviations.

## Hypothesis formulation




## Treatment effects

ATE is the average difference in potential outcomes of all subjects. E[y1 - y0]

ATE is a weighted average of ATET and average treatment effect of untreated.

ATE is also a weighted average of the compliers, always-takers, and never-takers.

ATE = ATET if treatment effects are homogenous (if they are not, then experiment measures ATET).

ATE = ITT under homogenous effects or under perfect compliance.


ATET (or TOT) is the average difference in potential outcomes of treated subjects. E[y1 - y0 | D=1] (D is treatment)

ATET = LATE in an experiment with no always-takers. This is relevant because there are many cases of onesided non-compliance (if participants who are randomised to treatment do not take it up but nobody who wasn't randomised to treatment has access).


ITT is the average difference in potential outcomes of those assigned to treatment (a weighted average between those who accepted treatment and those who didn't).

LATE is the average difference in potential outcomes of compliers.




## Imperfect compliance

Partial complience

- Partial compliance occurs if, for some reason, some people in the treatment group are not treated or some people in the control group are.

- It's problematic because it reduces the difference in treatment exposure between treatment and control group (in the extreme, if they are equal, we learn nothing), and because they might make treatment takeup non-random.

- A few things can help to limit the problem: make takeup easy and/or incentivise it, randomise at a higher level, and provide a treatment to everyone.

- We can adjust our analysis by calculating LATE, either using the Wald estimator (ITT / difference in take-up between treatment and control groups) or 2SLS where we instrument the behaviour we want to encourage with the treatment dummy (and possibly other covariates).


Non-compliers

Do not drop non-compliers or re-assign them to the control group -- compliers and non-compliers are different so that dropping or reclassifying non-compliers would re-introduce self-selection into out two samples, which defeats the whole point of the randomisation.


Defiers

- They are the opposite of compliers: they either take up the treatment because they were assigned to control or the other way around.

- They might occur in an encouragement design if they overestimated the benefit of treatment and got discouraged by the information provided in the treatment.

- They might make us significantly misinterpret the true effect (RRE page 303, or, better, MHE p 156).

- We can deal with them only if they form an identifiable subgroup, in which case we can estimate the treatment effect on defiers and compliers separately and calculate an average treatment effect.


## Attrition

Attrition is when, for some reason, we cannot collect endline data on some units.

It's a problem because when it's systematic rather than random, treatment and control group are no longer comparable, which is a threat to internal validity. (Even if the same number of people drop out, if they are different type, the problem is the same.) Also, it reduces sample size and thus power.

We can limit it if we promise access to the program to everyone (phase-in design), change the level of randomisation, and improve data collection.

In our analysis we should 1. Report the extent of attrition, 2. Check for differential attrition (between groups, and within groups based on observable characteristics), and 3. Determine the range of estimates given attrition using a selection model or bounds.

One method is to use Heckman's selection model: we look at the characteristics of those who attrite, assume that they have the same outcomes as those with the same characteristics for which we have data, and then fill in their outcome variables accordingly.

Another method is to use bounds. There are two types.

Manski-Horowitz bounds: Lower bound: replace all missing values in treatment with the least favourable outcome value from the observed sample and replace all missing values in control with most favourable value in the observed sample. Upper bound created in a reverse way. Unless attrition is low and the outcome variable is tightly bounded, this tends to lead to very large bounds.

Lee bounds: We treat the estimate from the available data as an upper bound, and construct the lower bound by trimming from the sample with less attrition the observations that most contribute to the treatment effect.




## Interpretation

- We often think of experiment as answering: "Does the intervention work?", but what it really answers is "What is the average effect of the intervention with this specific implementation for this particular population at this time?"

- When we have multiple treatments or analyse subgroups separately, we need to test whether different estimates are significantly different from each other (can't just look at whether some are significantly different from zero while some aren't) 






## Q&A

Questions:

1. Why don't we include MDE in our hypothesis statement (i.e. "a will increase b by at least MDE for target population")? Wouldn't that be a good idea in a business context where we strongly care about an MDE? Don't we just not do that because academics, who developed the methods, don't usually care about an effect being of a certain size but only about learning what the effect is?

Answers:

1. Including the MDE's in the hypothesis would be incorrect. Why? Because what we formulate is the alternative hypothesis. Without altering the null hypothesis, which traditionally states the effect is zero, our two hypotheses would not be complete (H0 testing effect different from 0, HA asserting that effect is at least MDE means rejecting H0 would not imply support of HA). So why not adapt H0, too? Because the hypothesis is the thing we test when we calculate our test statistic (duh!), and so if included the MDE in the hypothesis formulation, then we'd have to include it in the calculation of the test statistic (subtract it from the observed difference), which would be incorrect. Why? The question confused the MDE with the effect size under the Null hypothesis, which are conceptually different things: the MDE is the smallest effect size we want to detect -- we are interested in effects that are as large or larger than the MDES, we're not testing whether the observed difference is significantly different from it, which is what the null hypothesis value is.