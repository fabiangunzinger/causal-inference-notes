# Projection {#sec-projection}

A projection is a transformation of a vector onto a subspace.

::: {.callout-note collapse=true}

## Subspaces

A subspace is a specific type of subset within a vector space that itself qualifies as a vector space. This means that any linear combination of vectors within the subspace must also belong to the subspace, as well as to the original, larger, space (linear combinations are formed by adding together scalar multiples of vectors).

For example, consider a 2-dimensional plane within the 3-dimensional space denoted as $\mathbb{R}^3$. This 2-dimensional plane can be a subspace of $\mathbb{R}^3$ if it includes all possible linear combinations of any vectors lying within this plane. However, a crucial condition for this plane to be a subspace is that it must pass through the origin of $\mathbb{R}^3$, which is the point (0, 0, 0). This requirement is necessary because a subspace must contain the zero vector (in this case, the zero vector of $\mathbb{R}^3$) to fulfill the property of containing scalar multiples (including the zero scalar) of its vectors.

Similarly, a line in $\mathbb{R}^3$ can be a subspace if it passes through the origin. This ensures that any linear combination of two vectors that lie on this line will also resultY
 in a vector that lies on the same line. The key aspect here is that the line, like the plane, must include the origin to accommodate the zero vector and maintain closure under scalar multiplication and vector addition (meaning that the results or these operations are also part of the subspace).
 
 In summary, a subspace is not just any subset of a vector space; it must itself satisfy all the criteria of a vector space, including the inclusion of the zero vector, and closure under both vector addition and scalar multiplication.
:::

There are different types of projections, but the one that's relevant for us here is *orthogonal projection*, which projects a vector onto the nearest point on a subspace, where ]"nearest" usually refers to the Euclidean distance.[^euclid_dist] Why this type of projection is called "orthogonal projection" will be come clear below.


## Projecting from 2-D onto 1-D

Projecting a vector in two-dimensional space onto a line that goes through the origin is a nice way to build an understanding for what a projection does.[^line]

Say we want to orthogonally project the vector $b$ onto a line defined by another vector, $a$, and we will call the resulting projection $p$. Hence, $p$ is the point on the line defined by $a$ that is nearest to the (tip of) the vector $b$. (Note, though, that $p$ is a vector, not a point. Calling it a point just clarifies what we're doing here.)

We can think of the line as being generated by scaling vector $a$ with a scalar $x$, so that choosing a suitable $x$ allows us to reach any point on the line. Finding $p$ then boils down to finding the value of $x$ that gets us to that point of the line that is closest to $b$. We can thus write $p = ax$.

![Projecting a 2d vector onto a line](../inputs/projection_onto_1d.png){width=75%}

Let's start by finding $p$. We can find it in different ways.

**Using calculus**:

Given that we define closeness based on the Euclidean distance, minimising the distance between the (tip of) the vector, $b$, and the projection, $p$,  is akin to solving the following problem:

$$
\begin{aligned}
argmin_{x} \sqrt{\sum_{i=1}^2{(b_i - p_i)^2}} &= argmin_{x} \sum_{i=1}^2{(b_i - p_i)^2} \\
&= argmin_{x} \sum_{i=1}^2{(b_i - xa_i)^2} \\
&= argmin_{x} (b - xa)'(b - xa),
\end{aligned}
$$

Calculating the derivative with respect to $x$ we get:

$$
\begin{aligned}
\frac{d}{dx} (b - xa)'(b - xa) &= (-a)'(b - xa) + (b - xa)'(-a) & \\
&= -a'b + xa'a - a'b + xa'a \\
&= -2a'b + 2xa'a
\end{aligned}
$$

Finlly, setting the result to 0 and solving for $x$ we get:

$$
\begin{aligned}
-2a'b + 2xa'a &= 0 \\
xa'a &= a'b \\
x &= (a'a)^{-1}a'b
\end{aligned}
$$

Hence, given that $p = ax$, we have:

$$
\begin{aligned}
p = ax = \underbrace{a(a'a)^{-1}a'}_\text{$P_a$}b,
\end{aligned}
$$

where $P_a$ is the projection matrix. 

Let's reflect for a moment what this all means. In general, pre-multiplying a vector by a matrix transforms the vector in a particular way. When we perform orthogonal projection, we pre-multiply a vector by a matrix that transforms the vector into that point on a subspace that it closest to the original vector. In our case here, pre-multiplying our initial vector $b$ by the projection matrix $P_a$ transforms $b$ into that point on $a$ that is closest to $b$, which we call $p$. Given that we define "nearest" using the Euclidean distance, it makes sense that the projection matrix would emerge out of the solution to the minimisation problem of finding the point on the subspace that minimises the Euclidean distance to the original vector.

**Using basic geometry:**

We could also find $p$ using our understanding of basic geometry. Looking at the figure above, it is intuitively obvious that the shortest path between the tip of $b$ and the line $a$ is that which is perpendicular to $a$ -- the point at which the perpendicular line meets $a$ is thus $p$. The path between $b$ and $p$ is simply $b - p$. In linear algebra terms, we thus want that path to be orthogonal to the line $a$.

::: {.callout-note collapse=true}

## Orthogonal vectors

Orthogonality is a generalisation of perpendicularity; two vectors that are orthogonal are perpendicular to each other. The word comes from the Greek "orthos", meaning "straight" or "right", and "gonia", meaning "angle". To test whether two vectors $x$ and $y$ are perpendicular, we check whether their dot product, $x'y$ equals zero.

But why do we know that the dot-product of orthogonal vectors is zero? The answer follows from the Pythagorean theorem, which states that for a right triangle with sides $a$ and $b$ and hypotenuse $c$ we have $a^2$ + $b^2$ = $c^2$. Now, if vectors $x$ and $y$ are orthogonal, they form the sides of a right triangle, where the hypotenuse is given by $x + y$. By the Pythagorean theorem, we thus have

$$
||x||^2 + ||y||^2 = ||x + y||^2,
$$

which equals

$$
\left(\sqrt{x'x}\right)^2 + \left(\sqrt{y'y}\right)^2 = \left(\sqrt{(x + y)'(x + y)}\right)^2,
$$

and simplifies to

$$
\begin{aligned}
x'x + y'y &= (x + y)'(x + y) \\
x'x + y'y &= x'x + y'y + x'y + y'x \\
x'x + y'y &= x'x + y'y + 2x'y,
\end{aligned}
$$

from which it is clear that $x'y = 0$.

:::

Hence, we want:

$$
\begin{aligned}
a'(b - p) &= 0 \\
a'(b - xa) &= 0 \\
a'b - xa'a &= 0 \\
xa'a &= a'b \\
x &= (a'a)^{-1}a'b
\end{aligned}
$$

So that, again, we have:[^order]

$$
p = ax = \underbrace{a(a'a)^{-1}a'}_\text{$P_a$}b.
$$

This also makes clear why this type of projection is called "orthogonal projection": we want to project a vector onto a subspace in such a way that the distance between the original vector and the subspace is minimal. The resulting projection will be a point on the subspace such that a vector from that point to the original vector is orthogonal to the subspace. Intuitively, this is the case because the shortest path between the vector and the subspace will be that which is perpendicular to the subspace, and orthogonality is the generalisation of the notion of perpendicularity.


## Why project?

One reason projection is useful is because it allows us to approximately solve systems of linear equations that have no exact solution. Imagine we have the following system of equations:

$$
\begin{align*}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1k}x_k &= b_1 \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2k}x_k &= b_2 \\
\vdots \\
a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nk}x_k &= b_n
\end{align*}
$$

which we can write more compactly in matrix form as:

$$
Ax = b,
$$

where

$$
A = \begin{pmatrix}
  a_{11} & a_{12} & \cdots & a_{1k} \\
  a_{21} & a_{22} & \cdots & a_{2k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  a_{n1} & a_{n2} & \cdots & a_{nk}
 \end{pmatrix},
\quad
x = 
 \begin{pmatrix}
  x_{1} \\
  x_{2} \\
  \vdots \\
  x_{k} 
 \end{pmatrix},
\quad
b = 
 \begin{pmatrix}
  b_{1} \\
  b_{2} \\
  \vdots \\
  b_{n} 
 \end{pmatrix}.
$$

If $n > k$, the system is overdetermined -- it has more constarints (equations) than degrees of freedom (variables) -- and might not have a solution. In this case, it can be useful to solve

$$
Ax = \hat{b},
$$

where $\hat{b}$ is the orthogonal projection of $b$ onto the vector space spanned by $A$. This vector space is also called the span or column space of $A$. Using orthogonal projection achieves two things: first, it guarantees a solution because $\hat{b}$ lies on the same space as $Ax$ -- they both lie on the subspace spanned by the columns of $A$ -- and, second, it makes $\hat{p}$ the closest approximation to $b$ in terms of the Euclidean distance.


## Projection from 3-D onto 2-D and projecting onto N-D

As in the 2-D onto 1-D example above, we're going to project the vector $b$ onto a subspace. But $b$ is now a 3-D vector, and instead of projecting onto a line characterised by vector $a$, we're projecting onto a 2-D plane characterised by the 2 x 2 matrix $A$. Hence, our projection $p$ is now a vector in the 3-dimensional space that lies on the 2-dimensional subspace. Similarly to above, $p$ is defined by $p = Ax$, where $x$ is a 2-D vector of scalars.

We can still go about finding $p$ in the same way as above:

$$
\begin{aligned}
A'(b - p) &= 0 \\
A'(b - Ax) &= 0 \\
A'b - A'Ax &= 0 \\
A'Ax &= A'b \\
x &= (A'A)^{-1}A'b\\
\\
p = Ax = \underbrace{A(A'A)^{-1}A'}_\text{$P_a$}b.
\end{aligned}
$$

The last step above works only if $A'A$ is actually invertible, which is the case if $A$ has full rank.

::: {.callout-note collapse=true}

## Full rank

A matrix is said to have full rank if none of its columns can be constructed from a linear combination of any other columns.

We can think of the columns of a matrix are the basis vectors of its column space, and the rank of the matrix is the number of dimensions of that column space.

Full rank means thus means that the column space has as many dimensions as there are columns.

:::

The math above generalises directly to projections onto N-dimensional space. All that changes is that $b$, $A$, $x$, and $p$ are higher-dimensional objects, and that visualising what's happening becomes rather mind-bending.


## Useful references

-   [Gilbert Strang's linear algebra lectures at MIT](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/video_galleries/video-lectures/)

-   [10 Fundamental Theorems for Econometrics](https://bookdown.org/ts_robinson1994/10EconometricTheorems/linear_projection.html#linear_projection)


[^euclid_dist]: The Euclidean distance between two points $x$ and $\bar{x}$ in $\mathbb{R}^N$ is defined as $\sqrt{\sum_{i=1}^N{(\bar{a_i} - a_i})^2}$.

[^line]: A line through the origin is a subspace of a two-dimensional vector because it is 1-dimensional (and thus a subset of the 2-dimensional vector) and because all possible linear combinations of vectors on the line will also lie on the line (see box on subspaces for more details).

[^order]: One thing I used to wonder about was whether the order of the vectors in the scalar product $ax$ matters. That is, whether we could also write $p = xa$. Once you think about the rules of linear algebra, the answer becomes quite clear. For a scalar product, the order doesn't matter. However, $p = ax$ is preferred because it naturally generalises to the higher-dimensional case where instead of a scalar $a$, we have a matrix $A$. In this case, $p = Ax$, which is different from $p = xA$, which is generally not defined, and $p = x'A$, which leads to a different result.
