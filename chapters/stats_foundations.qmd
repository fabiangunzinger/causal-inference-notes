# Statistics foundation
<!-- 
```{python}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats
from statsmodels.api import ProbPlot

%config InlineBackend.figure_format ='retina'

sns.set_style("whitegrid")
``` -->

## Important moments of random variables

In general, the kth uncentered moment of a discrete random variable X is defined by

$$
E(X^k) = \sum_{i=1}^n p(x_i)x_i^k
$$,

and the kth centered moment as

$$
E\left(X-E(X)\right)^k = \sum_{i=1}^n p(x_i)(x_i - \mu)^k
$$,

Hence, the mean of a random variable is the first uncentered momement, and the variance is the second centered moment.









## Sampling

- We rely on a sample to learn about a larger population.

- We thus need to make sure that the sampling procedure is free of bias, so that units in the sample are representative of those in the population.

- While representativeness cannot be achieved perfectly, it's important to ensure that non-representativeness is due to random error and not due to systematic bias.

- Random errors produce deviations that vary over repeated samples, while systematic bias persists. Such selection bias can lead to misleading and ephemeral conclusions.

- Two basic sampling procedures are simple random sampling (randomly select $n$ units from a population of $N$) and stratified random sampling (randomly select $n_s$ from each stratum $S$ of a population of $N$).

- The mean outcome of the sample is denoted $\bar{x}$; that of the population, $\mu$.

- (On stratification: why does it reduce variance? Imagine an extreme case, where the number of strata were equal to the number of different units in the sample. In this case, the variance would be zero. Number of diff units here needns be individuals, but groups of units that share all relevant characteristics) 


## Sampling distributions

- A sampling distribution is the distribution of a statistic (e.g. the mean) over many repeated samples. Classical statistics is much concerned with making inferences from samples about the population based on such statistics.

- When we measure an attribute of the population based on a sample using a statistic, the result will vary over repeated samples. To capture by how much it varies, we are concerned with the sampling variability.

- Key distinctions:
    
    - The data distribution is the distribution of the data in the sample, and its spread is measured by the standard deviation.
    
    - The sampling distribution is the distribution of the sample statistic, and its spread is measured by the standard error.


<!-- ```{python}
rng = np.random.default_rng(2312)


def means(data, sample_size, num_means=1000):
    return rng.choice(data, (sample_size, num_means)).mean(0)


# Create dataset with population and sample data
data = pd.DataFrame({"Population": rng.normal(size=1_000_000)})
for n in [10, 100, 1000]:
    data = data.join(
        pd.Series(means(data.Population, n), name=f"Means of samples of {n}")
    )
data = data.melt()


g = sns.FacetGrid(data, col="variable")
g.map(sns.histplot, "value", bins=40, stat="percent")
g.set_axis_labels("Value", "Count")
g.set_titles("{col_name}");
``` -->


Figure show that:

- Data distribution has larger spread than sampling distributions (each data point is a special case of a sample with n = 1)

- The spread of sampling distributions decreases with increasing sample size


## Law of large numbers and central limit theorem

- Suppose that we have a sequence of independent and identically distributed (iid) random variables $\{x_1, ..., x_n\}$ drawn from a distribution with expected value $\mu$ and finite variance $\sigma^2$, and we are interested in the mean value $\bar{x} = \frac{x_1 + ... + x_n}{n}$.

- The law or large numbers states that $\bar{x}$ converges to $\mu$ as we increase the sample size. Formally: 

$$
\bar{x} \rightarrow \mu \text{ as } n \rightarrow \infty.
$$

- The (classical, Lindeberg-LÃ©vy) central limit theorem describes the spread of the sampling distribution of $\bar{x}$ around $\mu$ during this convergence. In particular, it implies that for large enough $n$, the distribution of $\bar{x}$ will be close to a normal distribution with mean $\mu$ and variance $\sigma^2/n$. The above figures are a visual representation of this. Formally:

$$
\lim _{n\to\infty} \sqrt{n}(\bar{x} - \mu) \rightarrow \mathcal{N}\left(0,\sigma ^{2}\right).
$$

- This is useful because it means that irrespective of the underlying distribution (i.e. the distribution of the values in our sequence above), we can use the normal distribution and approximations to it (such as the t-distribution) to calculate sampling distributions when we do inference. Because of this, the CLT is at the heart of the theory of hypothesis testing and confidence intervals, and thus of much of classical statistics.

- For experiments, this means that our estiamted treatment effect is normally distributed, which is what allows us to draw inferences from our experimental setting ot the population as a whole. The CLT is thus at the heart of the experimental approach.

- The CLT also explains the prevalence of the normal distribution in the natural world. Many characteristics of living things we observe and measure are the sum of the additive effects of many genetic and environmental factors, so their distribution tends to be normal. -->


## Standard error

- The standard error is a measure for the variability of the sampling distribution. 

- It is related to the standard deviation of the observations, $\sigma$ and the sample size $n$ in the following way:

$$
se = \frac{\sigma}{\sqrt{n}}
$$

- The relationship between sample size and se is sometimes called the "Square-root of n rule", since reducing the $se$ by a factor of 2 requires an increase in the sample size by a factor of 4.

Derivation:

The sum of a sequence of independent random variables is:

$$
T = (x_1 + x_2 + ... + x_n)
$$

Which has variance

$$
Var(T) = Var(x_1) + Var(x_2) + ... + Var(x_n) = n\sigma^2$$

and mean

$$
\bar{x} = T/n.
$$

The variance of $\bar{x}$ is then given by:

$$
Var(\bar{x}) = Var\left(\frac{T}{n}\right) = \frac{1}{n^2}Var(T) = \frac{1}{n^2}n\sigma^2 = \frac{\sigma^2}{n}.
$$

The standard error is defined as the standard deviation of $\bar{x}$, and is thus

$$
se(\bar{x}) = \sqrt{Var(\bar{x})} = \frac{\sigma}{\sqrt{n}}.
$$


## Bootstrap

- In practice, we often use the bootstrap to calculate standard errors of model parameters or statistics.

- Conceptually, the bootstrap works as follows:

  1) we draw an original sample and calculate our statistic
  2) we then create a blown-up version of that sample by duplicating it many times
  3) we then draw repeated samples from the large sample, recalculate our statistic, and calculate the standard deviation of these statistics to get the standard error.

- To achieve this easily, we can skip step 2) by simply sampling with replacement from the original distribution in step 3).

- The full procedure makes clear what the bootstrap results tell us, however: they tell us how lots of additional samples would behave if they were drawn from a population like our original sample.

- Hence, if the original sample is not representative of the population of interest, then bootstrap results are not informative about that population either.

- The bootstrap can also be used to improve the performance of classification or regression trees by fitting multiple trees on bootstrapped sample and then averaging their predictions. This is called "bagging", short for "bootstrap aggregating".

- We can use to boostrap also to calculate CIs following this algorithm:
  
  1) Draw a large number of bootstrap samples and calculate the statistic of interest
  2) Trim [(100-x)/2] percent of the bootstrap results on either end of the distribution
  3) The trim points are the end point of the CI.


<!-- 
```{python}
from sklearn.utils import resample

rng = np.random.default_rng(2312)

population = rng.normal(3, 5, 1_000_000)
sample = rng.choice(population, 1000)
resample_means = pd.Series(resample(sample).mean() for _ in range(1000))

print(f"{'Population mean:':20} {np.mean(population):.3f}")
print(f"{'Sample mean:':20} {np.mean(sample):.3f}")
print(f"{'Bootstrap mean:':20} {np.mean(resample_means.mean()):.3f}")
print(f"{'Bootstrap se:':20} {np.mean(resample_means.std()):.3f}")
``` -->


## Selection bias

Common types of selection bias in data science:
- The vast search effect (using the data to answer many questions will eventually reveal something interesting by mere chance -- if 20,000 people flip a coin 10 times, some will have 10 straight heads)
- Nonrandom sampling
- Cherry-picking data
- Selecting specific time-intervals
- Stopping experiments prematurely
- Regression to the mean (occurs in settings where we measure outcomes repeatedly over time and where luck and skill combine to determine outcomes, since winners of one period will be less lucky next period and perform closer to the mean performer)

Ways to guard against selection bias:
- have one or many holdout datasets to confirm your results. 


## Standard deviation vs standard error

- Standard deviation is the spread of the distribution of the values in the population of interest

- Standard error is the spread of the distribution of a sample statistic (such as the mean) based on a random sample of population values. 


## Degrees of freedom

In statistics, degrees of freedom generally refers to the number of values in a calculation that can vary freely.

Examples:

- Variance calculation: given that we have a mean, once we know all but one value, we also know final value, since sum of mean deviations has to be zero.

- Covariance calculation: given the two means, once we know the values for all but one x and y pair, we also know the values of the final pair. Hence, we loose one df (not clear to me why not two, given that both x and y are determined -- because we treat their product as a single value? but that seems arbitrary)

- Also, why no correction when we have popultion means? See wikipedia article on variance for section on bias correction

- There is lots of confusion out there when it comes to df. For instance, you sometimes hear people say that df is the number of parameters you had to calculate on route. But this is wrong. It happens to come to the same when calculating variance, but not if you calcualte covariance (where you calculate two means beforehand, but only loose one df).


## Commonly used distributions

from [here](https://en.wikipedia.org/wiki/Variance)

### Commonly used probability distributions

The following table lists the variance for some commonly used probability distributions.

| Name of the probability distribution | Probability distribution function | Mean | Variance |
|--------------------------------------|-----------------------------------|------|----------|
| Binomial distribution                | $\Pr\,(X=k) = \binom{n}{k}p^k(1 - p)^{n-k}$ | $np$ | $np(1 - p)$ |
| Geometric distribution               | $\Pr\,(X=k) = (1 - p)^{k-1}p$ | $\frac{1}{p}$ | $\frac{1 - p}{p^2}$ |
| Normal distribution                  | $f(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}$ | $\mu$ | $\sigma^2$ |
| Uniform distribution (continuous)    | $f(x \mid a, b) = \begin{cases} \frac{1}{b - a} & \text{for } a \le x \le b, \\[3pt] 0 & \text{for } x < a \text{ or } x > b \end{cases}$ | $\frac{a + b}{2}$ | $\frac{(b - a)^2}{12}$ |
| Exponential distribution             | $f(x \mid \lambda) = \lambda e^{-\lambda x}$ | $\frac{1}{\lambda}$ | $\frac{1}{\lambda^2}$ |
| Poisson distribution                 | $f(k \mid \lambda) = \frac{e^{-\lambda}\lambda^{k}}{k!}$ | $\lambda$ | $\lambda$ |


## p-values -- how to draw statistical conclusions ?

Limitations of reliying on pvap

- Arbitrary cutoff

- No appreciation for variation of coefficient -- focus on ci instead (see @romer2020praise, @imbens2021statistical)

- Multiple hypothesis testing (actual) -- report and apply MHT-correction

- Multiple hypothesis testing (potential [Gelman post](https://statmodeling.stat.columbia.edu/2016/03/07/29212/))


## False positive rate vs false discovery rate

- The false positive rate is $P(significant result|no true effect)$

- The false discovery rate is $P(no true effect|significant result)$ 


## Confidence interval interpretations

- 95% CI for control and treatment overlap. Does this imply treatment is not significant?


## Sources

- [Practical statistics for data science](https://learning.oreilly.com/library/view/practical-statistics-for/9781492072935/) -->