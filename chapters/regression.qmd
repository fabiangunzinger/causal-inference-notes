# Regression

Linear regression is by far the most often used method to estimate relationships between outcomes and explanatory variables in applied econometrics and -- probably -- all of applied social science. In this article I want to to two things: define the related terminology that is often used but not defined (OLS, linear equation, etc.) and -- mainly -- explain a less frequently taught and understand way to think about linear regression, that in terms of its linear algebra.

There are, of course, a lot of very good resources on linear regression and OLS, and I list my favourite ones at the bottom. But none of them quite tie everything together in the way I was looking for.

There are two ways to understand linear regression: one is to think of the variables involved as dimensions and of each row as a data point -- this is the way the problem is usually motivated in introductory econometrics classes. The other is the linear algebra approach -- to think of each row of data as a dimension, and to think of the variable as vectors in the space formed by those dimensions.

The first approach straightforwardly links to the intuition of minimising squares, which is useful. It's the one I have learned and relied on most of my life. The second one, though, provides an alternative and very powerful way to understand what linear regression does. And, importantly, understanding the linear algebra notation simplifies much of the notation and manipulations, and opens the way to much of the literature of econometric theory, such as an understanding of the Frisch-Waugh-Lowell theorem, which was the impetus for me to dig into the linear algebra of OLS.

In this post I want to cover the following:

-   Understand all the terminology related to linear regression so we fully know what we're talking about

-   Understand the matrix representation of linear regression

-   Understand how we can think of the least squares solution as a projection

-   Understand why this is a very useful way of seeing things


## Glossary

TODO:
- Linear regression vs OLS

## The setup

We usually start with data of the form $\{y_i, x_{i1}, \cdots, x_{ik}\}_{i=1}^N$, where we observe an outcome variable $y_i$ and a set of $k$ explanatory variables $x_i = (x_{i1}, \cdots, x_{ik})$ for each unit $i$ in the dataset. We think that it might be reasonable to think of the outcome being linearly related to the regressors, so that, for each unit in our dataset, we can write the following linear equation:

$$
y_{i} = \beta_{1}x_{i1} + \beta_{2}x_{i2} + ... + \beta_{k}x_{ik} + \epsilon_{i} 
$$

This says that the outcome $y$ can be thought of as a linear combination of all explanatory variables plus some error term.

TODO: What makes this a "linear" equation:
- The highest power to which any regressor is raised is 1
- The coefficients are constants, not variables
- Regressors are related to one another using addition and subtraction only
- The resulting line (in 2-D space), plane (in 3-D space) and hyperplane (in N-D space) are linear (the term linear equation originates from the simple case where there are two regressors, one of which is a constant, in which case we get a straight line in a Cartesian plane.)

TODO: discuss all the assumptions we're making here.

TODO: Discuss the Angist & Pischke view of linear regression being good approximation even if relationship is not linear.


We thus have a system of linear equations of the form

$$
\begin{aligned}
y_1 = \beta_0 + \beta_1 x_{1_1} + \beta_2 x_{2_1} + \ldots + \beta_k x_{k_1} + \epsilon_1 \\
y_2 = \beta_0 + \beta_1 x_{1_2} + \beta_2 x_{2_2} + \ldots + \beta_k x_{k_2} + \epsilon_2 \\
\vdots\\
y_n = \beta_0 + \beta_1 x_{1_n} + \beta_2 x_{2_n} + \ldots + \beta_k x_{k_n} + \epsilon_n \\
\end{aligned}
$$

which we can rewrite in vector notation as

$$
\begin{aligned}
y_1 = x_1'\beta + \epsilon_1 \\
y_2 = x_2'\beta + \epsilon_2 \\
\vdots\\
y_n = x_n'\beta + \epsilon_n,
\end{aligned}
$$

where 
$$
x_i' = (x_{i1}, x_{i2}, \ldots, x_{ik})
$$

is a $1 \times k$ row vector that contains all $k$ explanatory variables for each unit $i$ and
$$ \beta = 
 \begin{pmatrix}
  \beta_{1}\\
  \beta_{2}\\
  \vdots \\
  \beta_{k} 
 \end{pmatrix}.
$$

is a $k \times 1$ column vector that contains all $k$ regression coefficients.

To be even more succinct, we can stack all n equations to get the matrix notation:
$$
y = X\beta + \epsilon,
$$

where $\beta$ is defined as above,

$$ y = 
 \begin{pmatrix}
  y_{1}\\
  y_{2}\\
  \vdots \\
  y_{k} 
 \end{pmatrix}
$$

is a $n \times 1$ vector containing the $n$ outcome variables, one for each unit in the data,

$$ X = 
 \begin{pmatrix}
  x_{1}' \\
  x_{2}' \\
  \vdots \\
  x_{n}'' 
 \end{pmatrix}
 = 
 \begin{pmatrix}
  x_{1,1} & x_{1,2} & \cdots & x_{1,k} \\
  x_{2,1} & x_{2,2} & \cdots & x_{2,k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{n,1} & x_{n,2} & \cdots & x_{n,k} 
 \end{pmatrix},
$$

is an $n \times k$ matrix that contains all $n$ row vectors $x_i'$ stacked on top of each other, and

$$ \epsilon = 
 \begin{pmatrix}
  \epsilon_{1}\\
  \epsilon_{2}\\
  \vdots \\
  \epsilon_{k} 
 \end{pmatrix}
$$

a column vector containing the $n$ error terms.


## Classic motivation

## Linear algebra motivation







## Resources

- Hayashi, Wooldridge, Verbeek, online resources


Use stuff from cuped blog post


--- 

What does this mean in the context of linear regression? In the context of linear regression, with a covariance matrix $X$, the projection matrix is $P = X(X'X)^{-1}X'$. The coefficient estimates are given by:

$$
\hat{\beta} = (X'X)^{-1}X'y.
$$

and the predicted values are given by:

$$
\hat{y} = X\hat{\beta} = X(X'X)^{-1}X'y = Py.
$$

This tells us that the fitted values in a linear regression are a projection of the vector of observed outcomes, $y$, onto the subspace spanned by $X$.

Finally, the residuals of the linear model are:

$$
\hat{\epsilon} = y - \hat{y} = y - X\hat{\beta} = y - X(X'X)^{-1}X'y = My,
$$

where $M = I - X(X'X)^{-1}X'$. Hence, $M$ is called the residual-maker matrix because it is the matrix that, when pre-multiplied to the vector $y$, returns the vector of residuals.


----