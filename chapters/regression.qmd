# Regression

todo:
- consult ding2024linear
- Use DAaidson and MacKinnon metrics book: nicely separates mechanics of ols with statistical interpretation. Do the same. 
- Include intro on regression from [here](https://people.duke.edu/~rnau/regintro.htm)


Linear regression is by far the most often used method to estimate relationships between outcomes and explanatory variables in applied econometrics and -- probably -- all of applied social science. In this article I want to to two things: define the related terminology that is often used but not defined (OLS, linear equation, etc.) and -- mainly -- explain a less frequently taught and understand way to think about linear regression, that in terms of its linear algebra.

There are, of course, a lot of very good resources on linear regression and OLS, and I list my favourite ones at the bottom. But none of them quite tie everything together in the way I was looking for.

There are two ways to understand linear regression: one is to think of the variables involved as dimensions and of each row as a data point -- this is the way the problem is usually motivated in introductory econometrics classes. The other is the linear algebra approach -- to think of each row of data as a dimension, and to think of the variable as vectors in the space formed by those dimensions.

The first approach straightforwardly links to the intuition of minimising squares, which is useful. It's the one I have learned and relied on most of my life. The second one, though, provides an alternative and very powerful way to understand what linear regression does. Importantly, understanding the linear algebra of OLS allows for the use of much simpler notation and manipulations, and provides direct access to the literature of econometric theory. Case in point: for me, the desire to understand the mechanics, rather than just the intuition, of the  Frisch-Waugh-Lowell theorem, was the impetus for me to dig into the linear algebra of OLS.

In this post I want to cover the following:

-   Understand all the terminology related to linear regression so we fully know what we're talking about

-   Understand the matrix representation of linear regression

-   Understand how we can think of the least squares solution as a projection

-   Understand why this is a very useful way of seeing things


::: {.callout-note collapse=true}

## Why "regression"?

Explain where the name comes from. 

:::



## Glossary

TODO:
- Linear regression vs OLS

## The setup

We usually start with data of the form $\{y_i, x_{i1}, \cdots, x_{ik}\}_{i=1}^N$, where we observe an outcome variable $y_i$ and a set of $k$ explanatory variables $x_i = (x_{i1}, \cdots, x_{ik})$ for each unit $i$ in the dataset. We think that it might be reasonable to think of the outcome being linearly related to the regressors, so that, for each unit in our dataset, we can write the following linear equation:

$$
y_{i} = \beta_{1}x_{i1} + \beta_{2}x_{i2} + ... + \beta_{k}x_{ik} + \epsilon_{i} 
$$

This says that the outcome $y$ can be thought of as a linear combination of all explanatory variables plus some error term.

TODO: What makes this a "linear" equation:
- The highest power to which any regressor is raised is 1
- The coefficients are constants, not variables
- Regressors are related to one another using addition and subtraction only
- The resulting line (in 2-D space), plane (in 3-D space) and hyperplane (in N-D space) are linear (the term linear equation originates from the simple case where there are two regressors, one of which is a constant, in which case we get a straight line in a Cartesian plane.)

TODO: discuss all the assumptions we're making here.

TODO: Discuss the Angist & Pischke view of linear regression being good approximation even if relationship is not linear.


We thus have a system of linear equations of the form

$$
\begin{aligned}
y_1 = \beta_0 + \beta_1 x_{1_1} + \beta_2 x_{2_1} + \ldots + \beta_k x_{k_1} + \epsilon_1 \\
y_2 = \beta_0 + \beta_1 x_{1_2} + \beta_2 x_{2_2} + \ldots + \beta_k x_{k_2} + \epsilon_2 \\
\vdots\\
y_n = \beta_0 + \beta_1 x_{1_n} + \beta_2 x_{2_n} + \ldots + \beta_k x_{k_n} + \epsilon_n \\
\end{aligned}
$$

which we can rewrite in vector notation as

$$
\begin{aligned}
y_1 = x_1'\beta + \epsilon_1 \\
y_2 = x_2'\beta + \epsilon_2 \\
\vdots\\
y_n = x_n'\beta + \epsilon_n,
\end{aligned}
$$

where 
$$
x_i' = (x_{i1}, x_{i2}, \ldots, x_{ik})
$$

is a $1 \times k$ row vector that contains all $k$ explanatory variables for each unit $i$ and
$$ \beta = 
 \begin{pmatrix}
  \beta_{1}\\
  \beta_{2}\\
  \vdots \\
  \beta_{k} 
 \end{pmatrix}
$$

is a $k \times 1$ column vector that contains all $k$ regression coefficients.

To be even more succinct, we can stack all n equations to get the matrix notation:
$$
y = X\beta + \epsilon,
$$

where

$$ X = 
 \begin{pmatrix}
  x_{1}' \\
  x_{2}' \\
  \vdots \\
  x_{n}'' 
 \end{pmatrix}
 = 
 \begin{pmatrix}
  x_{1,1} & x_{1,2} & \cdots & x_{1,k} \\
  x_{2,1} & x_{2,2} & \cdots & x_{2,k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{n,1} & x_{n,2} & \cdots & x_{n,k} 
 \end{pmatrix}
$$

is an $n \times k$ matrix that contains all $n$ row vectors $x_i'$ stacked on top of each other, and

$$ y = 
 \begin{pmatrix}
  y_{1}\\
  y_{2}\\
  \vdots \\
  y_{k} 
 \end{pmatrix}
\quad
\epsilon = 
 \begin{pmatrix}
  \epsilon_{1}\\
  \epsilon_{2}\\
  \vdots \\
  \epsilon_{k} 
 \end{pmatrix}
$$

are both $n \times 1$ vectors containing, respectively, the $n$ outcome variables and error terms.

##Â The problem

So, what do we want to do here? We have data on an outcome variable $y$ and explanatory variables $x$ for each unit $i$, and we think that it is reasonable to think that this data is generated by a process whereby $y$ is the result of a linear combination of the $x$s plus some noise, which we capture in the error term. The challenge is to find the right linear combination.

## Classic motivation of the solution

TODO


## Linear algebra motivation of the solution

Notice how our problem here is exactly akin to the motivation for orthogonal projection discussed in @sec-projection. There we had a system of linear equations of the form

$$
Ax = b
$$

which was overdetermined because the number of equations exceeded the number of unknowns. Our setup is equivalent. We have $n$ equations and $k$ unknowns (the $\beta$s), so that -- in practice -- there will be no solution to the system:

$$
X\beta = y.
$$

In other words, there is no choice of $\beta$ that would linearly combine all the explanatory variables in each equation such that the result would be exactly $y$. We account for this by adding the error term $\epsilon$, so that we have

$$
X\beta + \epsilon = y.
$$

What we do, now, is to say that we want to find that linear combination of the explanatory variables that is closest to $y$, so that $\epsilon$ is as small as possible, which is the same as finding the orthogonal projection of $y$ onto $X$, which, traditionally, we call $\hat{y}$.

The solution is then the same as in @sec-projection:[^order]

$$
\begin{aligned}
X'\epsilon &= 0 \\
X'(y - \hat{y}) &= 0 \\
X'(y - X\beta) &= 0 \\
X'y - X'X\beta &= 0 \\
X'X\beta &= X'y \\
\beta &= (X'X)^{-1}X'y
\end{aligned}
$$



## Resources

- Hayashi, Wooldridge, Verbeek, online resources


[^order]: One thing I would always wonder about in textbook is how I knew that the condition was $X'\epsilon$ instead of $\epsilon'X$. The answer is that in cases where order doesn't matter, texts tend to choose what is more convenient for the math. We could solve $\epsilon'X$:
$$
\begin{aligned}
\epsilon'X &= 0 \\
(y - \hat{y})'X &= 0 \\
(y - X\beta)'X &= 0 \\
(y' - \beta'X')X &= 0 \\
y'X - \beta'X'X &= 0 \\
\beta'X'X &= y'X \\
(\beta'X'X)' &= (y'X)' \\
X'X\beta &= X'y \\
\beta &= (X'X)^{-1}X'y
\end{aligned}
$$
which gets us to the same result but in more steps.



## Assumptions

From Davidson and MacKinnon
- Identifying assumption: E(e) = 0 -- this ensures that regression equation can be true only for particular values of the betas, instead of for any given one (which it could be, if error could just be anything)

- Modelling e as a random variable: we're using the mathematical concept of randomness to model our ignorance of the data generating process. Assuming E(e) = 0 is saying that, on average, the determinants of y we ignore cancel out. That doesn't mean they are small. They can be very big. But estimating b still useful to know relationship between x we can observe and y.

- We model variables in econometric models as random variables -- sets of possibilities -- and we observe a single realisation of each random var for each unit in the data.




- Directly copied from or heavily based on https://people.duke.edu/~rnau/testing.htm. Edit and expand over time.

- In short, the assumptions are that the model is linear and additive, and that errors are iid. The latter assumption is often stated as three separate assumptions, as shown below.

- Note: The dependent and independent variables in a regression model do not need to be normally distributed by themselves--only the prediction errors need to be normally distributed.  (In fact, independent variables do not even need to be random, as in the case of trend or dummy or treatment or pricing variables.)  But if the distributions of some of the variables that are random are extremely asymmetric or long-tailed, it may be hard to fit them into a linear model whose errors will be normally distributed, and explaining the shape of their distributions may be an interesting topic all by itself.  Keep in mind that the normal error assumption is usually justified by appeal to the central limit theorem, which holds in the case where many random variations are added together.  If the underlying sources of randomness are not interacting additively, this argument fails to hold.

There are four principal assumptions which justify the use of linear regression models for purposes of inference or prediction:

(i) linearity and additivity of the relationship between dependent and independent variables:

    (a) The expected value of dependent variable is a straight-line function of each independent variable, holding the others fixed.

    (b) The slope of that line does not depend on the values of the other variables.

    (c)  The effects of different independent variables on the expected value of the dependent variable are additive.

(ii) statistical independence of the errors (in particular, no correlation between consecutive errors in the case of time series data)

(iii) homoscedasticity (constant variance) of the errors

  (a) versus time (in the case of time series data)

  (b) versus the predictions

  (c) versus any independent variable

(iv) normality of the error distribution.


If any of these assumptions is violated, then the forecasts, confidence intervals, and scientific insights yielded by a regression model may be (at best) inefficient or (at worst) seriously biased or misleading.

### Linearity and additivity

Why assume linearity?

- It's the simplest non-trivial relationship and the easiest to work with

- True relationships are often at least approximately linear for ranges we care about

- Even if above not true, we can often transform variables so that relationshiop is linear

How to test linearity

- Look at scatterplot before running regression

- Look at predicted values vs residual plot after running regression (linear model leads to evenly distributed dots across a horizontal line) -- can look at predicted vs observed value plot, too, in which case you expect points to fall on the 45 degree line, but this diagonal introduces additional visual noise.

What if linearity is violated?

- Add non-linear transformation like log (if data is strictly positive)

- Add regressor that is a non-liner function of a current regressor (e.g. x^2)

- Add new regressor that explains non-linear nature of current relationshiop


### iid errors

Why assume iid errors?

- In many cases, it's justified by the CLT. As long as we calculate averages (sums or random variables, really), which are independent and identically distributed, the distribution will be approximately normal.

- It's convenient because it implies that the optimal coefficient estimates for linear models minimise the MSE, and because it justified using tests based on the normal family (t, F, Chi-square distribution).

- Even if the "true" error process is not normal in terms of the original units of the data, it may be possible to transform the data so that your model's prediction errors are approximately normal.

- Note that normality is required for inferenence, not for parameter estimation or prediction.


Potential issues

- Heteroskedasticity (variance larger depending on come conditions -- certain covariate values)

- Interdependence (e.g. in time-series models, in the context of network effects)

How to check assumption

- Independence

  - In time-series: look at residual autocorrelation
  - In non-time series: plot independent variables vs residuals -- independence suggests symmetric distribution around zero and no dependence in subsequent residuals under any ordering that is not based on the independent variables (i.e. no correlation given covariates)

- Homoskedasticity

  - Time series: plot time vs residuals
  - Non-time series: plot predicted values vs residuals and independent variables vs residuals 

- Normality

    - Normal probability plot and normal quantile plot

    - Could also use formal tests (Kolmogorov-Smirnov test, Shapiro-Wilk test, Jarque-Bera test, and the Anderson-Darling test), but they are often too restrictive in practice.


How to fix violations

- Independence

  - Time-series: for mild violations (Durbin-Watson between 1.2 and 1) adding lags, for serious violations (DW > 2.6) use difference model 

  - Non-time series: either due to non-linearity of model or due to omitted variable.   

- Homoskedasticity

  - Log transformation
  - Model seasonality with dummies (do linearly or multiplicatively (take log of dependent variable))
  - Seasonally adjust data before fitting model
  
- Normality

  - Violations of normality often arise either because (a) the distributions of the dependent and/or independent variables are themselves significantly non-normal, and/or (b) the linearity assumption is violated. In such cases, a nonlinear transformation of variables might cure both problems. 

  - Another possibility is that there are two or more subsets of the data having different statistical properties, in which case separate models should be built, or else some data should merely be excluded, provided that there is some a priori criterion that can be applied to make this determination.

  - In some cases, the problem with the error distribution is mainly due to one or two very large errors. Such values should be scrutinized closely: are they genuine (i.e., not the result of data entry errors), are they explainable, are similar events likely to occur again in the future, and how influential are they in your model-fitting results? If they are merely errors or if they can be explained as unique events not likely to be repeated, then you may have cause to remove them. In some cases, however, it may be that the extreme values in the data provide the most useful information about values of some of the coefficients and/or provide the most realistic guide to the magnitudes of forecast errors.
