{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fisher's exact P-value approach {#sec-fisher}\n",
        "\n",
        "These are my notes from reading chapter 5 in @imbens2015causal.\n",
        "\n",
        "Fisher's aim in his original work on experimentation was to asses the *sharp (or exact) null hypothesis* -- the hypothesis that a treatment had no effect whatsoever, meaning that the potential outcomes for being treated and not treated are the same for each unit in the data. (Compared to the null hypothesis that there is no treatment effect on average, which we use more frequently in experiments, Fisher's hypothesis is stricter -- of, if you like, sharper -- but the former is, arguably, of more practical interest in most cases.)\n",
        "\n",
        "Unter that null hypothesis, the unobserved potential outcomes are known -- they are the same as the observed outcome -- and we can use the randomisation distribution to calculate P-values. The randomisation distribution is the distribution of values of the test statistic based on all possible assignment vectors. We can calculate P-values by calculating the test statistic for all possible assignments, and then calculate the probability that the test-statistic is as extreme or more extreme than the value of the test statistic in our experiment.\n",
        "\n",
        "Three important points about the approach:\n",
        "\n",
        "- The key point about the sharp null hypothesis is that it allows us to impute all unobserved potential outcomes, not that the treatment effect is zero. The postulated treatment effect could thus be any deterministic function of the available data (e.g. an additive or multiplicative homogenous effect or a function of observed covariates).\n",
        "\n",
        "- The only source of randomness in this framework is the randomness introduced by randomisation, meaning there is no randomness from sampling because we are interested in conclusions about the specific sample at hand, not about a superpopulation.\n",
        "\n",
        "- The approach is truly non-parametric in that it doesn't rely on a model of outcomes at all. In particular, we don't model the distribution of test statistic values but calculate it directly. We can do that because we know the assignment mechanism, which is known by definition in a completely randomised experiment, and because of the strict null hypothesis, which allows us to solve the fundamental problem of causal inference by imputing the unobserved potential values.\n",
        "\n",
        "- @imbens2015causal point out that while Fisher's approach is often a good starting point, we are often concerned not only with learning that a treatment has some effect for some individual (which is what rejecting the sharp null hypothesis implies) but with estimating an average treatment effect without worrying about variations of the effect. For this, we need to rely on [Neyman's approach](neyman.qmd).\n",
        "\n",
        "\n",
        "## A simple example\n",
        "\n",
        "We have a sample of six units, three assigned to treatment ($W_i = 1$) and three to control ($W_i = 0$). The first two columns in the table show the (unobserved) potential outcomes under control and treatment status, the third and fourth column show the observed treatment status and outcome. For each of the six units the outcome value we observe is the potential outcome corresponding to the treatment status.\n",
        "\n",
        "|   | $Y_i(0)$ | $Y_i(1)$ | $W_i$ | $Y_i^{obs}$ |\n",
        "|---|----------|----------|-------|-------------|\n",
        "| 1 | ?        | 3        | 1     | 3           |\n",
        "| 2 | ?        | 5        | 1     | 5           |\n",
        "| 3 | ?        | 0        | 1     | 0           |\n",
        "| 4 | 4        | ?        | 0     | 4           |\n",
        "| 5 | 0        | ?        | 0     | 0           |\n",
        "| 6 | 1        | ?        | 0     | 1           |\n",
        "\n",
        "This table highlights the fundamental problem of causal inference -- we can only ever observe one potential outcome for each unit.\n",
        "\n",
        "However, Fisher's sharp null hypothesis asserts that:\n",
        "\n",
        "$$\n",
        "H_0: Y_i(1) = Y_i(0) \\quad \\text{for $i = 1, \\dots, 6$},\n",
        "$$\n",
        "\n",
        "which makes filling in the missing values trivial:\n",
        "\n",
        "|   | $Y_i(0)$ | $Y_i(1)$ | $W_i$ | $Y_i^{obs}$ |\n",
        "|---|----------|----------|-------|-------------|\n",
        "| 1 | (3)        | 3        | 1     | 3           |\n",
        "| 2 | (5)        | 5        | 1     | 5           |\n",
        "| 3 | (0)        | 0        | 1     | 0           |\n",
        "| 4 | 4        | (4)        | 0     | 4           |\n",
        "| 5 | 0        | (0)        | 0     | 0           |\n",
        "| 6 | 1        | (1)        | 0     | 1           |\n",
        "\n",
        "There are a number of different test statistics we could use (and @imbens2015causal discuss and compare an number of them). Following the book, for this example I use the absolute value of the difference in average outcome by treatment status, $T(W, Y^{obs}) = |\\bar{Y}_t^{obs} - \\bar{Y}_c^{obs}|$, which is a function of the random assignment $W$ and the observed values $Y^{obs}$.\n",
        "\n",
        "For our little experiment above, we can calculate the test statistic easily as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "T^{obs} &= |(Y_1^{obs} + Y_2^{obs} + Y_3^{obs})/3 - (Y_4^{obs} + Y_5^{obs} + Y_6^{obs})/3| \\\\\n",
        "&= |(3 + 5 + 0)/3 - (4 + 0 + 1)/3| \\\\\n",
        "&= 8/3 - 5/3 \\\\\n",
        "&= 1\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To calculate the P-value, we need the distribution of test statistics under all possible random assignments. There are $\\begin{psmallmatrix}6\\\\3\\end{psmallmatrix} = 20$ different assignments, and we can calculate the distribution using Python:\n"
      ],
      "id": "753416e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import seaborn as sns\n",
        "\n",
        "y = np.array([3, 5, 0, 4, 0, 1])\n",
        "idx = range(len(y))\n",
        "\n",
        "ts = []\n",
        "for w in combinations(idx, 3):\n",
        "    w0, w1 = list(set(idx) - set(w)), list(w)\n",
        "    y0, y1 = y[w0], y[w1]\n",
        "    t = abs(np.mean(y1) - np.mean(y0))\n",
        "    ts.append(t)\n",
        "\n",
        "sns.histplot(ts);"
      ],
      "id": "f70953ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With that distribution in hand, we can easily calculate the P-value:\n"
      ],
      "id": "caa68fb2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "p = np.mean([t >= 1 for t in ts])\n",
        "p"
      ],
      "id": "bd372ec1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The P-value indicates that if there is no treatment effect, we'd expect a value of the test statistic equal to 1 or even larger in 50 out of 100 random experiments, which does not provide any evidencen against the null hypothesis.\n",
        "\n",
        "## Useful resources\n",
        "\n",
        "@imbens2015causal"
      ],
      "id": "cfb18e14"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}