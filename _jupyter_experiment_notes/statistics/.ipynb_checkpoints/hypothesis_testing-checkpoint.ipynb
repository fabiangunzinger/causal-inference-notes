{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e48a0f0-bb74-4873-96c0-f127754eca64",
   "metadata": {},
   "source": [
    "# Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd7fc50-41cd-4eae-9695-899659b5b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada9b62-133a-4363-bc76-ae4b1d148c93",
   "metadata": {},
   "source": [
    "## Historical origins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f2733-5217-491e-99a8-2a9f847a654e",
   "metadata": {},
   "source": [
    "- There are two schools of thought in statistical significance testing.\n",
    "\n",
    "- R.A. Fisher thought of the p-value as a useful metric to determine how surprising a given set of data was.\n",
    "\n",
    "- Jerzy Neyman and Egon Pearson realised that while it is not possible to eliminate false positives and negatives, it is possible to set up a process that guarantees that false positives occur only at a pre-defined rate, which they called $\\alpha$.\n",
    "\n",
    "- Unlike in Fisher's approach, the p-value in the Neyman-Pearson approach doesn't tell us anything about the strength of the evidence in any particular experiment besides whether or not to reject the null hypothesis, but guarantees that in the long run (over the course of many experiments), our false positive rate is not larger than $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b204b54-a824-41b5-8ba5-09c63046230d",
   "metadata": {},
   "source": [
    "## Basic proceedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7c096-e8dc-4562-8df2-9a1183829e54",
   "metadata": {},
   "source": [
    "## Type I and II errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffb2f2-b284-4868-ad92-96a8850fe4f3",
   "metadata": {},
   "source": [
    "\n",
    "- The p-value is the probability of observing a test result at least as large as the one we observed under the assumption that the null hypothesis is true.\n",
    "\n",
    "- A Type I error is when we reject the null hypothesis when it is true (a false positive). It's probability is usually denoted by $\\alpha$.\n",
    "\n",
    "- A Type II error is when we fail to reject the null  hypothesis when it is false (a false negative). It's probability is usually denoted by $\\beta$.\n",
    "\n",
    "- The statistical power of a binary hypothesis test is the probability of a true positive: the probability that we reject the hull hypothesis if it is false (and hence, if the alternative hypothesis is true). The probability of this is $1-\\beta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7e0f2-acd2-45b0-a5cf-93176bb3b9be",
   "metadata": {},
   "source": [
    "## Limitations of significance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8edfb6-cbd6-4ab5-a469-c36c58c40eb5",
   "metadata": {},
   "source": [
    "- p-values do not tell you anything about whether the result has any practical significance.\n",
    "\n",
    "- Any intervention usually has *some* effect, you will always find a significant result with enough data. The question is whether the range of plausible effect sizes (captured by the confidence interval) is relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da20d7c-7423-42af-bc9b-74ad9ea35075",
   "metadata": {},
   "source": [
    "## Alternatives\n",
    "\n",
    "- Rely on confidence intervals: they provide the same information as a p-value (if they include the value of the null hypothesis then we cannot reject it) but the width of the interval also provides additional information on the uncertainty of the effect and its practical significance. (See [In Praise of Confidence Intervals](https://www.aeaweb.org/articles?id=10.1257/pandp.20201059) by David Romer and [Statistical Significance, p-Values, and the Reporting of Uncertainty](https://www.aeaweb.org/articles?id=10.1257/jep.35.3.157) by Guido Imbens for more)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0746f-4fda-44eb-b6ec-62baa6569e74",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca94f8-dd1a-42b3-9e73-19e7f73a6239",
   "metadata": {},
   "source": [
    "**1.**\n",
    "\n",
    "a) A student is asked 12 true-or-false questions and gets 3 of them wrong. What is the probability that the student guessed randomly?\n",
    "\n",
    "b) The student above would actually have been asked questions until they got 3 wrong. What is the probability that they guessed randomly?\n",
    "\n",
    "c)What do you conclude from a) and b)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22fe978-8d3b-4941-a960-214905c4e925",
   "metadata": {},
   "source": [
    "**2.**\n",
    "\n",
    "Consider the outcome of a large sample size (e.g. 10K subjects) A/B test that yielded tiny (e.g. odds ratio of 1.0008) but statistically significant (e.g. p < 0.001) results. Would you deploy the change to production. Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c18e1d-be0d-47d5-b061-62659a1e1dc9",
   "metadata": {},
   "source": [
    "**3.**\n",
    "\n",
    "A researcher runs an experiment, gets a p-value of 0.033, and concludes that his false positive rate is 3.3 percent. Is that correct? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbe8e1-bdae-464f-8826-023f7e6d8589",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ca4e6-bf8a-4f0a-885d-3a6e50f7c5b2",
   "metadata": {},
   "source": [
    "**1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c2b314-ac7e-42d0-beb5-d91462c771cf",
   "metadata": {},
   "source": [
    "a) Calculate p-value for getting no more than observed number of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec836ab4-6607-4c3d-a1ec-69c0de5d1968",
   "metadata": {},
   "source": [
    "p = stats.binom.cdf(k=3, n=12, p=0.5)\n",
    "print(f\"p-value is: {p:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80e882-153e-4953-a5cd-4beec9ee33f6",
   "metadata": {},
   "source": [
    "**3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e6aad-155b-4742-9ff8-584469dc774c",
   "metadata": {},
   "source": [
    "No. An individual experiment doesn't have a false positive rate. The false positive rate is determined by the proceedure used for experiments over the long run. For example, if you consistently use a p-value of 0.05, then you are guaranteed to have a false positive rate of 5 percent in the long run. This is the main insight underlying the Neyman-Pearson hypothesis testing framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wow",
   "language": "python",
   "name": "wow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
